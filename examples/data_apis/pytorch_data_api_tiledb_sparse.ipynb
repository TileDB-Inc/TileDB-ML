{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This example notebook shows how we can train a Factorization Machines classifier.\n",
    "Factorization Machines ([FMs](https://www.csie.ntu.edu.tw/~b97053/paper/Rendle2010FM.pdf)) are a general predictor, that are able to perform well in problems with high sparsity, like recommender systems.\n",
    "We employ TileDB as a storage engine for our training data and labels.\n",
    "We will use the MovieLens 100K public data set, available [here](https://grouplens.org/datasets/movielens/100k/). We will first download the\n",
    "MovieLens, which contains 100.000 ratings, by 943 users on 1682 items.\n",
    "Continuing, we will use our TileDB support for PyTorch Sparse Dataloader API in order to train an FM classifier.\n",
    "First, let's import what we need and download our data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import urllib.request\n",
    "import numpy as np\n",
    "import tiledb\n",
    "import torch\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset\n",
    "Download MovieLens dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "('u.data', <http.client.HTTPMessage at 0x14b411eb0>)"
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "url = 'https://files.grouplens.org/datasets/movielens/ml-100k/u.data'\n",
    "filename = 'u.data'\n",
    "urllib.request.urlretrieve(url, filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "Use pandas to display dataset in readable form"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "   user_id  item_id  rating\n0      196      242       3\n1      186      302       3\n2       22      377       1\n3      244       51       2\n4      166      346       1",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>user_id</th>\n      <th>item_id</th>\n      <th>rating</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>196</td>\n      <td>242</td>\n      <td>3</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>186</td>\n      <td>302</td>\n      <td>3</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>22</td>\n      <td>377</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>244</td>\n      <td>51</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>166</td>\n      <td>346</td>\n      <td>1</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "(100000, 3)"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "data = pd.read_csv(filename, sep=\"\\t\", header=None, engine='python')\n",
    "data.columns = [\"user_id\", \"item_id\", \"rating\", \"timestamp\"]\n",
    "data.drop([\"timestamp\"], axis=1, inplace=True)\n",
    "display(data.head())\n",
    "display(data.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Data Analysis / Sparsity\n",
    "Before we apply the one-hot transformation let’s check the memory usage of our original data frame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memory usage is 2.4 MB\n"
     ]
    }
   ],
   "source": [
    "BYTES_TO_MB_DIV = 0.000001\n",
    "def print_memory_usage_of_data_frame(df):\n",
    "    mem = round(df.memory_usage().sum() * BYTES_TO_MB_DIV, 3)\n",
    "    print(\"Memory usage is \" + str(mem) + \" MB\")\n",
    "\n",
    "print_memory_usage_of_data_frame(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Data transformation\n",
    "\n",
    "Now, let’s apply the transformation and check the memory usage of the transformed data frame."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "data": {
      "text/plain": "   rating  user_id_1  user_id_2  user_id_3  user_id_4  user_id_5  user_id_6  \\\n0       3          0          0          0          0          0          0   \n1       3          0          0          0          0          0          0   \n2       1          0          0          0          0          0          0   \n3       2          0          0          0          0          0          0   \n4       1          0          0          0          0          0          0   \n\n   user_id_7  user_id_8  user_id_9  ...  item_id_1673  item_id_1674  \\\n0          0          0          0  ...             0             0   \n1          0          0          0  ...             0             0   \n2          0          0          0  ...             0             0   \n3          0          0          0  ...             0             0   \n4          0          0          0  ...             0             0   \n\n   item_id_1675  item_id_1676  item_id_1677  item_id_1678  item_id_1679  \\\n0             0             0             0             0             0   \n1             0             0             0             0             0   \n2             0             0             0             0             0   \n3             0             0             0             0             0   \n4             0             0             0             0             0   \n\n   item_id_1680  item_id_1681  item_id_1682  \n0             0             0             0  \n1             0             0             0  \n2             0             0             0  \n3             0             0             0  \n4             0             0             0  \n\n[5 rows x 2626 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>rating</th>\n      <th>user_id_1</th>\n      <th>user_id_2</th>\n      <th>user_id_3</th>\n      <th>user_id_4</th>\n      <th>user_id_5</th>\n      <th>user_id_6</th>\n      <th>user_id_7</th>\n      <th>user_id_8</th>\n      <th>user_id_9</th>\n      <th>...</th>\n      <th>item_id_1673</th>\n      <th>item_id_1674</th>\n      <th>item_id_1675</th>\n      <th>item_id_1676</th>\n      <th>item_id_1677</th>\n      <th>item_id_1678</th>\n      <th>item_id_1679</th>\n      <th>item_id_1680</th>\n      <th>item_id_1681</th>\n      <th>item_id_1682</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>3</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>3</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>2</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n  </tbody>\n</table>\n<p>5 rows × 2626 columns</p>\n</div>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "(100000, 2626)"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memory usage is 263.3 MB\n"
     ]
    }
   ],
   "source": [
    "data_one_hot = pd.get_dummies(data, columns=['user_id', 'item_id'])\n",
    "display(data_one_hot.head())\n",
    "display(data_one_hot.shape)\n",
    "print_memory_usage_of_data_frame(data_one_hot)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "We will slice the dataset `user_movie` will be our `x_train` data transformed with one-hot encoding.\n",
    "So we expect its schema to be the number of ratings as rows and binary columns for users + binary columns\n",
    "for each item (in our case movies). This will lead to (100000, 2625)\n",
    "\n",
    "The target data will be the `ratings`, which will include the ratings and thus will have a shape of (100000,1)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "user_movie = data_one_hot[data_one_hot.columns.difference(['rating'])]\n",
    "ratings = data['rating']\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Data Ingestion\n",
    "Then we proceed with ingesting `user_movies` into sparse TileDB arrays as our training data and `rating` into dense TileDB\n",
    "array as our target data. Here, we should point out that besides the\n",
    "flexibility of TileDB in defining a schema, i.e., multiple dimensions, multiple attributes, compression etc,\n",
    "we choose to define a simple schema. So, for a numpy array of D number of dimensions we create a dense TileDB array,\n",
    "with the same number of dimensions, and a single attribute of data type numpy float32."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def get_schema(data: np.array, batch_size: int, sparse: bool) -> tiledb.ArraySchema:\n",
    "    dims = [\n",
    "        tiledb.Dim(\n",
    "            name=\"dim_\" + str(dim),\n",
    "            domain=(0, data.shape[dim] - 1),\n",
    "            tile=data.shape[dim] if dim > 0 else batch_size,\n",
    "            dtype=np.int32,\n",
    "        )\n",
    "        for dim in range(data.ndim)\n",
    "    ]\n",
    "\n",
    "    # TileDB schema\n",
    "    schema = tiledb.ArraySchema(\n",
    "        domain=tiledb.Domain(*dims),\n",
    "        sparse=sparse,\n",
    "        attrs=[tiledb.Attr(name=\"features\", dtype=np.float32)],\n",
    "    )\n",
    "\n",
    "    return schema\n",
    "\n",
    "# Let's define an ingestion function\n",
    "def ingest_in_tiledb(data: np.array, batch_size: int, uri: str, sparse: bool):\n",
    "    schema = get_schema(data, batch_size, sparse)\n",
    "\n",
    "    # Create the (empty) array on disk.\n",
    "    tiledb.Array.create(uri, schema)\n",
    "\n",
    "    # Ingest\n",
    "    with tiledb.open(uri, \"w\") as tiledb_array:\n",
    "        idx = np.nonzero(data) if sparse else slice(None)\n",
    "        tiledb_array[idx] = {\"features\": data[idx]}"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "We transform our sliced/split data to numpy arrays, so we can ingest them in TileDB arrays"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [],
   "source": [
    "user_movie = user_movie.to_numpy()\n",
    "ratings = ratings.to_numpy()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "We ingest `user_movie` as sparse TileDB array and `ratings` as dense TileDB array"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [],
   "source": [
    "#Ingest images\n",
    "ingest_in_tiledb(data=user_movie, batch_size=64, uri='training_images', sparse=True)\n",
    "\n",
    "# Ingest labels\n",
    "ingest_in_tiledb(data=ratings, batch_size=64, uri='training_labels', sparse=False)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now explore our TileDB arrays and check their structure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ArraySchema(\n",
      "  domain=Domain(*[\n",
      "    Dim(name='dim_0', domain=(0, 99999), tile=64, dtype='int32'),\n",
      "    Dim(name='dim_1', domain=(0, 2624), tile=2625, dtype='int32'),\n",
      "  ]),\n",
      "  attrs=[\n",
      "    Attr(name='features', dtype='float32', var=False, nullable=False),\n",
      "  ],\n",
      "  cell_order='row-major',\n",
      "  tile_order='row-major',\n",
      "  capacity=10000,\n",
      "  sparse=True,\n",
      "  allows_duplicates=False,\n",
      "  coords_filters=FilterList([ZstdFilter(level=-1)]),\n",
      ")\n",
      "\n",
      "ArraySchema(\n",
      "  domain=Domain(*[\n",
      "    Dim(name='dim_0', domain=(0, 99999), tile=64, dtype='int32'),\n",
      "  ]),\n",
      "  attrs=[\n",
      "    Attr(name='features', dtype='float32', var=False, nullable=False),\n",
      "  ],\n",
      "  cell_order='row-major',\n",
      "  tile_order='row-major',\n",
      "  capacity=10000,\n",
      "  sparse=False,\n",
      "  coords_filters=FilterList([ZstdFilter(level=-1)]),\n",
      ")\n",
      "\n"
     ]
    }
   ],
   "source": [
    "user_movie_array = tiledb.open('training_images')\n",
    "ratings_array = tiledb.open('training_labels')\n",
    "\n",
    "print(user_movie_array.schema)\n",
    "print(ratings_array.schema)"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Model training\n",
    "Although we used Factorization Machines as a reference model to create\n",
    "our training set, here we will train a simple Logistic Regression model in Pytorch only\n",
    "for demonstration purposes. Anyone can easily build any Model to train on the data."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Declare Model Class"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "class LogisticRegression(nn.Module):\n",
    "    def __init__(self, shape):\n",
    "        super(LogisticRegression, self).__init__()\n",
    "        self.linear = torch.nn.Linear(shape[0], shape[1])\n",
    "\n",
    "    def forward(self, x):\n",
    "        outputs = self.linear(x)\n",
    "        return outputs"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Train the Model\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1 Batch: 0 Loss: 11.897947\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/konstantinostsitsimpikos/tileroot/TileDB-ML/venv/lib/python3.8/site-packages/torch/nn/modules/loss.py:528: UserWarning: Using a target size (torch.Size([32])) that is different to the input size (torch.Size([32, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1 Batch: 100 Loss: 1.434249\n",
      "Train Epoch: 1 Batch: 200 Loss: 1.122545\n",
      "Train Epoch: 1 Batch: 300 Loss: 1.596507\n",
      "Train Epoch: 1 Batch: 400 Loss: 1.251316\n",
      "Train Epoch: 1 Batch: 500 Loss: 1.563675\n",
      "Train Epoch: 1 Batch: 600 Loss: 1.439507\n",
      "Train Epoch: 1 Batch: 700 Loss: 1.296800\n",
      "Train Epoch: 1 Batch: 800 Loss: 1.036165\n",
      "Train Epoch: 1 Batch: 900 Loss: 1.375567\n",
      "Train Epoch: 1 Batch: 1000 Loss: 0.937286\n",
      "Train Epoch: 1 Batch: 1100 Loss: 1.061382\n",
      "Train Epoch: 1 Batch: 1200 Loss: 1.314131\n",
      "Train Epoch: 1 Batch: 1300 Loss: 1.419514\n",
      "Train Epoch: 1 Batch: 1400 Loss: 1.049100\n",
      "Train Epoch: 1 Batch: 1500 Loss: 1.067766\n",
      "Train Epoch: 1 Batch: 1600 Loss: 1.188462\n",
      "Train Epoch: 1 Batch: 1700 Loss: 0.808228\n",
      "Train Epoch: 1 Batch: 1800 Loss: 1.374337\n",
      "Train Epoch: 1 Batch: 1900 Loss: 1.305547\n",
      "Train Epoch: 1 Batch: 2000 Loss: 1.811797\n",
      "Train Epoch: 1 Batch: 2100 Loss: 1.568941\n",
      "Train Epoch: 1 Batch: 2200 Loss: 1.187829\n",
      "Train Epoch: 1 Batch: 2300 Loss: 1.315253\n",
      "Train Epoch: 1 Batch: 2400 Loss: 1.187909\n",
      "Train Epoch: 1 Batch: 2500 Loss: 1.376827\n",
      "Train Epoch: 1 Batch: 2600 Loss: 1.125839\n",
      "Train Epoch: 1 Batch: 2700 Loss: 0.663477\n",
      "Train Epoch: 1 Batch: 2800 Loss: 1.135154\n",
      "Train Epoch: 1 Batch: 2900 Loss: 0.742940\n",
      "Train Epoch: 1 Batch: 3000 Loss: 1.584489\n",
      "Train Epoch: 1 Batch: 3100 Loss: 1.247983\n",
      "Train Epoch: 2 Batch: 0 Loss: 1.530597\n",
      "Train Epoch: 2 Batch: 100 Loss: 1.415536\n",
      "Train Epoch: 2 Batch: 200 Loss: 1.122042\n",
      "Train Epoch: 2 Batch: 300 Loss: 1.596394\n",
      "Train Epoch: 2 Batch: 400 Loss: 1.250659\n",
      "Train Epoch: 2 Batch: 500 Loss: 1.563430\n",
      "Train Epoch: 2 Batch: 600 Loss: 1.439235\n",
      "Train Epoch: 2 Batch: 700 Loss: 1.296204\n",
      "Train Epoch: 2 Batch: 800 Loss: 1.036124\n",
      "Train Epoch: 2 Batch: 900 Loss: 1.375446\n",
      "Train Epoch: 2 Batch: 1000 Loss: 0.937038\n",
      "Train Epoch: 2 Batch: 1100 Loss: 1.061051\n",
      "Train Epoch: 2 Batch: 1200 Loss: 1.314221\n",
      "Train Epoch: 2 Batch: 1300 Loss: 1.416454\n",
      "Train Epoch: 2 Batch: 1400 Loss: 1.048643\n",
      "Train Epoch: 2 Batch: 1500 Loss: 1.067720\n",
      "Train Epoch: 2 Batch: 1600 Loss: 1.188342\n",
      "Train Epoch: 2 Batch: 1700 Loss: 0.808307\n",
      "Train Epoch: 2 Batch: 1800 Loss: 1.374311\n",
      "Train Epoch: 2 Batch: 1900 Loss: 1.305571\n",
      "Train Epoch: 2 Batch: 2000 Loss: 1.811845\n",
      "Train Epoch: 2 Batch: 2100 Loss: 1.569407\n",
      "Train Epoch: 2 Batch: 2200 Loss: 1.187827\n",
      "Train Epoch: 2 Batch: 2300 Loss: 1.315123\n",
      "Train Epoch: 2 Batch: 2400 Loss: 1.187890\n",
      "Train Epoch: 2 Batch: 2500 Loss: 1.376836\n",
      "Train Epoch: 2 Batch: 2600 Loss: 1.125744\n",
      "Train Epoch: 2 Batch: 2700 Loss: 0.663002\n",
      "Train Epoch: 2 Batch: 2800 Loss: 1.135006\n",
      "Train Epoch: 2 Batch: 2900 Loss: 0.742829\n",
      "Train Epoch: 2 Batch: 3000 Loss: 1.582676\n",
      "Train Epoch: 2 Batch: 3100 Loss: 1.247571\n",
      "Train Epoch: 3 Batch: 0 Loss: 1.527772\n",
      "Train Epoch: 3 Batch: 100 Loss: 1.415422\n",
      "Train Epoch: 3 Batch: 200 Loss: 1.121867\n",
      "Train Epoch: 3 Batch: 300 Loss: 1.596310\n",
      "Train Epoch: 3 Batch: 400 Loss: 1.250072\n",
      "Train Epoch: 3 Batch: 500 Loss: 1.563414\n",
      "Train Epoch: 3 Batch: 600 Loss: 1.439129\n",
      "Train Epoch: 3 Batch: 700 Loss: 1.295902\n",
      "Train Epoch: 3 Batch: 800 Loss: 1.036162\n",
      "Train Epoch: 3 Batch: 900 Loss: 1.375399\n",
      "Train Epoch: 3 Batch: 1000 Loss: 0.936955\n",
      "Train Epoch: 3 Batch: 1100 Loss: 1.060873\n",
      "Train Epoch: 3 Batch: 1200 Loss: 1.314372\n",
      "Train Epoch: 3 Batch: 1300 Loss: 1.414384\n",
      "Train Epoch: 3 Batch: 1400 Loss: 1.048405\n",
      "Train Epoch: 3 Batch: 1500 Loss: 1.067653\n",
      "Train Epoch: 3 Batch: 1600 Loss: 1.188313\n",
      "Train Epoch: 3 Batch: 1700 Loss: 0.808413\n",
      "Train Epoch: 3 Batch: 1800 Loss: 1.374330\n",
      "Train Epoch: 3 Batch: 1900 Loss: 1.305672\n",
      "Train Epoch: 3 Batch: 2000 Loss: 1.812016\n",
      "Train Epoch: 3 Batch: 2100 Loss: 1.569800\n",
      "Train Epoch: 3 Batch: 2200 Loss: 1.187870\n",
      "Train Epoch: 3 Batch: 2300 Loss: 1.315038\n",
      "Train Epoch: 3 Batch: 2400 Loss: 1.187927\n",
      "Train Epoch: 3 Batch: 2500 Loss: 1.376878\n",
      "Train Epoch: 3 Batch: 2600 Loss: 1.125712\n",
      "Train Epoch: 3 Batch: 2700 Loss: 0.662717\n",
      "Train Epoch: 3 Batch: 2800 Loss: 1.134891\n",
      "Train Epoch: 3 Batch: 2900 Loss: 0.742787\n",
      "Train Epoch: 3 Batch: 3000 Loss: 1.581501\n",
      "Train Epoch: 3 Batch: 3100 Loss: 1.247313\n",
      "Train Epoch: 4 Batch: 0 Loss: 1.525708\n",
      "Train Epoch: 4 Batch: 100 Loss: 1.415370\n",
      "Train Epoch: 4 Batch: 200 Loss: 1.121801\n",
      "Train Epoch: 4 Batch: 300 Loss: 1.596236\n",
      "Train Epoch: 4 Batch: 400 Loss: 1.249584\n",
      "Train Epoch: 4 Batch: 500 Loss: 1.563444\n",
      "Train Epoch: 4 Batch: 600 Loss: 1.439096\n",
      "Train Epoch: 4 Batch: 700 Loss: 1.295736\n",
      "Train Epoch: 4 Batch: 800 Loss: 1.036207\n",
      "Train Epoch: 4 Batch: 900 Loss: 1.375387\n",
      "Train Epoch: 4 Batch: 1000 Loss: 0.936936\n",
      "Train Epoch: 4 Batch: 1100 Loss: 1.060778\n",
      "Train Epoch: 4 Batch: 1200 Loss: 1.314515\n",
      "Train Epoch: 4 Batch: 1300 Loss: 1.412949\n",
      "Train Epoch: 4 Batch: 1400 Loss: 1.048283\n",
      "Train Epoch: 4 Batch: 1500 Loss: 1.067593\n",
      "Train Epoch: 4 Batch: 1600 Loss: 1.188315\n",
      "Train Epoch: 4 Batch: 1700 Loss: 0.808524\n",
      "Train Epoch: 4 Batch: 1800 Loss: 1.374369\n",
      "Train Epoch: 4 Batch: 1900 Loss: 1.305787\n",
      "Train Epoch: 4 Batch: 2000 Loss: 1.812207\n",
      "Train Epoch: 4 Batch: 2100 Loss: 1.570131\n",
      "Train Epoch: 4 Batch: 2200 Loss: 1.187927\n",
      "Train Epoch: 4 Batch: 2300 Loss: 1.314981\n",
      "Train Epoch: 4 Batch: 2400 Loss: 1.187983\n",
      "Train Epoch: 4 Batch: 2500 Loss: 1.376935\n",
      "Train Epoch: 4 Batch: 2600 Loss: 1.125711\n",
      "Train Epoch: 4 Batch: 2700 Loss: 0.662540\n",
      "Train Epoch: 4 Batch: 2800 Loss: 1.134789\n",
      "Train Epoch: 4 Batch: 2900 Loss: 0.742774\n",
      "Train Epoch: 4 Batch: 3000 Loss: 1.580723\n",
      "Train Epoch: 4 Batch: 3100 Loss: 1.247141\n"
     ]
    }
   ],
   "source": [
    "from tiledb.ml.data_apis.pytorch_sparse import PyTorchTileDBSparseDataset\n",
    "\n",
    "with tiledb.open('training_images') as x, tiledb.open('training_labels') as y:\n",
    "    tiledb_dataset = PyTorchTileDBSparseDataset(x_array=x, y_array=y, batch_size=32)\n",
    "    train_loader = torch.utils.data.DataLoader(tiledb_dataset, batch_size=None)\n",
    "\n",
    "    #Number of ratings x (user + movies)\n",
    "    datashape_x = (100000, 2625)\n",
    "\n",
    "    logre = LogisticRegression(shape=(2625, 1))\n",
    "    criterion = nn.MSELoss()\n",
    "    optimizer = optim.SGD(logre.parameters(), lr=0.01, momentum=0.5)\n",
    "\n",
    "    for epoch in range(1, 5):\n",
    "        logre.train()\n",
    "        for batch_idx, (inputs, labels) in enumerate(train_loader):\n",
    "            # zero the parameter gradients\n",
    "            optimizer.zero_grad()\n",
    "            # forward + backward + optimize\n",
    "            outputs = logre(inputs.to(torch.float))\n",
    "            loss = criterion(outputs, labels.type(torch.FloatTensor))\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            if batch_idx % 100 == 0:\n",
    "                print('Train Epoch: {} Batch: {} Loss: {:.6f}'.format(\n",
    "                epoch, batch_idx, loss.item()))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Garbage Collection\n",
    "We delete the RAW dataset `u.data`"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [],
   "source": [
    "os.remove(filename)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}