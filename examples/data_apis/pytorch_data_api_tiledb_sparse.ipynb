{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This example notebook shows how we can train a Factorization Machines classifier.\n",
    "Factorization Machines ([FMs](https://www.csie.ntu.edu.tw/~b97053/paper/Rendle2010FM.pdf)) are a general predictor, that are able to perform well in problems with high sparsity, like recommender systems.\n",
    "We employ TileDB as a storage engine for our training data and labels.\n",
    "We will use the MovieLens 100K public data set, available [here](https://grouplens.org/datasets/movielens/100k/). We will first download the\n",
    "MovieLens, which contains 100.000 ratings, by 943 users on 1682 items.\n",
    "Continuing, we will use our TileDB support for PyTorch Sparse Dataloader API in order to train an FM classifier.\n",
    "First, let's import what we need and download our data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import urllib.request\n",
    "import numpy as np\n",
    "import tiledb\n",
    "import torch\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset\n",
    "Download MovieLens dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "url = 'https://files.grouplens.org/datasets/movielens/ml-100k/u.data'\n",
    "filename = 'u.data'\n",
    "urllib.request.urlretrieve(url, filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "Use pandas to display dataset in readable form"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "data = pd.read_csv(filename, sep=\"\\t\", header=None, engine='python')\n",
    "data.columns = [\"user_id\", \"item_id\", \"rating\", \"timestamp\"]\n",
    "data.drop([\"timestamp\"], axis=1, inplace=True)\n",
    "display(data.head())\n",
    "display(data.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Data Analysis / Sparsity\n",
    "Before we apply the one-hot transformation let’s check the memory usage of our original data frame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "BYTES_TO_MB_DIV = 0.000001\n",
    "def print_memory_usage_of_data_frame(df):\n",
    "    mem = round(df.memory_usage().sum() * BYTES_TO_MB_DIV, 3)\n",
    "    print(\"Memory usage is \" + str(mem) + \" MB\")\n",
    "\n",
    "print_memory_usage_of_data_frame(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Data transformation\n",
    "\n",
    "Now, let’s apply the transformation and check the memory usage of the transformed data frame."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "data_one_hot = pd.get_dummies(data, columns=['user_id', 'item_id'])\n",
    "display(data_one_hot.head())\n",
    "display(data_one_hot.shape)\n",
    "print_memory_usage_of_data_frame(data_one_hot)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "We will slice the dataset `user_movie` will be our `x_train` data transformed with one-hot encoding.\n",
    "So we expect its schema to be the number of ratings as rows and binary columns for users + binary columns\n",
    "for each item (in our case movies). This will lead to (100000, 2625)\n",
    "\n",
    "The target data will be the `ratings`, which will include the ratings and thus will have a shape of (100000,1)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "user_movie = data_one_hot[data_one_hot.columns.difference(['rating'])]\n",
    "ratings = data['rating']\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Data Ingestion\n",
    "Then we proceed with ingesting `user_movies` into sparse TileDB arrays as our training data and `rating` into dense TileDB\n",
    "array as our target data. Here, we should point out that besides the\n",
    "flexibility of TileDB in defining a schema, i.e., multiple dimensions, multiple attributes, compression etc,\n",
    "we choose to define a simple schema. So, for a numpy array of D number of dimensions we create a dense TileDB array,\n",
    "with the same number of dimensions, and a single attribute of data type numpy float32."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def get_schema(data: np.array, batch_size: int, sparse: bool) -> tiledb.ArraySchema:\n",
    "    dims = [\n",
    "        tiledb.Dim(\n",
    "            name=\"dim_\" + str(dim),\n",
    "            domain=(0, data.shape[dim] - 1),\n",
    "            tile=data.shape[dim] if dim > 0 else batch_size,\n",
    "            dtype=np.int32,\n",
    "        )\n",
    "        for dim in range(data.ndim)\n",
    "    ]\n",
    "\n",
    "    # TileDB schema\n",
    "    schema = tiledb.ArraySchema(\n",
    "        domain=tiledb.Domain(*dims),\n",
    "        sparse=sparse,\n",
    "        attrs=[tiledb.Attr(name=\"features\", dtype=np.float32)],\n",
    "    )\n",
    "\n",
    "    return schema\n",
    "\n",
    "# Let's define an ingestion function\n",
    "def ingest_in_tiledb(data: np.array, batch_size: int, uri: str, sparse: bool):\n",
    "    schema = get_schema(data, batch_size, sparse)\n",
    "\n",
    "    # Create the (empty) array on disk.\n",
    "    tiledb.Array.create(uri, schema)\n",
    "\n",
    "    # Ingest\n",
    "    with tiledb.open(uri, \"w\") as tiledb_array:\n",
    "        idx = np.nonzero(data) if sparse else slice(None)\n",
    "        tiledb_array[idx] = {\"features\": data[idx]}"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "We transform our sliced/split data to numpy arrays, so we can ingest them in TileDB arrays"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "user_movie = user_movie.to_numpy()\n",
    "ratings = ratings.to_numpy()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "We ingest `user_movie` as sparse TileDB array and `ratings` as dense TileDB array"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#Ingest images\n",
    "ingest_in_tiledb(data=user_movie, batch_size=64, uri='training_images', sparse=True)\n",
    "\n",
    "# Ingest labels\n",
    "ingest_in_tiledb(data=ratings, batch_size=64, uri='training_labels', sparse=False)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now explore our TileDB arrays and check their structure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "user_movie_array = tiledb.open('training_images')\n",
    "ratings_array = tiledb.open('training_labels')\n",
    "\n",
    "print(user_movie_array.schema)\n",
    "print(ratings_array.schema)"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Model training\n",
    "Although we used Factorization Machines as a reference model to create\n",
    "our training set, here we will train a simple Logistic Regression model in Pytorch only\n",
    "for demonstration purposes. Anyone can easily build any Model to train on the data."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Declare Model Class"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "class LogisticRegression(nn.Module):\n",
    "    def __init__(self, shape):\n",
    "        super(LogisticRegression, self).__init__()\n",
    "        self.linear = torch.nn.Linear(shape[0], shape[1])\n",
    "\n",
    "    def forward(self, x):\n",
    "        outputs = self.linear(x)\n",
    "        return outputs"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Train the Model\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from tiledb.ml.data_apis.pytorch_sparse import PyTorchTileDBSparseDataset\n",
    "\n",
    "with tiledb.open('training_images') as x, tiledb.open('training_labels') as y:\n",
    "tiledb_dataset = PyTorchTileDBSparseDataset(x_array=x, y_array=y, batch_size=32)\n",
    "train_loader = torch.utils.data.DataLoader(tiledb_dataset, batch_size=None)\n",
    "\n",
    "#Number of ratings x (user + movies)\n",
    "datashape_x = (100000, 2625)\n",
    "\n",
    "logre = LogisticRegression(shape=(2625, 1))\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.SGD(logre.parameters(), lr=0.01, momentum=0.5)\n",
    "\n",
    "for epoch in range(1, 5):\n",
    "    logre.train()\n",
    "    for batch_idx, (inputs, labels) in enumerate(train_loader):\n",
    "        # zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "        # forward + backward + optimize\n",
    "        outputs = logre(inputs.to(torch.float))\n",
    "        loss = criterion(outputs, labels.type(torch.FloatTensor))\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if batch_idx % 100 == 0:\n",
    "            print('Train Epoch: {} Batch: {} Loss: {:.6f}'.format(\n",
    "            epoch, batch_idx, loss.item()))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Garbage Collection\n",
    "We delete the RAW dataset `u.data`"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "os.remove(filename)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}