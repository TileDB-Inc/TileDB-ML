{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This example notebook shows how we can train an [image/digit classification](https://pytorch.org/tutorials/beginner/nn_tutorial.html?highlight=mnist)\n",
    "model based on MNIST dataset, and store it as TileDB array. Firstly, let's import what we need."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "import tiledb\n",
    "import glob\n",
    "import json\n",
    "import pickle\n",
    "import os\n",
    "\n",
    "from pprint import pprint\n",
    "\n",
    "\n",
    "from tiledb.ml.models.pytorch import PyTorchTileDBModel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First let's define the parameters/hyperparameters we will need."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "<torch._C.Generator at 0x104dbe670>"
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "epochs = 1\n",
    "batch_size_train = 128\n",
    "learning_rate = 0.01\n",
    "momentum = 0.5\n",
    "log_interval = 10\n",
    "\n",
    "# Set random seeds for anything using random number generation\n",
    "random_seed = 1\n",
    "\n",
    "# Disable nondeterministic algorithms\n",
    "torch.backends.cudnn.enabled = False\n",
    "torch.manual_seed(random_seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We  will also need the DataLoaders API for the dataset. We will also employ TorchVision which let's as load the MNIST\n",
    "dataset in a handy way. We'll use a batch_size of 64 for training while the values 0.1307 and 0.3081 used for\n",
    "the Normalize() transformation below are the global mean and standard deviation of the MNIST dataset,\n",
    "we'll take them as a given here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\n",
      "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz to MNIST/raw/train-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "data": {
      "text/plain": "  0%|          | 0/9912422 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "f7e6ca9a8c3f47849d0721b86bf8ac34"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting MNIST/raw/train-images-idx3-ubyte.gz to MNIST/raw\n",
      "\n",
      "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz\n",
      "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz to MNIST/raw/train-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "data": {
      "text/plain": "  0%|          | 0/28881 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "01039f5f57c143969272404cb6bc7e8f"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting MNIST/raw/train-labels-idx1-ubyte.gz to MNIST/raw\n",
      "\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz to MNIST/raw/t10k-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "data": {
      "text/plain": "  0%|          | 0/1648877 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "e55e30e90abe4220b8c508f8d25296b1"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting MNIST/raw/t10k-images-idx3-ubyte.gz to MNIST/raw\n",
      "\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz to MNIST/raw/t10k-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "data": {
      "text/plain": "  0%|          | 0/4542 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "103fcf102f0b4efaac87850ffcbac953"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting MNIST/raw/t10k-labels-idx1-ubyte.gz to MNIST/raw\n",
      "\n"
     ]
    }
   ],
   "source": [
    "train_loader = torch.utils.data.DataLoader(\n",
    "  torchvision.datasets.MNIST('', train=True, download=True,\n",
    "                             transform=torchvision.transforms.Compose([\n",
    "                               torchvision.transforms.ToTensor(),\n",
    "                               torchvision.transforms.Normalize(\n",
    "                                 (0.1307,), (0.3081,))\n",
    "                             ])),\n",
    "  batch_size=batch_size_train, shuffle=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Moving on, we build our network. We'll use two 2-D convolutional layers followed by two fully-connected\n",
    "layers. As activation function we'll choose ReLUs and as a means of regularization we'll use two dropout layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 10, kernel_size=5)\n",
    "        self.conv2 = nn.Conv2d(10, 20, kernel_size=5)\n",
    "        self.conv2_drop = nn.Dropout2d()\n",
    "        self.fc1 = nn.Linear(320, 50)\n",
    "        self.fc2 = nn.Linear(50, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(F.max_pool2d(self.conv1(x), 2))\n",
    "        x = F.relu(F.max_pool2d(self.conv2_drop(self.conv2(x)), 2))\n",
    "        x = x.view(-1, 320)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.dropout(x, training=self.training)\n",
    "        x = self.fc2(x)\n",
    "        return F.log_softmax(x, dim = 1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will now initialise our Neural Network and optimizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "network = Net()\n",
    "optimizer = optim.SGD(network.parameters(), lr=learning_rate,\n",
    "                      momentum=momentum)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "We continue with the training loop and we iterate over all training data once per epoch. Loading the individual batches\n",
    "is handled by the DataLoader. We need to set the gradients to zero using optimizer.zero_grad() since PyTorch by default\n",
    "accumulates gradients. We then produce the output of the network (forward pass) and compute a negative log-likelihodd\n",
    "loss between the output and the ground truth label. The backward() call we now collect a new set of gradients which we\n",
    "propagate back into each of the network's parameters using optimizer.step()."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1 [0/60000 (0%)]\tLoss: 2.358812\n",
      "Train Epoch: 1 [1280/60000 (2%)]\tLoss: 2.285138\n",
      "Train Epoch: 1 [2560/60000 (4%)]\tLoss: 2.306635\n",
      "Train Epoch: 1 [3840/60000 (6%)]\tLoss: 2.270880\n",
      "Train Epoch: 1 [5120/60000 (9%)]\tLoss: 2.236740\n",
      "Train Epoch: 1 [6400/60000 (11%)]\tLoss: 2.243347\n",
      "Train Epoch: 1 [7680/60000 (13%)]\tLoss: 2.183255\n",
      "Train Epoch: 1 [8960/60000 (15%)]\tLoss: 2.148512\n",
      "Train Epoch: 1 [10240/60000 (17%)]\tLoss: 2.104912\n",
      "Train Epoch: 1 [11520/60000 (19%)]\tLoss: 2.004407\n",
      "Train Epoch: 1 [12800/60000 (21%)]\tLoss: 1.862252\n",
      "Train Epoch: 1 [14080/60000 (23%)]\tLoss: 1.884371\n",
      "Train Epoch: 1 [15360/60000 (26%)]\tLoss: 1.797316\n",
      "Train Epoch: 1 [16640/60000 (28%)]\tLoss: 1.687911\n",
      "Train Epoch: 1 [17920/60000 (30%)]\tLoss: 1.508046\n",
      "Train Epoch: 1 [19200/60000 (32%)]\tLoss: 1.764280\n",
      "Train Epoch: 1 [20480/60000 (34%)]\tLoss: 1.470073\n",
      "Train Epoch: 1 [21760/60000 (36%)]\tLoss: 1.351447\n",
      "Train Epoch: 1 [23040/60000 (38%)]\tLoss: 1.290582\n",
      "Train Epoch: 1 [24320/60000 (41%)]\tLoss: 1.017757\n",
      "Train Epoch: 1 [25600/60000 (43%)]\tLoss: 1.042162\n",
      "Train Epoch: 1 [26880/60000 (45%)]\tLoss: 1.098766\n",
      "Train Epoch: 1 [28160/60000 (47%)]\tLoss: 1.228552\n",
      "Train Epoch: 1 [29440/60000 (49%)]\tLoss: 1.149593\n",
      "Train Epoch: 1 [30720/60000 (51%)]\tLoss: 0.845248\n",
      "Train Epoch: 1 [32000/60000 (53%)]\tLoss: 0.974113\n",
      "Train Epoch: 1 [33280/60000 (55%)]\tLoss: 0.856906\n",
      "Train Epoch: 1 [34560/60000 (58%)]\tLoss: 0.923459\n",
      "Train Epoch: 1 [35840/60000 (60%)]\tLoss: 1.021857\n",
      "Train Epoch: 1 [37120/60000 (62%)]\tLoss: 0.806954\n",
      "Train Epoch: 1 [38400/60000 (64%)]\tLoss: 0.878951\n",
      "Train Epoch: 1 [39680/60000 (66%)]\tLoss: 0.818505\n",
      "Train Epoch: 1 [40960/60000 (68%)]\tLoss: 0.805543\n",
      "Train Epoch: 1 [42240/60000 (70%)]\tLoss: 0.823152\n",
      "Train Epoch: 1 [43520/60000 (72%)]\tLoss: 0.854361\n",
      "Train Epoch: 1 [44800/60000 (75%)]\tLoss: 0.774645\n",
      "Train Epoch: 1 [46080/60000 (77%)]\tLoss: 0.718349\n",
      "Train Epoch: 1 [47360/60000 (79%)]\tLoss: 0.543338\n",
      "Train Epoch: 1 [48640/60000 (81%)]\tLoss: 0.759377\n",
      "Train Epoch: 1 [49920/60000 (83%)]\tLoss: 0.654922\n",
      "Train Epoch: 1 [51200/60000 (85%)]\tLoss: 0.699930\n",
      "Train Epoch: 1 [52480/60000 (87%)]\tLoss: 0.805351\n",
      "Train Epoch: 1 [53760/60000 (90%)]\tLoss: 0.790734\n",
      "Train Epoch: 1 [55040/60000 (92%)]\tLoss: 0.759933\n",
      "Train Epoch: 1 [56320/60000 (94%)]\tLoss: 0.540410\n",
      "Train Epoch: 1 [57600/60000 (96%)]\tLoss: 0.641233\n",
      "Train Epoch: 1 [58880/60000 (98%)]\tLoss: 0.659374\n"
     ]
    }
   ],
   "source": [
    "train_losses = []\n",
    "train_counter = []\n",
    "\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "writer = SummaryWriter()\n",
    "\n",
    "\n",
    "def train(epoch):\n",
    "  network.train()\n",
    "  for batch_idx, (data, target) in enumerate(train_loader):\n",
    "    optimizer.zero_grad()\n",
    "    output = network(data)\n",
    "    loss = F.nll_loss(output, target)\n",
    "    writer.add_scalar(\"Loss/train\", loss, epoch)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    if batch_idx % log_interval == 0:\n",
    "      print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "        epoch, batch_idx * len(data), len(train_loader.dataset),\n",
    "        100. * batch_idx / len(train_loader), loss.item()))\n",
    "      train_losses.append(loss.item())\n",
    "      train_counter.append(\n",
    "        (batch_idx*64) + ((epoch-1)*len(train_loader.dataset)))\n",
    "\n",
    "for epoch in range(1, epochs + 1):\n",
    "  train(epoch)\n",
    "writer.flush()\n",
    "writer.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now save the trained model as a TileDB array. In case we want to train  the model further in a later time, we can also save\n",
    "the optimizer in our TileDB array. In case we will use our model only for inference, we don't have to save the optimizer and we\n",
    "only keep the model. We first declare a PytTorchTileDB object and initialize it with the corresponding TileDB uri, model and optimizer,\n",
    "and then save the model as a TileDB array. Finally, we can save any kind of metadata (in any structure, i.e., list, tuple or dictionary)\n",
    "by passing a dictionary to the meta attribute."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "runs/May09_11-59-08_ktsitsi/events.out.tfevents.1652086751.ktsitsi.58263.0\n",
      "{'epochs': 1, 'train_loss': [2.358812093734741, 2.285137891769409, 2.3066349029541016, 2.2708795070648193, 2.2367401123046875, 2.24334716796875, 2.1832549571990967, 2.1485116481781006, 2.1049115657806396, 2.0044069290161133, 1.8622523546218872, 1.8843708038330078, 1.7973158359527588, 1.6879109144210815, 1.508046269416809, 1.764279842376709, 1.4700727462768555, 1.3514467477798462, 1.2905819416046143, 1.0177571773529053, 1.042162299156189, 1.0987662076950073, 1.2285516262054443, 1.1495932340621948, 0.8452475070953369, 0.9741130471229553, 0.8569056987762451, 0.9234588146209717, 1.0218565464019775, 0.8069543242454529, 0.8789511919021606, 0.8185049891471863, 0.8055434226989746, 0.8231522440910339, 0.8543609976768494, 0.7746452689170837, 0.718348503112793, 0.5433375239372253, 0.7593768239021301, 0.65492182970047, 0.6999298930168152, 0.8053513765335083, 0.790733814239502, 0.7599329948425293, 0.540409505367279, 0.6412327885627747, 0.6593738198280334]}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/konstantinostsitsimpikos/tileroot/TileDB-ML/venv2/lib/python3.9/site-packages/tiledb/ctx.py:28: UserWarning: tiledb.default_ctx and scope_ctx will not function correctly due to bug in IPython contextvar support.  You must supply a Ctx object to each function for custom configuration options. Please consider upgrading to ipykernel >= 6!Please see https://github.com/TileDB-Inc/TileDB-Py/issues/667 for more information.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "tiledb_model_1 = PyTorchTileDBModel(uri='tiledb-pytorch-mnist-1', model=network, optimizer=optimizer)\n",
    "\n",
    "tiledb_model_1.save(update=False,\n",
    "                    meta={'epochs': epochs,\n",
    "                          'train_loss': train_losses},\n",
    "                    summary_writer=writer)\n",
    "tiledb_model_1.load_tensorboard()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "The above step will create a TileDB array in your working directory. For information about the structure of a dense\n",
    "TileDB array in terms of files on disk please take a look [here](https://docs.tiledb.com/main/concepts/data-format).\n",
    "Let's open our TileDB array model and check metadata. Metadata that are of type list, dict or tuple have been JSON\n",
    "serialized while saving, i.e., we need json.loads to deserialize them."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['tiledb-pytorch-mnist-1/__meta',\n",
      " 'tiledb-pytorch-mnist-1/__fragment_meta',\n",
      " 'tiledb-pytorch-mnist-1/__commits',\n",
      " 'tiledb-pytorch-mnist-1/__schema',\n",
      " 'tiledb-pytorch-mnist-1/__fragments']\n",
      "\n",
      "Key: TILEDB_ML_MODEL_ML_FRAMEWORK, Value: PYTORCH\n",
      "Key: TILEDB_ML_MODEL_ML_FRAMEWORK_VERSION, Value: 1.10.2\n",
      "Key: TILEDB_ML_MODEL_PREVIEW, Value: Net(\n",
      "  (conv1): Conv2d(1, 10, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2): Conv2d(10, 20, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2_drop): Dropout2d(p=0.5, inplace=False)\n",
      "  (fc1): Linear(in_features=320, out_features=50, bias=True)\n",
      "  (fc2): Linear(in_features=50, out_features=10, bias=True)\n",
      ")\n",
      "Key: TILEDB_ML_MODEL_PYTHON_VERSION, Value: 3.9.5\n",
      "Key: TILEDB_ML_MODEL_STAGE, Value: STAGING\n",
      "Key: __TENSORBOARD__, Value: b'\\x80\\x04\\x95oX\\x00\\x00\\x00\\x00\\x00\\x00}\\x94\\x8cJruns/May09_11-37-14_ktsitsi/events.out.tfevents.1652085437.ktsitsi.58013.0\\x94B\\x18X\\x00\\x00\\x18\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\xa3\\x7fK\"\\t\\x8d~A\\xaf4\\x9e\\xd8A\\x1a\\rbrain.Event:2\\x85\\xa4\\xba) \\x00\\x00\\x00\\x00\\x00\\x00\\x00)\\xed\\xa9P\\t\\xa5\\x88C\\xaf4\\x9e\\xd8A\\x10\\x01*\\x13\\n\\x11\\n\\nLoss/train\\x15\\xc7\\xf6\\x16@\\xbb\\xb4\\xae\\x92 \\x00\\x00\\x00\\x00\\x00\\x00\\x00)\\xed\\xa9P\\tj\\x89F\\xaf4\\x9e\\xd8A\\x10\\x01*\\x13\\n\\x11\\n\\nLoss/train\\x15Qk\\x14@\\xb0(I\\xa5 \\x00\\x00\\x00\\x00\\x00\\x00\\x00)\\xed\\xa9P\\t\\x07>I\\xaf4\\x9e\\xd8A\\x10\\x01*\\x13\\n\\x11\\n\\nLoss/train\\x155\\xee\\x12@6\\xc0\\x1a\\x9d \\x00\\x00\\x00\\x00\\x00\\x00\\x00)\\xed\\xa9P\\tN\\xcfK\\xaf4\\x9e\\xd8A\\x10\\x01*\\x13\\n\\x11\\n\\nLoss/train\\x15%|\\x14@sN\\x90< \\x00\\x00\\x00\\x00\\x00\\x00\\x00)\\xed\\xa9P\\t\\x89\\nN\\xaf4\\x9e\\xd8A\\x10\\x01*\\x13\\n\\x11\\n\\nLoss/train\\x15\\xf0\\xc6\\x16@\\x01\\n\\xa9P \\x00\\x00\\x00\\x00\\x00\\x00\\x00)\\xed\\xa9P\\tnMP\\xaf4\\x9e\\xd8A\\x10\\x01*\\x13\\n\\x11\\n\\nLoss/train\\x15@H\\x16@=p\\xc7& \\x00\\x00\\x00\\x00\\x00\\x00\\x00)\\xed\\xa9P\\t\\x89\\xe9R\\xaf4\\x9e\\xd8A\\x10\\x01*\\x13\\n\\x11\\n\\nLoss/train\\x15\\xd52\\x14@\\x88+\\xa7r \\x00\\x00\\x00\\x00\\x00\\x00\\x00)\\xed\\xa9P\\t\\r\\xc4U\\xaf4\\x9e\\xd8A\\x10\\x01*\\x13\\n\\x11\\n\\nLoss/train\\x15\\xa2)\\x14@\\x97\\xea\\xe8X \\x00\\x00\\x00\\x00\\x00\\x00\\x00)\\xed\\xa9P\\t\\x96\\xb2X\\xaf4\\x9e\\xd8A\\x10\\x01*\\x13\\n\\x11\\n\\nLoss/train\\x15\\xe6k\\x13@\\'\\xb7\\xbel \\x00\\x00\\x00\\x00\\x00\\x00\\x00)\\xed\\xa9P\\t\\x1fa[\\xaf4\\x9e\\xd8A\\x10\\x01*\\x13\\n\\x11\\n\\nLoss/train\\x15\\x02\\xd4\\x15@\\xea\\x89\\xa1\\x91 \\x00\\x00\\x00\\x00\\x00\\x00\\x00)\\xed\\xa9P\\t\\xf3 ^\\xaf4\\x9e\\xd8A\\x10\\x01*\\x13\\n\\x11\\n\\nLoss/train\\x15\\xb3?\\x12@c\\xd8?\\xa9 \\x00\\x00\\x00\\x00\\x00\\x00\\x00)\\xed\\xa9P\\t\\x92\\x01a\\xaf4\\x9e\\xd8A\\x10\\x01*\\x13\\n\\x11\\n\\nLoss/train\\x15#\\xa9\\x14@\\'\\t\\x84V \\x00\\x00\\x00\\x00\\x00\\x00\\x00)\\xed\\xa9P\\t\\xfcnc\\xaf4\\x9e\\xd8A\\x10\\x01*\\x13\\n\\x11\\n\\nLoss/train\\x15%\\xd1\\x13@\\xb5\\x01Y\\x0b \\x00\\x00\\x00\\x00\\x00\\x00\\x00)\\xed\\xa9P\\tu\\xb0e\\xaf4\\x9e\\xd8A\\x10\\x01*\\x13\\n\\x11\\n\\nLoss/train\\x15\\xf9i\\x14@\\x0evd\\xfd \\x00\\x00\\x00\\x00\\x00\\x00\\x00)\\xed\\xa9P\\t^\\xd7g\\xaf4\\x9e\\xd8A\\x10\\x01*\\x13\\n\\x11\\n\\nLoss/train\\x15\\xce\\xda\\x12@y\\x9d\\xe3\\xbb \\x00\\x00\\x00\\x00\\x00\\x00\\x00)\\xed\\xa9P\\t\\x02\\x12j\\xaf4\\x9e\\xd8A\\x10\\x01*\\x13\\n\\x11\\n\\nLoss/train\\x15;h\\x13@\\xd7p\\xb8c \\x00\\x00\\x00\\x00\\x00\\x00\\x00)\\xed\\xa9P\\t{kl\\xaf4\\x9e\\xd8A\\x10\\x01*\\x13\\n\\x11\\n\\nLoss/train\\x15\\x08\\xf7\\x12@\\x8d\\xad;) \\x00\\x00\\x00\\x00\\x00\\x00\\x00)\\xed\\xa9P\\t\\r\\xe0n\\xaf4\\x9e\\xd8A\\x10\\x01*\\x13\\n\\x11\\n\\nLoss/train\\x15\\xfb\\xe4\\x13@\\x03H?\\xa1 \\x00\\x00\\x00\\x00\\x00\\x00\\x00)\\xed\\xa9P\\t\\xc2Qq\\xaf4\\x9e\\xd8A\\x10\\x01*\\x13\\n\\x11\\n\\nLoss/train\\x15D\\xb0\\x12@9\\x8c\\x12c \\x00\\x00\\x00\\x00\\x00\\x00\\x00)\\xed\\xa9P\\t\\xe6\\xb3s\\xaf4\\x9e\\xd8A\\x10\\x01*\\x13\\n\\x11\\n\\nLoss/train\\x15\\xd5\\xdc\\x12@\\xb6\\x02\\xc5^ \\x00\\x00\\x00\\x00\\x00\\x00\\x00)\\xed\\xa9P\\t\\x9d/v\\xaf4\\x9e\\xd8A\\x10\\x01*\\x13\\n\\x11\\n\\nLoss/train\\x15\\xe8\\x9f\\x13@\\x0c\\x18\\x14? \\x00\\x00\\x00\\x00\\x00\\x00\\x00)\\xed\\xa9P\\tL2y\\xaf4\\x9e\\xd8A\\x10\\x01*\\x13\\n\\x11\\n\\nLoss/train\\x15\\xb3\\xe3\\x12@5\\xaa\\x13\\xe0 \\x00\\x00\\x00\\x00\\x00\\x00\\x00)\\xed\\xa9P\\t\\xd0\\xb8{\\xaf4\\x9e\\xd8A\\x10\\x01*\\x13\\n\\x11\\n\\nLoss/train\\x15\\x1a\\x90\\x12@r\\xca4# \\x00\\x00\\x00\\x00\\x00\\x00\\x00)\\xed\\xa9P\\t\\x81\\xec}\\xaf4\\x9e\\xd8A\\x10\\x01*\\x13\\n\\x11\\n\\nLoss/train\\x15\\xa0\\xca\\x11@\\xa8\\xa7\\x1cx \\x00\\x00\\x00\\x00\\x00\\x00\\x00)\\xed\\xa9P\\t(d\\x80\\xaf4\\x9e\\xd8A\\x10\\x01*\\x13\\n\\x11\\n\\nLoss/train\\x15n\\xe7\\x10@C\\x04\\xa0  \\x00\\x00\\x00\\x00\\x00\\x00\\x00)\\xed\\xa9P\\t\\xe4\\xd8\\x82\\xaf4\\x9e\\xd8A\\x10\\x01*\\x13\\n\\x11\\n\\nLoss/train\\x15\\xef\\xeb\\x12@c\\xbb\\xb7I \\x00\\x00\\x00\\x00\\x00\\x00\\x00)\\xed\\xa9P\\t\\xbb&\\x85\\xaf4\\x9e\\xd8A\\x10\\x01*\\x13\\n\\x11\\n\\nLoss/train\\x15\\x96\\xdd\\x11@\\xe5A\\xa0\\x9e \\x00\\x00\\x00\\x00\\x00\\x00\\x00)\\xed\\xa9P\\t\\x10\\xcc\\x87\\xaf4\\x9e\\xd8A\\x10\\x01*\\x13\\n\\x11\\n\\nLoss/train\\x15\\xe9\\xd6\\x10@\\xb4FF~ \\x00\\x00\\x00\\x00\\x00\\x00\\x00)\\xed\\xa9P\\t\\xd4\\xb7\\x8a\\xaf4\\x9e\\xd8A\\x10\\x01*\\x13\\n\\x11\\n\\nLoss/train\\x15\\xbcY\\x11@\\xeb_ql \\x00\\x00\\x00\\x00\\x00\\x00\\x00)\\xed\\xa9P\\t\\xb7%\\x8d\\xaf4\\x9e\\xd8A\\x10\\x01*\\x13\\n\\x11\\n\\nLoss/train\\x15uC\\x10@(\\x92\\'\\xdf \\x00\\x00\\x00\\x00\\x00\\x00\\x00)\\xed\\xa9P\\t\\xb5\\xe4\\x8f\\xaf4\\x9e\\xd8A\\x10\\x01*\\x13\\n\\x11\\n\\nLoss/train\\x15\\x17V\\x11@\\xafr\\xa5\\x8a \\x00\\x00\\x00\\x00\\x00\\x00\\x00)\\xed\\xa9P\\t#\\xbc\\x92\\xaf4\\x9e\\xd8A\\x10\\x01*\\x13\\n\\x11\\n\\nLoss/train\\x15r\\xcd\\x12@\\xce\\x83x\" \\x00\\x00\\x00\\x00\\x00\\x00\\x00)\\xed\\xa9P\\t\\x13)\\x95\\xaf4\\x9e\\xd8A\\x10\\x01*\\x13\\n\\x11\\n\\nLoss/train\\x15\\xaa\\x94\\x11@\\x8cE\"> \\x00\\x00\\x00\\x00\\x00\\x00\\x00)\\xed\\xa9P\\t\\xf6`\\x97\\xaf4\\x9e\\xd8A\\x10\\x01*\\x13\\n\\x11\\n\\nLoss/train\\x15!\\x87\\x10@\\xa6f\\xba\\xf5 \\x00\\x00\\x00\\x00\\x00\\x00\\x00)\\xed\\xa9P\\tb\\xbe\\x99\\xaf4\\x9e\\xd8A\\x10\\x01*\\x13\\n\\x11\\n\\nLoss/train\\x15\\x12\\xa6\\x10@\\xd1\\x88\\x8aZ \\x00\\x00\\x00\\x00\\x00\\x00\\x00)\\xed\\xa9P\\tO\\xeb\\x9b\\xaf4\\x9e\\xd8A\\x10\\x01*\\x13\\n\\x11\\n\\nLoss/train\\x15\\xe4z\\x0f@%\\xcfGR \\x00\\x00\\x00\\x00\\x00\\x00\\x00)\\xed\\xa9P\\t\\x941\\x9e\\xaf4\\x9e\\xd8A\\x10\\x01*\\x13\\n\\x11\\n\\nLoss/train\\x15\\x97o\\x10@/\\xccq\\xce \\x00\\x00\\x00\\x00\\x00\\x00\\x00)\\xed\\xa9P\\t\\xe7\\xe0\\xa0\\xaf4\\x9e\\xd8A\\x10\\x01*\\x13\\n\\x11\\n\\nLoss/train\\x15|5\\x10@|\\xde,\\x15 \\x00\\x00\\x00\\x00\\x00\\x00\\x00)\\xed\\xa9P\\t`\\xcc\\xa3\\xaf4\\x9e\\xd8A\\x10\\x01*\\x13\\n\\x11\\n\\nLoss/train\\x15u\\x1e\\x11@\\xd4\\x88\\x053 \\x00\\x00\\x00\\x00\\x00\\x00\\x00)\\xed\\xa9P\\tB_\\xa6\\xaf4\\x9e\\xd8A\\x10\\x01*\\x13\\n\\x11\\n\\nLoss/train\\x15\\'g\\x0f@\\x10\\xaa\\xf8z \\x00\\x00\\x00\\x00\\x00\\x00\\x00)\\xed\\xa9P\\tN\\n\\xa9\\xaf4\\x9e\\xd8A\\x10\\x01*\\x13\\n\\x11\\n\\nLoss/train\\x15\\xc0&\\x0f@\\xa4DNo \\x00\\x00\\x00\\x00\\x00\\x00\\x00)\\xed\\xa9P\\t\\xe8\\x19\\xac\\xaf4\\x9e\\xd8A\\x10\\x01*\\x13\\n\\x11\\n\\nLoss/train\\x15Qk\\x11@\\x8bj\\xd3? \\x00\\x00\\x00\\x00\\x00\\x00\\x00)\\xed\\xa9P\\t\\xf3\\xad\\xae\\xaf4\\x9e\\xd8A\\x10\\x01*\\x13\\n\\x11\\n\\nLoss/train\\x15\\x16V\\x0e@ONz\\x18 \\x00\\x00\\x00\\x00\\x00\\x00\\x00)\\xed\\xa9P\\tI\\xf2\\xb0\\xaf4\\x9e\\xd8A\\x10\\x01*\\x13\\n\\x11\\n\\nLoss/train\\x15\\xf0\\xfc\\x10@\\x1fg\\x8c\\xc1 \\x00\\x00\\x00\\x00\\x00\\x00\\x00)\\xed\\xa9P\\t\\xa6\\x7f\\xb3\\xaf4\\x9e\\xd8A\\x10\\x01*\\x13\\n\\x11\\n\\nLoss/train\\x15\\xe1\\xe5\\x0e@\\r\\xc5\\x80l \\x00\\x00\\x00\\x00\\x00\\x00\\x00)\\xed\\xa9P\\t\\xed\\xef\\xb5\\xaf4\\x9e\\xd8A\\x10\\x01*\\x13\\n\\x11\\n\\nLoss/train\\x15\\x97\\xc1\\x0e@\\xfc\\x9doA \\x00\\x00\\x00\\x00\\x00\\x00\\x00)\\xed\\xa9P\\t\\x85>\\xb8\\xaf4\\x9e\\xd8A\\x10\\x01*\\x13\\n\\x11\\n\\nLoss/train\\x15\\x85\\xd9\\x10@\\\\\\xc8?\\x17 \\x00\\x00\\x00\\x00\\x00\\x00\\x00)\\xed\\xa9P\\t;\\x1f\\xbb\\xaf4\\x9e\\xd8A\\x10\\x01*\\x13\\n\\x11\\n\\nLoss/train\\x15\\xdf^\\x10@\\x13\\x80\\xfc\\x1e \\x00\\x00\\x00\\x00\\x00\\x00\\x00)\\xed\\xa9P\\t\\xc7\\xbb\\xbd\\xaf4\\x9e\\xd8A\\x10\\x01*\\x13\\n\\x11\\n\\nLoss/train\\x15\\xcdu\\x0f@\\xa4\\x08\\xd7\\xa2 \\x00\\x00\\x00\\x00\\x00\\x00\\x00)\\xed\\xa9P\\tH/\\xc0\\xaf4\\x9e\\xd8A\\x10\\x01*\\x13\\n\\x11\\n\\nLoss/train\\x15n\\xe5\\x0f@\\xa4_\\xae\\x99 \\x00\\x00\\x00\\x00\\x00\\x00\\x00)\\xed\\xa9P\\t\\n\\x83\\xc3\\xaf4\\x9e\\xd8A\\x10\\x01*\\x13\\n\\x11\\n\\nLoss/train\\x15\\x00\\x93\\x0f@\\xfb\\xb5\\x89\\xb2 \\x00\\x00\\x00\\x00\\x00\\x00\\x00)\\xed\\xa9P\\tRd\\xc6\\xaf4\\x9e\\xd8A\\x10\\x01*\\x13\\n\\x11\\n\\nLoss/train\\x15&\\x87\\x0f@\\x9c\\xd2\\xd4\\xb8 \\x00\\x00\\x00\\x00\\x00\\x00\\x00)\\xed\\xa9P\\t\\xf8\\x16\\xc9\\xaf4\\x9e\\xd8A\\x10\\x01*\\x13\\n\\x11\\n\\nLoss/train\\x15T\\x14\\x0b@8|@\\xd4 \\x00\\x00\\x00\\x00\\x00\\x00\\x00)\\xed\\xa9P\\tV\\x96\\xcb\\xaf4\\x9e\\xd8A\\x10\\x01*\\x13\\n\\x11\\n\\nLoss/train\\x15\\xb7\\xfa\\x0c@\\xaf\\xa7:\\x91 \\x00\\x00\\x00\\x00\\x00\\x00\\x00)\\xed\\xa9P\\tQ3\\xce\\xaf4\\x9e\\xd8A\\x10\\x01*\\x13\\n\\x11\\n\\nLoss/train\\x15\\rH\\x10@J\\x04mM \\x00\\x00\\x00\\x00\\x00\\x00\\x00)\\xed\\xa9P\\t\\x00\\xa5\\xd0\\xaf4\\x9e\\xd8A\\x10\\x01*\\x13\\n\\x11\\n\\nLoss/train\\x15\\r\\x0e\\x10@\\xc3\\\\\\x00\\x8a \\x00\\x00\\x00\\x00\\x00\\x00\\x00)\\xed\\xa9P\\t\\xc7\\x0e\\xd3\\xaf4\\x9e\\xd8A\\x10\\x01*\\x13\\n\\x11\\n\\nLoss/train\\x15\\xab\\xd9\\r@e9\\xcfr \\x00\\x00\\x00\\x00\\x00\\x00\\x00)\\xed\\xa9P\\t\\xa4\\xbe\\xd5\\xaf4\\x9e\\xd8A\\x10\\x01*\\x13\\n\\x11\\n\\nLoss/train\\x15t\\xdd\\x0b@\\xc0\\xab\\xcc2 \\x00\\x00\\x00\\x00\\x00\\x00\\x00)\\xed\\xa9P\\t\\x93\\x8d\\xd8\\xaf4\\x9e\\xd8A\\x10\\x01*\\x13\\n\\x11\\n\\nLoss/train\\x15\\xbb\\xb6\\x0b@\\xc6\\xecK\\x92 \\x00\\x00\\x00\\x00\\x00\\x00\\x00)\\xed\\xa9P\\t\\xdf\\x1e\\xdb\\xaf4\\x9e\\xd8A\\x10\\x01*\\x13\\n\\x11\\n\\nLoss/train\\x15\\xd1b\\r@_\\x10\\xb6\\x8d \\x00\\x00\\x00\\x00\\x00\\x00\\x00)\\xed\\xa9P\\t\\xbb\\r\\xde\\xaf4\\x9e\\xd8A\\x10\\x01*\\x13\\n\\x11\\n\\nLoss/train\\x15s\\xba\\x0b@!\\xef0s \\x00\\x00\\x00\\x00\\x00\\x00\\x00)\\xed\\xa9P\\tY\\xa4\\xe0\\xaf4\\x9e\\xd8A\\x10\\x01*\\x13\\n\\x11\\n\\nLoss/train\\x151>\\n@NR\\n\\x8b \\x00\\x00\\x00\\x00\\x00\\x00\\x00)\\xed\\xa9P\\tt\\xe9\\xe2\\xaf4\\x9e\\xd8A\\x10\\x01*\\x13\\n\\x11\\n\\nLoss/train\\x15\\x81\\n\\r@\\xe43\\\\[ \\x00\\x00\\x00\\x00\\x00\\x00\\x00)\\xed\\xa9P\\t\\x8bQ\\xe5\\xaf4\\x9e\\xd8A\\x10\\x01*\\x13\\n\\x11\\n\\nLoss/train\\x15\\xd0E\\x0c@c\\xee\\x10\\xb4 \\x00\\x00\\x00\\x00\\x00\\x00\\x00)\\xed\\xa9P\\t3\\x8c\\xe7\\xaf4\\x9e\\xd8A\\x10\\x01*\\x13\\n\\x11\\n\\nLoss/train\\x15\\x1e0\\x0f@\\xd3h`\\xf1 \\x00\\x00\\x00\\x00\\x00\\x00\\x00)\\xed\\xa9P\\t&\\xc2\\xe9\\xaf4\\x9e\\xd8A\\x10\\x01*\\x13\\n\\x11\\n\\nLoss/train\\x15q\\xe3\\t@\\xebGtb \\x00\\x00\\x00\\x00\\x00\\x00\\x00)\\xed\\xa9P\\t\\x8d%\\xec\\xaf4\\x9e\\xd8A\\x10\\x01*\\x13\\n\\x11\\n\\nLoss/train\\x15O\\xd0\\x07@\\x0fP\\xa8W \\x00\\x00\\x00\\x00\\x00\\x00\\x00)\\xed\\xa9P\\tM\\xda\\xee\\xaf4\\x9e\\xd8A\\x10\\x01*\\x13\\n\\x11\\n\\nLoss/train\\x15;\\n\\x0c@\\xfc_\\xf9\\x9e \\x00\\x00\\x00\\x00\\x00\\x00\\x00)\\xed\\xa9P\\t\\xc3\\xb7\\xf1\\xaf4\\x9e\\xd8A\\x10\\x01*\\x13\\n\\x11\\n\\nLoss/train\\x15eI\\t@]\\xad\\xb10 \\x00\\x00\\x00\\x00\\x00\\x00\\x00)\\xed\\xa9P\\t9&\\xf4\\xaf4\\x9e\\xd8A\\x10\\x01*\\x13\\n\\x11\\n\\nLoss/train\\x15\\xa8\\x98\\r@=\\x84Z\\x89 \\x00\\x00\\x00\\x00\\x00\\x00\\x00)\\xed\\xa9P\\t\\xfd\\x88\\xf6\\xaf4\\x9e\\xd8A\\x10\\x01*\\x13\\n\\x11\\n\\nLoss/train\\x157\\x81\\t@\\xc0\\xca\\x87\\x1f \\x00\\x00\\x00\\x00\\x00\\x00\\x00)\\xed\\xa9P\\tT\\x8a\\xf9\\xaf4\\x9e\\xd8A\\x10\\x01*\\x13\\n\\x11\\n\\nLoss/train\\x15\\r\\xcd\\x0b@d\\xf9\\xf6\\x8b \\x00\\x00\\x00\\x00\\x00\\x00\\x00)\\xed\\xa9P\\t\\xb2*\\xfc\\xaf4\\x9e\\xd8A\\x10\\x01*\\x13\\n\\x11\\n\\nLoss/train\\x15\\x94\\xcb\\x08@\\xc2\\xbe\\x04\\x84 \\x00\\x00\\x00\\x00\\x00\\x00\\x00)\\xed\\xa9P\\t6\\x95\\xfe\\xaf4\\x9e\\xd8A\\x10\\x01*\\x13\\n\\x11\\n\\nLoss/train\\x15\\xf6\\xd8\\x06@t)G\\xaa \\x00\\x00\\x00\\x00\\x00\\x00\\x00)\\xed\\xa9P\\t\\xba\\xd6\\x00\\xb04\\x9e\\xd8A\\x10\\x01*\\x13\\n\\x11\\n\\nLoss/train\\x15z\\xc2\\n@\\xafFRo \\x00\\x00\\x00\\x00\\x00\\x00\\x00)\\xed\\xa9P\\t\\xfa\\x08\\x03\\xb04\\x9e\\xd8A\\x10\\x01*\\x13\\n\\x11\\n\\nLoss/train\\x15J{\\n@\\x9aw1\\xe5 \\x00\\x00\\x00\\x00\\x00\\x00\\x00)\\xed\\xa9P\\t\\xb5\\x87\\x05\\xb04\\x9e\\xd8A\\x10\\x01*\\x13\\n\\x11\\n\\nLoss/train\\x15\\xb6D\\n@\\xa1YS] \\x00\\x00\\x00\\x00\\x00\\x00\\x00)\\xed\\xa9P\\ta\\x13\\x08\\xb04\\x9e\\xd8A\\x10\\x01*\\x13\\n\\x11\\n\\nLoss/train\\x15vo\\x08@\\x82\\x10\\x1f> \\x00\\x00\\x00\\x00\\x00\\x00\\x00)\\xed\\xa9P\\tF\\xd0\\n\\xb04\\x9e\\xd8A\\x10\\x01*\\x13\\n\\x11\\n\\nLoss/train\\x15\\xee\\xe6\\x07@\\xeaQ\\xfe\\xef \\x00\\x00\\x00\\x00\\x00\\x00\\x00)\\xed\\xa9P\\t\\x19o\\r\\xb04\\x9e\\xd8A\\x10\\x01*\\x13\\n\\x11\\n\\nLoss/train\\x15\\xd4C\\x08@r\\x10-\\xd9 \\x00\\x00\\x00\\x00\\x00\\x00\\x00)\\xed\\xa9P\\t\\xec\\x17\\x10\\xb04\\x9e\\xd8A\\x10\\x01*\\x13\\n\\x11\\n\\nLoss/train\\x15\\xdf\\xb6\\x06@\\xa0\\t\\xc0\\xec \\x00\\x00\\x00\\x00\\x00\\x00\\x00)\\xed\\xa9P\\tM\\xd5\\x12\\xb04\\x9e\\xd8A\\x10\\x01*\\x13\\n\\x11\\n\\nLoss/train\\x15\\x04\\xa4\\x05@\\xc6\\x17\\x88\\xe5 \\x00\\x00\\x00\\x00\\x00\\x00\\x00)\\xed\\xa9P\\t\\xadK\\x15\\xb04\\x9e\\xd8A\\x10\\x01*\\x13\\n\\x11\\n\\nLoss/train\\x15\\x08\\x01\\x06@\\xa6B\\xdb\\x98 \\x00\\x00\\x00\\x00\\x00\\x00\\x00)\\xed\\xa9P\\thw\\x17\\xb04\\x9e\\xd8A\\x10\\x01*\\x13\\n\\x11\\n\\nLoss/train\\x15\\xc1\\xae\\x06@t\\xe9a\\x8f \\x00\\x00\\x00\\x00\\x00\\x00\\x00)\\xed\\xa9P\\t#\\xa1\\x19\\xb04\\x9e\\xd8A\\x10\\x01*\\x13\\n\\x11\\n\\nLoss/train\\x15v\\x90\\x02@\\x95\\xec\\xcce \\x00\\x00\\x00\\x00\\x00\\x00\\x00)\\xed\\xa9P\\tU\\xda\\x1b\\xb04\\x9e\\xd8A\\x10\\x01*\\x13\\n\\x11\\n\\nLoss/train\\x15^\\x15\\x02@\\xd3\\x16\\x94\\x80 \\x00\\x00\\x00\\x00\\x00\\x00\\x00)\\xed\\xa9P\\t\\xe2\\x01\\x1e\\xb04\\x9e\\xd8A\\x10\\x01*\\x13\\n\\x11\\n\\nLoss/train\\x15\\xd9\\x01\\x04@\\xd3\\xac\\x8f\\xce \\x00\\x00\\x00\\x00\\x00\\x00\\x00)\\xed\\xa9P\\tal \\xb04\\x9e\\xd8A\\x10\\x01*\\x13\\n\\x11\\n\\nLoss/train\\x15\\x17\\n\\x03@5\\xc5\\'\\xca \\x00\\x00\\x00\\x00\\x00\\x00\\x00)\\xed\\xa9P\\t?<#\\xb04\\x9e\\xd8A\\x10\\x01*\\x13\\n\\x11\\n\\nLoss/train\\x15[A\\xfe?\\x8ck<\\xc6 \\x00\\x00\\x00\\x00\\x00\\x00\\x00)\\xed\\xa9P\\t\\x8e\\xb1%\\xb04\\x9e\\xd8A\\x10\\x01*\\x13\\n\\x11\\n\\nLoss/train\\x15\\xbdN\\x04@W\\x9fX\\xa1 \\x00\\x00\\x00\\x00\\x00\\x00\\x00)\\xed\\xa9P\\tf\\xf9\\'\\xb04\\x9e\\xd8A\\x10\\x01*\\x13\\n\\x11\\n\\nLoss/train\\x154H\\x00@\\xfc/\\x03$ \\x00\\x00\\x00\\x00\\x00\\x00\\x00)\\xed\\xa9P\\tL\\xc6*\\xb04\\x9e\\xd8A\\x10\\x01*\\x13\\n\\x11\\n\\nLoss/train\\x15\\xcc\\xf2\\x01@u\\x91\\xfe\\xfc \\x00\\x00\\x00\\x00\\x00\\x00\\x00)\\xed\\xa9P\\t\\xb1\\xab-\\xb04\\x9e\\xd8A\\x10\\x01*\\x13\\n\\x11\\n\\nLoss/train\\x15\\xf9\\x8d\\xff?\\xc0\\xf6j\\xd8 \\x00\\x00\\x00\\x00\\x00\\x00\\x00)\\xed\\xa9P\\t\\xf8\\x170\\xb04\\x9e\\xd8A\\x10\\x01*\\x13\\n\\x11\\n\\nLoss/train\\x15\\xaa\\x05\\x00@\\xbcz\\x0c\\xc7 \\x00\\x00\\x00\\x00\\x00\\x00\\x00)\\xed\\xa9P\\t{j2\\xb04\\x9e\\xd8A\\x10\\x01*\\x13\\n\\x11\\n\\nLoss/train\\x15W \\x02@\\xc9Ww\\x8f \\x00\\x00\\x00\\x00\\x00\\x00\\x00)\\xed\\xa9P\\t:\\x914\\xb04\\x9e\\xd8A\\x10\\x01*\\x13\\n\\x11\\n\\nLoss/train\\x15x(\\x03@:\\x97\"\\r \\x00\\x00\\x00\\x00\\x00\\x00\\x00)\\xed\\xa9P\\t^\\xc06\\xb04\\x9e\\xd8A\\x10\\x01*\\x13\\n\\x11\\n\\nLoss/train\\x15\\x03R\\xf9?\\x04\\xe1\\xdd\\xbc \\x00\\x00\\x00\\x00\\x00\\x00\\x00)\\xed\\xa9P\\t\\xcei9\\xb04\\x9e\\xd8A\\x10\\x01*\\x13\\n\\x11\\n\\nLoss/train\\x15\\x1eq\\xfc?\\xdb\\x1d\\x8d\\xb8 \\x00\\x00\\x00\\x00\\x00\\x00\\x00)\\xed\\xa9P\\t\\xcd\\x05<\\xb04\\x9e\\xd8A\\x10\\x01*\\x13\\n\\x11\\n\\nLoss/train\\x15\\xd4\\x9b\\xf5?k\\xbd=\\xcc \\x00\\x00\\x00\\x00\\x00\\x00\\x00)\\xed\\xa9P\\t\\x86R>\\xb04\\x9e\\xd8A\\x10\\x01*\\x13\\n\\x11\\n\\nLoss/train\\x15\\x82\\xc1\\xfd?\\xd4\\x13+r \\x00\\x00\\x00\\x00\\x00\\x00\\x00)\\xed\\xa9P\\t\\x0c\\xab@\\xb04\\x9e\\xd8A\\x10\\x01*\\x13\\n\\x11\\n\\nLoss/train\\x15I^\\xee?\\x93m&o \\x00\\x00\\x00\\x00\\x00\\x00\\x00)\\xed\\xa9P\\t+\\xbbC\\xb04\\x9e\\xd8A\\x10\\x01*\\x13\\n\\x11\\n\\nLoss/train\\x15\\x0b\\x83\\xf9?y+\\xa4^ \\x00\\x00\\x00\\x00\\x00\\x00\\x00)\\xed\\xa9P\\t\\xca7F\\xb04\\x9e\\xd8A\\x10\\x01*\\x13\\n\\x11\\n\\nLoss/train\\x15\\xd1i\\xf6?\\xb7\\xd7}\\xfd \\x00\\x00\\x00\\x00\\x00\\x00\\x00)\\xed\\xa9P\\t\\xf9\\x9dH\\xb04\\x9e\\xd8A\\x10\\x01*\\x13\\n\\x11\\n\\nLoss/train\\x15\\xd9\\xd3\\xf8?^\\xe77\\xd4 \\x00\\x00\\x00\\x00\\x00\\x00\\x00)\\xed\\xa9P\\t\\xcd\\x01K\\xb04\\x9e\\xd8A\\x10\\x01*\\x13\\n\\x11\\n\\nLoss/train\\x15I\\x92\\xf6?\\xe0r\\xe5\\xed \\x00\\x00\\x00\\x00\\x00\\x00\\x00)\\xed\\xa9P\\tg~M\\xb04\\x9e\\xd8A\\x10\\x01*\\x13\\n\\x11\\n\\nLoss/train\\x15R\\x07\\xf7?Z\\x1fo\\xeb \\x00\\x00\\x00\\x00\\x00\\x00\\x00)\\xed\\xa9P\\t\\xa6\\xcfO\\xb04\\x9e\\xd8A\\x10\\x01*\\x13\\n\\x11\\n\\nLoss/train\\x15:T\\xf4?\\xdc<\\xe6\\x96 \\x00\\x00\\x00\\x00\\x00\\x00\\x00)\\xed\\xa9P\\t\\x1ePR\\xb04\\x9e\\xd8A\\x10\\x01*\\x13\\n\\x11\\n\\nLoss/train\\x15A-\\xed?\\xe5\\xfb;\\xd2 \\x00\\x00\\x00\\x00\\x00\\x00\\x00)\\xed\\xa9P\\t\\xa2\\xf2T\\xb04\\x9e\\xd8A\\x10\\x01*\\x13\\n\\x11\\n\\nLoss/train\\x15\\xe7H\\xf0?\\xb8\\xc1\\xbd1 \\x00\\x00\\x00\\x00\\x00\\x00\\x00)\\xed\\xa9P\\t\\x8cIW\\xb04\\x9e\\xd8A\\x10\\x01*\\x13\\n\\x11\\n\\nLoss/train\\x15\\xf1\\xe1\\xf4?\\xc4\\x16\\xfe\\x9c \\x00\\x00\\x00\\x00\\x00\\x00\\x00)\\xed\\xa9P\\tb\\xdbY\\xb04\\x9e\\xd8A\\x10\\x01*\\x13\\n\\x11\\n\\nLoss/train\\x15\\x103\\xf1?\\x92\\xc1I\\xd7 \\x00\\x00\\x00\\x00\\x00\\x00\\x00)\\xed\\xa9P\\t\\xcf\\xbc\\\\\\xb04\\x9e\\xd8A\\x10\\x01*\\x13\\n\\x11\\n\\nLoss/train\\x15\\r,\\xef?\\x9d\\x95\\x0b\\x96 \\x00\\x00\\x00\\x00\\x00\\x00\\x00)\\xed\\xa9P\\t\\x97Z_\\xb04\\x9e\\xd8A\\x10\\x01*\\x13\\n\\x11\\n\\nLoss/train\\x15\\xb6\\x98\\xdf?\\x98C\\xc5\\x87 \\x00\\x00\\x00\\x00\\x00\\x00\\x00)\\xed\\xa9P\\tx\\xd3a\\xb04\\x9e\\xd8A\\x10\\x01*\\x13\\n\\x11\\n\\nLoss/train\\x15\\xb4\\xd3\\xe5?\\xb5Y\\xf1\\xe6 \\x00\\x00\\x00\\x00\\x00\\x00\\x00)\\xed\\xa9P\\t\\xbc(d\\xb04\\x9e\\xd8A\\x10\\x01*\\x13\\n\\x11\\n\\nLoss/train\\x15\\xbd\\xb5\\xe9?gn>8 \\x00\\x00\\x00\\x00\\x00\\x00\\x00)\\xed\\xa9P\\t\\xb2Nf\\xb04\\x9e\\xd8A\\x10\\x01*\\x13\\n\\x11\\n\\nLoss/train\\x15$J\\xe4?\\xc5\\x00\\xcc\\xad \\x00\\x00\\x00\\x00\\x00\\x00\\x00)\\xed\\xa9P\\t\\xeb\\x8fh\\xb04\\x9e\\xd8A\\x10\\x01*\\x13\\n\\x11\\n\\nLoss/train\\x15\\xa0\\x87\\xe2?\\xf7\\xc4\\xfa\\xd0 \\x00\\x00\\x00\\x00\\x00\\x00\\x00)\\xed\\xa9P\\t\\x03\\x0ek\\xb04\\x9e\\xd8A\\x10\\x01*\\x13\\n\\x11\\n\\nLoss/train\\x15\\x05\\xfc\\xde?\\x8c\\x06\\xd5\\xa6 \\x00\\x00\\x00\\x00\\x00\\x00\\x00)\\xed\\xa9P\\t\\xf9\\x16n\\xb04\\x9e\\xd8A\\x10\\x01*\\x13\\n\\x11\\n\\nLoss/train\\x15\\x9bG\\xd7?\\xff\\x8b\\x99x \\x00\\x00\\x00\\x00\\x00\\x00\\x00)\\xed\\xa9P\\t\\x14\\xadp\\xb04\\x9e\\xd8A\\x10\\x01*\\x13\\n\\x11\\n\\nLoss/train\\x15qm\\xe8?\\x85\\xb2\\x8e\\xf5 \\x00\\x00\\x00\\x00\\x00\\x00\\x00)\\xed\\xa9P\\t\\x82Xs\\xb04\\x9e\\xd8A\\x10\\x01*\\x13\\n\\x11\\n\\nLoss/train\\x15r\\x0e\\xe6?\\xb6\\x05\\xca\\xd6 \\x00\\x00\\x00\\x00\\x00\\x00\\x00)\\xed\\xa9P\\t\\xc0 v\\xb04\\x9e\\xd8A\\x10\\x01*\\x13\\n\\x11\\n\\nLoss/train\\x15\\\\!\\xd5?\\x9e\\r\\x0fU \\x00\\x00\\x00\\x00\\x00\\x00\\x00)\\xed\\xa9P\\t$\\xd3x\\xb04\\x9e\\xd8A\\x10\\x01*\\x13\\n\\x11\\n\\nLoss/train\\x15\\x80\\r\\xdb?\\x8d\\xd2\\xda\\xbc \\x00\\x00\\x00\\x00\\x00\\x00\\x00)\\xed\\xa9P\\teS{\\xb04\\x9e\\xd8A\\x10\\x01*\\x13\\n\\x11\\n\\nLoss/train\\x15\\x87\\x8d\\xd6?\\xa8\\xde\\xef\\xe3 \\x00\\x00\\x00\\x00\\x00\\x00\\x00)\\xed\\xa9P\\t\\xac\\xa7}\\xb04\\x9e\\xd8A\\x10\\x01*\\x13\\n\\x11\\n\\nLoss/train\\x15\\x9f;\\xdb?\\xdb\\x1bK\\xff \\x00\\x00\\x00\\x00\\x00\\x00\\x00)\\xed\\xa9P\\tq\\xcb\\x7f\\xb04\\x9e\\xd8A\\x10\\x01*\\x13\\n\\x11\\n\\nLoss/train\\x15\\xd2\\xc4\\xdc?*\\x8f\\xae\\x05 \\x00\\x00\\x00\\x00\\x00\\x00\\x00)\\xed\\xa9P\\t\\x1aN\\x82\\xb04\\x9e\\xd8A\\x10\\x01*\\x13\\n\\x11\\n\\nLoss/train\\x15\\xf4q\\xcf?N\\x8b{\\xf8 \\x00\\x00\\x00\\x00\\x00\\x00\\x00)\\xed\\xa9P\\tU\\xf9\\x84\\xb04\\x9e\\xd8A\\x10\\x01*\\x13\\n\\x11\\n\\nLoss/train\\x15\\xc6\\xbf\\xd5?\\xcc\\x9b\\x15\\x88 \\x00\\x00\\x00\\x00\\x00\\x00\\x00)\\xed\\xa9P\\t[\\xb6\\x87\\xb04\\x9e\\xd8A\\x10\\x01*\\x13\\n\\x11\\n\\nLoss/train\\x15?\\x1a\\xde?\\xc1V)\\x94 \\x00\\x00\\x00\\x00\\x00\\x00\\x00)\\xed\\xa9P\\t\\x1eO\\x8a\\xb04\\x9e\\xd8A\\x10\\x01*\\x13\\n\\x11\\n\\nLoss/train\\x1537\\xca?xG\\xf4\\xf1 \\x00\\x00\\x00\\x00\\x00\\x00\\x00)\\xed\\xa9P\\t\\xf7\\x02\\x8d\\xb04\\x9e\\xd8A\\x10\\x01*\\x13\\n\\x11\\n\\nLoss/train\\x15w\\r\\xd8?l\\xf2\\xb1\\xc9 \\x00\\x00\\x00\\x00\\x00\\x00\\x00)\\xed\\xa9P\\tV\\xd6\\x8f\\xb04\\x9e\\xd8A\\x10\\x01*\\x13\\n\\x11\\n\\nLoss/train\\x15\\x8d\\xe9\\xd4?\\x05\\x9b\\x85+ \\x00\\x00\\x00\\x00\\x00\\x00\\x00)\\xed\\xa9P\\t\\xaeK\\x92\\xb04\\x9e\\xd8A\\x10\\x01*\\x13\\n\\x11\\n\\nLoss/train\\x15@\\x1d\\xdf?QGo\\xf0 \\x00\\x00\\x00\\x00\\x00\\x00\\x00)\\xed\\xa9P\\tfl\\x94\\xb04\\x9e\\xd8A\\x10\\x01*\\x13\\n\\x11\\n\\nLoss/train\\x15lv\\xce?\\xa7t=\\xd5 \\x00\\x00\\x00\\x00\\x00\\x00\\x00)\\xed\\xa9P\\t!\\xb0\\x96\\xb04\\x9e\\xd8A\\x10\\x01*\\x13\\n\\x11\\n\\nLoss/train\\x15~<\\xd3?\\xaaeR\\xc2 \\x00\\x00\\x00\\x00\\x00\\x00\\x00)\\xed\\xa9P\\t\\xaf&\\x99\\xb04\\x9e\\xd8A\\x10\\x01*\\x13\\n\\x11\\n\\nLoss/train\\x15M\\x11\\xc5?kN^\\x97 \\x00\\x00\\x00\\x00\\x00\\x00\\x00)\\xed\\xa9P\\tG\\x90\\x9b\\xb04\\x9e\\xd8A\\x10\\x01*\\x13\\n\\x11\\n\\nLoss/train\\x15\\xc9\\x10\\xd4?\\xa3[Z+ \\x00\\x00\\x00\\x00\\x00\\x00\\x00)\\xed\\xa9P\\t#/\\x9e\\xb04\\x9e\\xd8A\\x10\\x01*\\x13\\n\\x11\\n\\nLoss/train\\x15\\xdb/\\xc8?\\x1384% \\x00\\x00\\x00\\x00\\x00\\x00\\x00)\\xed\\xa9P\\t=\\xbb\\xa0\\xb04\\x9e\\xd8A\\x10\\x01*\\x13\\n\\x11\\n\\nLoss/train\\x15\\xcb1\\xd7?\\x97\\xcc\\xc2e \\x00\\x00\\x00\\x00\\x00\\x00\\x00)\\xed\\xa9P\\t9\\x0c\\xa3\\xb04\\x9e\\xd8A\\x10\\x01*\\x13\\n\\x11\\n\\nLoss/train\\x15\\n\\x15\\xc9?M\\xbd2\\xbe \\x00\\x00\\x00\\x00\\x00\\x00\\x00)\\xed\\xa9P\\t\\x8aZ\\xa5\\xb04\\x9e\\xd8A\\x10\\x01*\\x13\\n\\x11\\n\\nLoss/train\\x15\\xa9\\x07\\xc1?;\\xdbS\\x96 \\x00\\x00\\x00\\x00\\x00\\x00\\x00)\\xed\\xa9P\\t_\\x07\\xa8\\xb04\\x9e\\xd8A\\x10\\x01*\\x13\\n\\x11\\n\\nLoss/train\\x15\\xbf\\xd0\\xd1?\\x90I\\x00\\x7f \\x00\\x00\\x00\\x00\\x00\\x00\\x00)\\xed\\xa9P\\tT\\x90\\xaa\\xb04\\x9e\\xd8A\\x10\\x01*\\x13\\n\\x11\\n\\nLoss/train\\x15\\x03\\xf5\\xdb?\\xf7}QA \\x00\\x00\\x00\\x00\\x00\\x00\\x00)\\xed\\xa9P\\t}%\\xad\\xb04\\x9e\\xd8A\\x10\\x01*\\x13\\n\\x11\\n\\nLoss/train\\x15\\xad`\\xc0?\\xceF\\x00\\x85 \\x00\\x00\\x00\\x00\\x00\\x00\\x00)\\xed\\xa9P\\t\\xf2}\\xaf\\xb04\\x9e\\xd8A\\x10\\x01*\\x13\\n\\x11\\n\\nLoss/train\\x15\\x86A\\xc0?\\x83\\xaf\\xe8N \\x00\\x00\\x00\\x00\\x00\\x00\\x00)\\xed\\xa9P\\t\\x9d\\xbd\\xb1\\xb04\\x9e\\xd8A\\x10\\x01*\\x13\\n\\x11\\n\\nLoss/train\\x15u\\xb5\\xc0?\\x82\\x15\\x84f \\x00\\x00\\x00\\x00\\x00\\x00\\x00)\\xed\\xa9P\\t\\\\\\x06\\xb4\\xb04\\x9e\\xd8A\\x10\\x01*\\x13\\n\\x11\\n\\nLoss/train\\x15\\xbb\\x02\\xca?v\\x0c\\xb6\\xcf \\x00\\x00\\x00\\x00\\x00\\x00\\x00)\\xed\\xa9P\\t\\x12\\x83\\xb6\\xb04\\x9e\\xd8A\\x10\\x01*\\x13\\n\\x11\\n\\nLoss/train\\x15,9\\xaa?S[5\\xd1 \\x00\\x00\\x00\\x00\\x00\\x00\\x00)\\xed\\xa9P\\t\\x13D\\xb9\\xb04\\x9e\\xd8A\\x10\\x01*\\x13\\n\\x11\\n\\nLoss/train\\x15\\xd8\\n\\xc6?\\xde\\x8e\\x831 \\x00\\x00\\x00\\x00\\x00\\x00\\x00)\\xed\\xa9P\\t\\xbb\\xed\\xbb\\xb04\\x9e\\xd8A\\x10\\x01*\\x13\\n\\x11\\n\\nLoss/train\\x15\\xb7[\\xc7?\\x96W\\xe4\\x94 \\x00\\x00\\x00\\x00\\x00\\x00\\x00)\\xed\\xa9P\\t$\\x9a\\xbe\\xb04\\x9e\\xd8A\\x10\\x01*\\x13\\n\\x11\\n\\nLoss/train\\x15\\xec\\xd3\\xe1?^A\\xb8d \\x00\\x00\\x00\\x00\\x00\\x00\\x00)\\xed\\xa9P\\t\\x9bR\\xc1\\xb04\\x9e\\xd8A\\x10\\x01*\\x13\\n\\x11\\n\\nLoss/train\\x15S\\xc5\\xb7?\\xed\\xfe,M \\x00\\x00\\x00\\x00\\x00\\x00\\x00)\\xed\\xa9P\\ta\\xc5\\xc3\\xb04\\x9e\\xd8A\\x10\\x01*\\x13\\n\\x11\\n\\nLoss/train\\x15\\x11\\xaa\\xc3?$\\xb9\\xbe\\xab \\x00\\x00\\x00\\x00\\x00\\x00\\x00)\\xed\\xa9P\\tZ\\xd8\\xc5\\xb04\\x9e\\xd8A\\x10\\x01*\\x13\\n\\x11\\n\\nLoss/train\\x15\\xcbR\\xbf?\\x15\\x8a\\xa5\\x0e \\x00\\x00\\x00\\x00\\x00\\x00\\x00)\\xed\\xa9P\\t<-\\xc8\\xb04\\x9e\\xd8A\\x10\\x01*\\x13\\n\\x11\\n\\nLoss/train\\x15p(\\xb9?I@S\\xff \\x00\\x00\\x00\\x00\\x00\\x00\\x00)\\xed\\xa9P\\tm\\x8d\\xca\\xb04\\x9e\\xd8A\\x10\\x01*\\x13\\n\\x11\\n\\nLoss/train\\x15i\\xbf\\xbe?\\xc0%\\x95n \\x00\\x00\\x00\\x00\\x00\\x00\\x00)\\xed\\xa9P\\t\\xe1(\\xcd\\xb04\\x9e\\xd8A\\x10\\x01*\\x13\\n\\x11\\n\\nLoss/train\\x15\\xda!\\xb8?\\xb4\\xf7\\x1e\\xfa \\x00\\x00\\x00\\x00\\x00\\x00\\x00)\\xed\\xa9P\\t\\xc5\\xfd\\xcf\\xb04\\x9e\\xd8A\\x10\\x01*\\x13\\n\\x11\\n\\nLoss/train\\x15\\x81\\xda\\xb9?\\xef\\xe0;S \\x00\\x00\\x00\\x00\\x00\\x00\\x00)\\xed\\xa9P\\tg\\x80\\xd2\\xb04\\x9e\\xd8A\\x10\\x01*\\x13\\n\\x11\\n\\nLoss/train\\x15-\\xb5\\xb3?To\\xe7\\xc4 \\x00\\x00\\x00\\x00\\x00\\x00\\x00)\\xed\\xa9P\\t,\\xd2\\xd4\\xb04\\x9e\\xd8A\\x10\\x01*\\x13\\n\\x11\\n\\nLoss/train\\x152\\xeb\\xad?\\xc7\\xb5\\x86\\xa7 \\x00\\x00\\x00\\x00\\x00\\x00\\x00)\\xed\\xa9P\\t\\'\\xfa\\xd6\\xb04\\x9e\\xd8A\\x10\\x01*\\x13\\n\\x11\\n\\nLoss/train\\x15X+\\xbc?\"\\xfd\\xf8\\xbc \\x00\\x00\\x00\\x00\\x00\\x00\\x00)\\xed\\xa9P\\t\\xf4\\x88\\xd9\\xb04\\x9e\\xd8A\\x10\\x01*\\x13\\n\\x11\\n\\nLoss/train\\x15H\\xf1\\xad?w\\t\\xb4? \\x00\\x00\\x00\\x00\\x00\\x00\\x00)\\xed\\xa9P\\te\\xc3\\xdb\\xb04\\x9e\\xd8A\\x10\\x01*\\x13\\n\\x11\\n\\nLoss/train\\x15\\xbe\\x1b\\xbc?1$A\\xc6 \\x00\\x00\\x00\\x00\\x00\\x00\\x00)\\xed\\xa9P\\t3\\xe3\\xdd\\xb04\\x9e\\xd8A\\x10\\x01*\\x13\\n\\x11\\n\\nLoss/train\\x15\\'^\\xb3?\\xdf\\x8d\\xba\\x10 \\x00\\x00\\x00\\x00\\x00\\x00\\x00)\\xed\\xa9P\\t\\x0c\\n\\xe0\\xb04\\x9e\\xd8A\\x10\\x01*\\x13\\n\\x11\\n\\nLoss/train\\x15\\xdd\\x1e\\xb9?w\\xc2\\xbc< \\x00\\x00\\x00\\x00\\x00\\x00\\x00)\\xed\\xa9P\\t\\xbb~\\xe2\\xb04\\x9e\\xd8A\\x10\\x01*\\x13\\n\\x11\\n\\nLoss/train\\x15\\x92H\\xb1?\\x84.\\xeb\" \\x00\\x00\\x00\\x00\\x00\\x00\\x00)\\xed\\xa9P\\t\\xfc\\xfc\\xe4\\xb04\\x9e\\xd8A\\x10\\x01*\\x13\\n\\x11\\n\\nLoss/train\\x15\\xe8\\x8b\\xbe?\\xa6\\xea\\x7fK \\x00\\x00\\x00\\x00\\x00\\x00\\x00)\\xed\\xa9P\\t\\xd6\\xc7\\xe7\\xb04\\x9e\\xd8A\\x10\\x01*\\x13\\n\\x11\\n\\nLoss/train\\x15\\x05\\xb5\\xc0?\\x87w\\xe5J \\x00\\x00\\x00\\x00\\x00\\x00\\x00)\\xed\\xa9P\\t\\x0b\\xea\\xea\\xb04\\x9e\\xd8A\\x10\\x01*\\x13\\n\\x11\\n\\nLoss/train\\x15\\xa7#\\xaf?\\x06\\xbe^\\xb4 \\x00\\x00\\x00\\x00\\x00\\x00\\x00)\\xed\\xa9P\\tZ\\x11\\xee\\xb04\\x9e\\xd8A\\x10\\x01*\\x13\\n\\x11\\n\\nLoss/train\\x15\\xee8\\xba?\\xbe1?\\xd2 \\x00\\x00\\x00\\x00\\x00\\x00\\x00)\\xed\\xa9P\\t\\xb57\\xf1\\xb04\\x9e\\xd8A\\x10\\x01*\\x13\\n\\x11\\n\\nLoss/train\\x155\\xfc\\xac??\\xfe\\xbfT \\x00\\x00\\x00\\x00\\x00\\x00\\x00)\\xed\\xa9P\\tPQ\\xf4\\xb04\\x9e\\xd8A\\x10\\x01*\\x13\\n\\x11\\n\\nLoss/train\\x15\\xfb3\\xb7?#h\\'\\x8b \\x00\\x00\\x00\\x00\\x00\\x00\\x00)\\xed\\xa9P\\t\\x9b;\\xf7\\xb04\\x9e\\xd8A\\x10\\x01*\\x13\\n\\x11\\n\\nLoss/train\\x15\\n\\x10\\xa5?\\xe0\\xaf\\xa6\\x9c \\x00\\x00\\x00\\x00\\x00\\x00\\x00)\\xed\\xa9P\\t\\xde\\x00\\xfa\\xb04\\x9e\\xd8A\\x10\\x01*\\x13\\n\\x11\\n\\nLoss/train\\x15n^\\x98?B\\xab\\x9f\\x16 \\x00\\x00\\x00\\x00\\x00\\x00\\x00)\\xed\\xa9P\\tT\\x8e\\xfc\\xb04\\x9e\\xd8A\\x10\\x01*\\x13\\n\\x11\\n\\nLoss/train\\x15\\xaa\\xba\\xaa?\\xbd\\x18UN \\x00\\x00\\x00\\x00\\x00\\x00\\x00)\\xed\\xa9P\\t6\\xea\\xfe\\xb04\\x9e\\xd8A\\x10\\x01*\\x13\\n\\x11\\n\\nLoss/train\\x15\\xb8Z\\x99?\\xe1\\xcfg\\\\ \\x00\\x00\\x00\\x00\\x00\\x00\\x00)\\xed\\xa9P\\th\\x95\\x01\\xb14\\x9e\\xd8A\\x10\\x01*\\x13\\n\\x11\\n\\nLoss/train\\x15\\'\\x8d\\xa3?E<\\xb4\\x08 \\x00\\x00\\x00\\x00\\x00\\x00\\x00)\\xed\\xa9P\\t\\x03\\xb2\\x04\\xb14\\x9e\\xd8A\\x10\\x01*\\x13\\n\\x11\\n\\nLoss/train\\x15\\xd8\\xce\\xa1?|\\xa3\\xdb5 \\x00\\x00\\x00\\x00\\x00\\x00\\x00)\\xed\\xa9P\\t\\xf8\\xa7\\x07\\xb14\\x9e\\xd8A\\x10\\x01*\\x13\\n\\x11\\n\\nLoss/train\\x15V\\xc5\\x9f?D\\xb6{% \\x00\\x00\\x00\\x00\\x00\\x00\\x00)\\xed\\xa9P\\t\\xbf\\x7f\\n\\xb14\\x9e\\xd8A\\x10\\x01*\\x13\\n\\x11\\n\\nLoss/train\\x15=\\xf4\\x88?\\x14-\\xdb\\x7f \\x00\\x00\\x00\\x00\\x00\\x00\\x00)\\xed\\xa9P\\t~\\x1b\\r\\xb14\\x9e\\xd8A\\x10\\x01*\\x13\\n\\x11\\n\\nLoss/train\\x15\\xca1\\xa5?\\x1a\\x9eH\\x84 \\x00\\x00\\x00\\x00\\x00\\x00\\x00)\\xed\\xa9P\\tY\\x8d\\x0f\\xb14\\x9e\\xd8A\\x10\\x01*\\x13\\n\\x11\\n\\nLoss/train\\x15:b\\x8b?tS\\xa2g \\x00\\x00\\x00\\x00\\x00\\x00\\x00)\\xed\\xa9P\\t\\x9b\\xeb\\x11\\xb14\\x9e\\xd8A\\x10\\x01*\\x13\\n\\x11\\n\\nLoss/train\\x15\\x1aA\\xa8?L\\xd4\\xdd! \\x00\\x00\\x00\\x00\\x00\\x00\\x00)\\xed\\xa9P\\t\\x9e#\\x14\\xb14\\x9e\\xd8A\\x10\\x01*\\x13\\n\\x11\\n\\nLoss/train\\x15\\xe7\\xc1\\x90?K4\\xd9\\xa8 \\x00\\x00\\x00\\x00\\x00\\x00\\x00)\\xed\\xa9P\\t\\xedJ\\x16\\xb14\\x9e\\xd8A\\x10\\x01*\\x13\\n\\x11\\n\\nLoss/train\\x15\\xa0\\x12\\xac?\\xdfd?O \\x00\\x00\\x00\\x00\\x00\\x00\\x00)\\xed\\xa9P\\t\\xdd~\\x18\\xb14\\x9e\\xd8A\\x10\\x01*\\x13\\n\\x11\\n\\nLoss/train\\x15\\xda\\xdb\\x9d?\\x95\\xe0\\x86\\x88 \\x00\\x00\\x00\\x00\\x00\\x00\\x00)\\xed\\xa9P\\t\\x16\\xa6\\x1a\\xb14\\x9e\\xd8A\\x10\\x01*\\x13\\n\\x11\\n\\nLoss/train\\x15D\\xb4\\xab?\\xf2/\\xcbA \\x00\\x00\\x00\\x00\\x00\\x00\\x00)\\xed\\xa9P\\t~\\xe7\\x1c\\xb14\\x9e\\xd8A\\x10\\x01*\\x13\\n\\x11\\n\\nLoss/train\\x15\\xeaZ\\x9c?\\xad\\x14\\xcb\\x04 \\x00\\x00\\x00\\x00\\x00\\x00\\x00)\\xed\\xa9P\\t\\xf5\\x9b\\x1f\\xb14\\x9e\\xd8A\\x10\\x01*\\x13\\n\\x11\\n\\nLoss/train\\x15fQ\\xa6?\\x7f\\x8d5\\x87 \\x00\\x00\\x00\\x00\\x00\\x00\\x00)\\xed\\xa9P\\t\\x92\\x04\"\\xb14\\x9e\\xd8A\\x10\\x01*\\x13\\n\\x11\\n\\nLoss/train\\x15tf\\xa9?@Q\\x9b\\xf8 \\x00\\x00\\x00\\x00\\x00\\x00\\x00)\\xed\\xa9P\\t\\x0c9$\\xb14\\x9e\\xd8A\\x10\\x01*\\x13\\n\\x11\\n\\nLoss/train\\x15\\xdeE\\x82?\\xf1\\x90\\x00; \\x00\\x00\\x00\\x00\\x00\\x00\\x00)\\xed\\xa9P\\taj&\\xb14\\x9e\\xd8A\\x10\\x01*\\x13\\n\\x11\\n\\nLoss/train\\x15\\xc5\\xf9\\x93?\\x1c\\xe0\\xe0\\xe1 \\x00\\x00\\x00\\x00\\x00\\x00\\x00)\\xed\\xa9P\\t\\x97\\xa9(\\xb14\\x9e\\xd8A\\x10\\x01*\\x13\\n\\x11\\n\\nLoss/train\\x15=3\\x80?\\xe7X\\xa0\\x9a \\x00\\x00\\x00\\x00\\x00\\x00\\x00)\\xed\\xa9P\\t@/+\\xb14\\x9e\\xd8A\\x10\\x01*\\x13\\n\\x11\\n\\nLoss/train\\x15\\xc0\\x1a\\xa2?1D\\xba\\xb9 \\x00\\x00\\x00\\x00\\x00\\x00\\x00)\\xed\\xa9P\\t\\x1b\\xdc-\\xb14\\x9e\\xd8A\\x10\\x01*\\x13\\n\\x11\\n\\nLoss/train\\x15\\x8d\\xa2\\x88?xz\\xa91 \\x00\\x00\\x00\\x00\\x00\\x00\\x00)\\xed\\xa9P\\t\\x9b?0\\xb14\\x9e\\xd8A\\x10\\x01*\\x13\\n\\x11\\n\\nLoss/train\\x15\\r3\\x8f?\\x08\\xd4\\xefW \\x00\\x00\\x00\\x00\\x00\\x00\\x00)\\xed\\xa9P\\t\\x8f\\xa52\\xb14\\x9e\\xd8A\\x10\\x01*\\x13\\n\\x11\\n\\nLoss/train\\x15V\\xdd\\x8b?\\xa5\\x19\\xb9\\xcd \\x00\\x00\\x00\\x00\\x00\\x00\\x00)\\xed\\xa9P\\t\\x80\\x105\\xb14\\x9e\\xd8A\\x10\\x01*\\x13\\n\\x11\\n\\nLoss/train\\x15\\x11\\xb9\\x93?\\xe7\\x06\\x9c\\x92 \\x00\\x00\\x00\\x00\\x00\\x00\\x00)\\xed\\xa9P\\t\\xe9\\xb88\\xb14\\x9e\\xd8A\\x10\\x01*\\x13\\n\\x11\\n\\nLoss/train\\x15 \\x96\\x8f?\\x19\\x0c\\xae\\xff \\x00\\x00\\x00\\x00\\x00\\x00\\x00)\\xed\\xa9P\\t\\xb55;\\xb14\\x9e\\xd8A\\x10\\x01*\\x13\\n\\x11\\n\\nLoss/train\\x15\\x883\\x8e?8\\x8c-l \\x00\\x00\\x00\\x00\\x00\\x00\\x00)\\xed\\xa9P\\tUd=\\xb14\\x9e\\xd8A\\x10\\x01*\\x13\\n\\x11\\n\\nLoss/train\\x15\\x93e\\x85?\\xb2\\xeb\\x9e\\x83 \\x00\\x00\\x00\\x00\\x00\\x00\\x00)\\xed\\xa9P\\t\\xd5\\x97?\\xb14\\x9e\\xd8A\\x10\\x01*\\x13\\n\\x11\\n\\nLoss/train\\x15\\x173\\x8d?t\\xbd@j \\x00\\x00\\x00\\x00\\x00\\x00\\x00)\\xed\\xa9P\\tb\\xdbA\\xb14\\x9e\\xd8A\\x10\\x01*\\x13\\n\\x11\\n\\nLoss/train\\x15\\xfb\\xde\\x96?B\\x81\\x12= \\x00\\x00\\x00\\x00\\x00\\x00\\x00)\\xed\\xa9P\\t\\xdd#D\\xb14\\x9e\\xd8A\\x10\\x01*\\x13\\n\\x11\\n\\nLoss/train\\x15\\x14\\\\\\x91?\\x05s\\x14t \\x00\\x00\\x00\\x00\\x00\\x00\\x00)\\xed\\xa9P\\t\\xc7eF\\xb14\\x9e\\xd8A\\x10\\x01*\\x13\\n\\x11\\n\\nLoss/train\\x15\\xba\\xb9\\x8b?\\x99/\\x19i \\x00\\x00\\x00\\x00\\x00\\x00\\x00)\\xed\\xa9P\\t\\xfb\\xcdH\\xb14\\x9e\\xd8A\\x10\\x01*\\x13\\n\\x11\\n\\nLoss/train\\x15|\\x10\\x8c?\\xf8\\xe1\\xdd\\xe1 \\x00\\x00\\x00\\x00\\x00\\x00\\x00)\\xed\\xa9P\\t\\xe2\"K\\xb14\\x9e\\xd8A\\x10\\x01*\\x13\\n\\x11\\n\\nLoss/train\\x15\\x14\\xa3\\x8c?Wl\\x1c\\xf4 \\x00\\x00\\x00\\x00\\x00\\x00\\x00)\\xed\\xa9P\\t\\xec\\xdcM\\xb14\\x9e\\xd8A\\x10\\x01*\\x13\\n\\x11\\n\\nLoss/train\\x15\\xbc\\x14\\x82?\\x0f\\xdcw\\x95 \\x00\\x00\\x00\\x00\\x00\\x00\\x00)\\xed\\xa9P\\t\\x1d\\x94P\\xb14\\x9e\\xd8A\\x10\\x01*\\x13\\n\\x11\\n\\nLoss/train\\x15\\x95!\\x8d?\\xeb\\x98\\xe4% \\x00\\x00\\x00\\x00\\x00\\x00\\x00)\\xed\\xa9P\\tY\\xf7R\\xb14\\x9e\\xd8A\\x10\\x01*\\x13\\n\\x11\\n\\nLoss/train\\x15\\xc1M\\x88?\\x18\\xf4\\x9b  \\x00\\x00\\x00\\x00\\x00\\x00\\x00)\\xed\\xa9P\\t\\x1e3U\\xb14\\x9e\\xd8A\\x10\\x01*\\x13\\n\\x11\\n\\nLoss/train\\x15_\\xa4\\x8c?\\x81x}/ \\x00\\x00\\x00\\x00\\x00\\x00\\x00)\\xed\\xa9P\\tEhW\\xb14\\x9e\\xd8A\\x10\\x01*\\x13\\n\\x11\\n\\nLoss/train\\x15\\x99X{?|\\xcc@R \\x00\\x00\\x00\\x00\\x00\\x00\\x00)\\xed\\xa9P\\t\\x14\\xc9Y\\xb14\\x9e\\xd8A\\x10\\x01*\\x13\\n\\x11\\n\\nLoss/train\\x15e\\xe5\\x99?\\xb1$@S \\x00\\x00\\x00\\x00\\x00\\x00\\x00)\\xed\\xa9P\\t.R\\\\\\xb14\\x9e\\xd8A\\x10\\x01*\\x13\\n\\x11\\n\\nLoss/train\\x15\\x9f\\xd0v?Nf\\x1c\\x05 \\x00\\x00\\x00\\x00\\x00\\x00\\x00)\\xed\\xa9P\\t\\xb8\\xab^\\xb14\\x9e\\xd8A\\x10\\x01*\\x13\\n\\x11\\n\\nLoss/train\\x15\\x10\\xc9\\x8c?\\xe3A\\xcb\\x99 \\x00\\x00\\x00\\x00\\x00\\x00\\x00)\\xed\\xa9P\\t+\\xdf`\\xb14\\x9e\\xd8A\\x10\\x01*\\x13\\n\\x11\\n\\nLoss/train\\x15W\\t\\x85?\\x8el\\xa3\\xa7 \\x00\\x00\\x00\\x00\\x00\\x00\\x00)\\xed\\xa9P\\t\\x9a\\x05c\\xb14\\x9e\\xd8A\\x10\\x01*\\x13\\n\\x11\\n\\nLoss/train\\x15\\x11D\\x8a?\\xd2\\x05\\xe9~ \\x00\\x00\\x00\\x00\\x00\\x00\\x00)\\xed\\xa9P\\t\\x06\\x9fe\\xb14\\x9e\\xd8A\\x10\\x01*\\x13\\n\\x11\\n\\nLoss/train\\x154Hy?\\xe0\\xf9I\\xe3 \\x00\\x00\\x00\\x00\\x00\\x00\\x00)\\xed\\xa9P\\t\\x9aAh\\xb14\\x9e\\xd8A\\x10\\x01*\\x13\\n\\x11\\n\\nLoss/train\\x15\\xd4\\xee\\x98?\\xcdyD\\xd9 \\x00\\x00\\x00\\x00\\x00\\x00\\x00)\\xed\\xa9P\\t\\x96\\xb6j\\xb14\\x9e\\xd8A\\x10\\x01*\\x13\\n\\x11\\n\\nLoss/train\\x15\\x93\\x05\\x93?2\\xbf\\'\\x07 \\x00\\x00\\x00\\x00\\x00\\x00\\x00)\\xed\\xa9P\\t\\x82Sm\\xb14\\x9e\\xd8A\\x10\\x01*\\x13\\n\\x11\\n\\nLoss/train\\x15.A\\x9d?%^\\xab\\xb3 \\x00\\x00\\x00\\x00\\x00\\x00\\x00)\\xed\\xa9P\\tT\\xc5o\\xb14\\x9e\\xd8A\\x10\\x01*\\x13\\n\\x11\\n\\nLoss/train\\x15\\xb9\\x12\\x91?\\xa0\\xc3\\x0b\\xe8 \\x00\\x00\\x00\\x00\\x00\\x00\\x00)\\xed\\xa9P\\t\\x81&r\\xb14\\x9e\\xd8A\\x10\\x01*\\x13\\n\\x11\\n\\nLoss/train\\x15]\\xa5\\x92?\\\\\\xc8\\xa3\\x12 \\x00\\x00\\x00\\x00\\x00\\x00\\x00)\\xed\\xa9P\\t\\xb5Ot\\xb14\\x9e\\xd8A\\x10\\x01*\\x13\\n\\x11\\n\\nLoss/train\\x15N@\\x88?\\x92\\xb1@b \\x00\\x00\\x00\\x00\\x00\\x00\\x00)\\xed\\xa9P\\to\\x81v\\xb14\\x9e\\xd8A\\x10\\x01*\\x13\\n\\x11\\n\\nLoss/train\\x15?Yc?\\x9a\\x88O\\x8e \\x00\\x00\\x00\\x00\\x00\\x00\\x00)\\xed\\xa9P\\t_\\xb8x\\xb14\\x9e\\xd8A\\x10\\x01*\\x13\\n\\x11\\n\\nLoss/train\\x151i\\x85?R\\xfcs\\xc2 \\x00\\x00\\x00\\x00\\x00\\x00\\x00)\\xed\\xa9P\\t\\xa5\\xddz\\xb14\\x9e\\xd8A\\x10\\x01*\\x13\\n\\x11\\n\\nLoss/train\\x15\\xbe\\x1b~?\\xac\\xc9\\x0c\\xfb \\x00\\x00\\x00\\x00\\x00\\x00\\x00)\\xed\\xa9P\\t=E}\\xb14\\x9e\\xd8A\\x10\\x01*\\x13\\n\\x11\\n\\nLoss/train\\x15I\\xd2\\x80?.xa\\xe3 \\x00\\x00\\x00\\x00\\x00\\x00\\x00)\\xed\\xa9P\\t\\xbb%\\x80\\xb14\\x9e\\xd8A\\x10\\x01*\\x13\\n\\x11\\n\\nLoss/train\\x15pY\\x81?\\xdd\\xaf\\x97] \\x00\\x00\\x00\\x00\\x00\\x00\\x00)\\xed\\xa9P\\t=\\xbb\\x82\\xb14\\x9e\\xd8A\\x10\\x01*\\x13\\n\\x11\\n\\nLoss/train\\x15H\\x7fb?\\xe6\\x97(\\xd6 \\x00\\x00\\x00\\x00\\x00\\x00\\x00)\\xed\\xa9P\\t\\x89\\xec\\x84\\xb14\\x9e\\xd8A\\x10\\x01*\\x13\\n\\x11\\n\\nLoss/train\\x15\\xdf%\\x93?\\x80\\x84?\\xef \\x00\\x00\\x00\\x00\\x00\\x00\\x00)\\xed\\xa9P\\t\\x9a%\\x87\\xb14\\x9e\\xd8A\\x10\\x01*\\x13\\n\\x11\\n\\nLoss/train\\x15\\x9ck\\x8a?o,\\xad\\xf2 \\x00\\x00\\x00\\x00\\x00\\x00\\x00)\\xed\\xa9P\\t/L\\x89\\xb14\\x9e\\xd8A\\x10\\x01*\\x13\\n\\x11\\n\\nLoss/train\\x15\\xd6\\xf2z?]/D\\x18 \\x00\\x00\\x00\\x00\\x00\\x00\\x00)\\xed\\xa9P\\t\\x9c\\xc1\\x8b\\xb14\\x9e\\xd8A\\x10\\x01*\\x13\\n\\x11\\n\\nLoss/train\\x15X\\x02\\x89?r0\\xa1\\xc6 \\x00\\x00\\x00\\x00\\x00\\x00\\x00)\\xed\\xa9P\\t\\xba\\x18\\x8e\\xb14\\x9e\\xd8A\\x10\\x01*\\x13\\n\\x11\\n\\nLoss/train\\x15\\x91Q\\\\?\\xd9^\\n\\x8d \\x00\\x00\\x00\\x00\\x00\\x00\\x00)\\xed\\xa9P\\t\\xbaH\\x90\\xb14\\x9e\\xd8A\\x10\\x01*\\x13\\n\\x11\\n\\nLoss/train\\x15o\\x9co?F\\xee\\x03\\x83 \\x00\\x00\\x00\\x00\\x00\\x00\\x00)\\xed\\xa9P\\t\\xef\\xad\\x92\\xb14\\x9e\\xd8A\\x10\\x01*\\x13\\n\\x11\\n\\nLoss/train\\x15O\\xb6\\x88?[\\x1e1\\xfc \\x00\\x00\\x00\\x00\\x00\\x00\\x00)\\xed\\xa9P\\t\\n+\\x95\\xb14\\x9e\\xd8A\\x10\\x01*\\x13\\n\\x11\\n\\nLoss/train\\x15\\xc1\\x18\\x96?a\\xfb\\x86I \\x00\\x00\\x00\\x00\\x00\\x00\\x00)\\xed\\xa9P\\t@\\xdd\\x97\\xb14\\x9e\\xd8A\\x10\\x01*\\x13\\n\\x11\\n\\nLoss/train\\x15\\x04\\xef\\x83?\\xc5\\x0fs\\xfd \\x00\\x00\\x00\\x00\\x00\\x00\\x00)\\xed\\xa9P\\t\\x9f9\\x9a\\xb14\\x9e\\xd8A\\x10\\x01*\\x13\\n\\x11\\n\\nLoss/train\\x15\\x1d\\xc2\\x80?A\\x0f\\x01/ \\x00\\x00\\x00\\x00\\x00\\x00\\x00)\\xed\\xa9P\\t\\xd0\\xb8\\x9c\\xb14\\x9e\\xd8A\\x10\\x01*\\x13\\n\\x11\\n\\nLoss/train\\x15$bX?9\\xe5\\x1cq \\x00\\x00\\x00\\x00\\x00\\x00\\x00)\\xed\\xa9P\\t\\xc1\\xff\\x9e\\xb14\\x9e\\xd8A\\x10\\x01*\\x13\\n\\x11\\n\\nLoss/train\\x15\\x12u\\x84?o\\xe7\\x94. \\x00\\x00\\x00\\x00\\x00\\x00\\x00)\\xed\\xa9P\\t`#\\xa1\\xb14\\x9e\\xd8A\\x10\\x01*\\x13\\n\\x11\\n\\nLoss/train\\x15\\x95\\xecr?L_\\xd4f \\x00\\x00\\x00\\x00\\x00\\x00\\x00)\\xed\\xa9P\\t\\xa5G\\xa3\\xb14\\x9e\\xd8A\\x10\\x01*\\x13\\n\\x11\\n\\nLoss/train\\x15t{a?\\xa8\\xaf\\xb6\\x92 \\x00\\x00\\x00\\x00\\x00\\x00\\x00)\\xed\\xa9P\\t~\\x92\\xa5\\xb14\\x9e\\xd8A\\x10\\x01*\\x13\\n\\x11\\n\\nLoss/train\\x15O\\xc4\\x95?\\xf1?\\xce\\x0f \\x00\\x00\\x00\\x00\\x00\\x00\\x00)\\xed\\xa9P\\t\\xfe\\xd4\\xa7\\xb14\\x9e\\xd8A\\x10\\x01*\\x13\\n\\x11\\n\\nLoss/train\\x15\\xeeBe?$\\t\\x92\\x9b \\x00\\x00\\x00\\x00\\x00\\x00\\x00)\\xed\\xa9P\\t\\xe9\\n\\xaa\\xb14\\x9e\\xd8A\\x10\\x01*\\x13\\n\\x11\\n\\nLoss/train\\x15\\xbc\\xb4a?\\x1fy\\xb9\\x8f \\x00\\x00\\x00\\x00\\x00\\x00\\x00)\\xed\\xa9P\\t\\xb9\\xc6\\xac\\xb14\\x9e\\xd8A\\x10\\x01*\\x13\\n\\x11\\n\\nLoss/train\\x15E\\x95B?oW[% \\x00\\x00\\x00\\x00\\x00\\x00\\x00)\\xed\\xa9P\\tMO\\xaf\\xb14\\x9e\\xd8A\\x10\\x01*\\x13\\n\\x11\\n\\nLoss/train\\x15\\xa7\\x99~?\\x11\\x93\\x9b\\x97 \\x00\\x00\\x00\\x00\\x00\\x00\\x00)\\xed\\xa9P\\t\\n\\xb7\\xb1\\xb14\\x9e\\xd8A\\x10\\x01*\\x13\\n\\x11\\n\\nLoss/train\\x15b\\xa0@?D\\xf3\\xd1\\\\ \\x00\\x00\\x00\\x00\\x00\\x00\\x00)\\xed\\xa9P\\t\\x86\\xe3\\xb3\\xb14\\x9e\\xd8A\\x10\\x01*\\x13\\n\\x11\\n\\nLoss/train\\x15y_y?\\xd7$T\\x96 \\x00\\x00\\x00\\x00\\x00\\x00\\x00)\\xed\\xa9P\\t\\xfe\\r\\xb6\\xb14\\x9e\\xd8A\\x10\\x01*\\x13\\n\\x11\\n\\nLoss/train\\x15\\x80Vy?2$\\xc1[ \\x00\\x00\\x00\\x00\\x00\\x00\\x00)\\xed\\xa9P\\tj4\\xb8\\xb14\\x9e\\xd8A\\x10\\x01*\\x13\\n\\x11\\n\\nLoss/train\\x15\\x8b\\xa9>?v8\\xf0\\xd3 \\x00\\x00\\x00\\x00\\x00\\x00\\x00)\\xed\\xa9P\\te\\xc6\\xba\\xb14\\x9e\\xd8A\\x10\\x01*\\x13\\n\\x11\\n\\nLoss/train\\x15\\xea\\xa5>?\\\\/\\x86B \\x00\\x00\\x00\\x00\\x00\\x00\\x00)\\xed\\xa9P\\t\\xc8\\'\\xbd\\xb14\\x9e\\xd8A\\x10\\x01*\\x13\\n\\x11\\n\\nLoss/train\\x15\\xa0\\xa7]?\\xb35`/ \\x00\\x00\\x00\\x00\\x00\\x00\\x00)\\xed\\xa9P\\tL\\x89\\xbf\\xb14\\x9e\\xd8A\\x10\\x01*\\x13\\n\\x11\\n\\nLoss/train\\x15S\\xdb|?e\\x04\\x84\\xb2 \\x00\\x00\\x00\\x00\\x00\\x00\\x00)\\xed\\xa9P\\t\\xac\\xe7\\xc1\\xb14\\x9e\\xd8A\\x10\\x01*\\x13\\n\\x11\\n\\nLoss/train\\x15`3]?\\x1eh\\xb8\\xc9 \\x00\\x00\\x00\\x00\\x00\\x00\\x00)\\xed\\xa9P\\t\\x81<\\xc4\\xb14\\x9e\\xd8A\\x10\\x01*\\x13\\n\\x11\\n\\nLoss/train\\x15\\x05\\xa0g?\\xcb\\x19\\xa6l \\x00\\x00\\x00\\x00\\x00\\x00\\x00)\\xed\\xa9P\\t\\xbf\\xf5\\xc6\\xb14\\x9e\\xd8A\\x10\\x01*\\x13\\n\\x11\\n\\nLoss/train\\x15\\xaa\\xe6\\x83?\\x91\\xb6=\\x11 \\x00\\x00\\x00\\x00\\x00\\x00\\x00)\\xed\\xa9P\\t]T\\xc9\\xb14\\x9e\\xd8A\\x10\\x01*\\x13\\n\\x11\\n\\nLoss/train\\x15\\x9dm\\x83?7ik\\xac \\x00\\x00\\x00\\x00\\x00\\x00\\x00)\\xed\\xa9P\\t\\xc9\\xae\\xcb\\xb14\\x9e\\xd8A\\x10\\x01*\\x13\\n\\x11\\n\\nLoss/train\\x15,^[?\\xa53\\xa2Z \\x00\\x00\\x00\\x00\\x00\\x00\\x00)\\xed\\xa9P\\t\\xd1>\\xce\\xb14\\x9e\\xd8A\\x10\\x01*\\x13\\n\\x11\\n\\nLoss/train\\x15\\xe8\\xf1S?.t\\xe7\\x9c \\x00\\x00\\x00\\x00\\x00\\x00\\x00)\\xed\\xa9P\\tJ_\\xd0\\xb14\\x9e\\xd8A\\x10\\x01*\\x13\\n\\x11\\n\\nLoss/train\\x15\\xec0l?\\xe1>\\xff\\x0c \\x00\\x00\\x00\\x00\\x00\\x00\\x00)\\xed\\xa9P\\t3\\xa1\\xd2\\xb14\\x9e\\xd8A\\x10\\x01*\\x13\\n\\x11\\n\\nLoss/train\\x15\\xb6\\x1f>?G\\xcc5K \\x00\\x00\\x00\\x00\\x00\\x00\\x00)\\xed\\xa9P\\t\\x9a\\xd0\\xd4\\xb14\\x9e\\xd8A\\x10\\x01*\\x13\\n\\x11\\n\\nLoss/train\\x15\\xe43G?\\x9a\\x8f\\xb1\\xef \\x00\\x00\\x00\\x00\\x00\\x00\\x00)\\xed\\xa9P\\t\\xbe\\xfb\\xd6\\xb14\\x9e\\xd8A\\x10\\x01*\\x13\\n\\x11\\n\\nLoss/train\\x15bI\\x81?\"\\xfc\\x97\\xc1 \\x00\\x00\\x00\\x00\\x00\\x00\\x00)\\xed\\xa9P\\t\\xaf}\\xd9\\xb14\\x9e\\xd8A\\x10\\x01*\\x13\\n\\x11\\n\\nLoss/train\\x15\\x99\\xf1K?br\\xf3\\xe8 \\x00\\x00\\x00\\x00\\x00\\x00\\x00)\\xed\\xa9P\\t\\xc9\\x1b\\xdc\\xb14\\x9e\\xd8A\\x10\\x01*\\x13\\n\\x11\\n\\nLoss/train\\x15\\x92\\x90S?\\x01%\\xa3M \\x00\\x00\\x00\\x00\\x00\\x00\\x00)\\xed\\xa9P\\t,\\x85\\xde\\xb14\\x9e\\xd8A\\x10\\x01*\\x13\\n\\x11\\n\\nLoss/train\\x15a\\xb2\\x85?W;Z\\xf3 \\x00\\x00\\x00\\x00\\x00\\x00\\x00)\\xed\\xa9P\\t^\\xbe\\xe0\\xb14\\x9e\\xd8A\\x10\\x01*\\x13\\n\\x11\\n\\nLoss/train\\x15\\rGa?\\x90hb\\xb7 \\x00\\x00\\x00\\x00\\x00\\x00\\x00)\\xed\\xa9P\\t-\\x0c\\xe3\\xb14\\x9e\\xd8A\\x10\\x01*\\x13\\n\\x11\\n\\nLoss/train\\x15\\xccgl?\\xa7\\xc0<s \\x00\\x00\\x00\\x00\\x00\\x00\\x00)\\xed\\xa9P\\t\\x92^\\xe5\\xb14\\x9e\\xd8A\\x10\\x01*\\x13\\n\\x11\\n\\nLoss/train\\x15\\xa7\\x01U?\\xf0\\x15`\\xff \\x00\\x00\\x00\\x00\\x00\\x00\\x00)\\xed\\xa9P\\tc\\x7f\\xe7\\xb14\\x9e\\xd8A\\x10\\x01*\\x13\\n\\x11\\n\\nLoss/train\\x15bSX?jS\\xf2) \\x00\\x00\\x00\\x00\\x00\\x00\\x00)\\xed\\xa9P\\t\\x1a\\xa0\\xe9\\xb14\\x9e\\xd8A\\x10\\x01*\\x13\\n\\x11\\n\\nLoss/train\\x15\\xb2 j?%b\\xe67 \\x00\\x00\\x00\\x00\\x00\\x00\\x00)\\xed\\xa9P\\t\\xa1\\xf7\\xeb\\xb14\\x9e\\xd8A\\x10\\x01*\\x13\\n\\x11\\n\\nLoss/train\\x15\\xc0xR?\\xb9\\x1b\\xeb\\xeb \\x00\\x00\\x00\\x00\\x00\\x00\\x00)\\xed\\xa9P\\t\\x84\\x84\\xee\\xb14\\x9e\\xd8A\\x10\\x01*\\x13\\n\\x11\\n\\nLoss/train\\x15\\xd6\\xff@?\\xbc\\xbeP] \\x00\\x00\\x00\\x00\\x00\\x00\\x00)\\xed\\xa9P\\t%\\xed\\xf0\\xb14\\x9e\\xd8A\\x10\\x01*\\x13\\n\\x11\\n\\nLoss/train\\x152eq?\\xd6\\'\\x91\\x9f \\x00\\x00\\x00\\x00\\x00\\x00\\x00)\\xed\\xa9P\\t\\xa6\\x07\\xf4\\xb14\\x9e\\xd8A\\x10\\x01*\\x13\\n\\x11\\n\\nLoss/train\\x15\\x05\\x17R?3\\xf4\\x04\\xb0 \\x00\\x00\\x00\\x00\\x00\\x00\\x00)\\xed\\xa9P\\t\\x0eH\\xf6\\xb14\\x9e\\xd8A\\x10\\x01*\\x13\\n\\x11\\n\\nLoss/train\\x15K\\xa4d?\\xa5\\xd7[\\x93 \\x00\\x00\\x00\\x00\\x00\\x00\\x00)\\xed\\xa9P\\t\\xce\\xbf\\xf8\\xb14\\x9e\\xd8A\\x10\\x01*\\x13\\n\\x11\\n\\nLoss/train\\x15]\\x86_?\\xadr7A \\x00\\x00\\x00\\x00\\x00\\x00\\x00)\\xed\\xa9P\\tz\\xde\\xfa\\xb14\\x9e\\xd8A\\x10\\x01*\\x13\\n\\x11\\n\\nLoss/train\\x152\\xcc\\x82?\"\\xdf\\xc3\\xb1 \\x00\\x00\\x00\\x00\\x00\\x00\\x00)\\xed\\xa9P\\tk\\x0c\\xfd\\xb14\\x9e\\xd8A\\x10\\x01*\\x13\\n\\x11\\n\\nLoss/train\\x15\\xe9/d?5\\xe4-v \\x00\\x00\\x00\\x00\\x00\\x00\\x00)\\xed\\xa9P\\t\\xea\\x8f\\xff\\xb14\\x9e\\xd8A\\x10\\x01*\\x13\\n\\x11\\n\\nLoss/train\\x15;\\x9c2?\\x89\\x99\\xa9\\x1b \\x00\\x00\\x00\\x00\\x00\\x00\\x00)\\xed\\xa9P\\t\\x02E\\x07\\xb24\\x9e\\xd8A\\x10\\x01*\\x13\\n\\x11\\n\\nLoss/train\\x15u\\xd0g?\\xe4\\xd2\\xc23 \\x00\\x00\\x00\\x00\\x00\\x00\\x00)\\xed\\xa9P\\t\\x08 \\n\\xb24\\x9e\\xd8A\\x10\\x01*\\x13\\n\\x11\\n\\nLoss/train\\x15^\\xae_?\\xf3\\xc5\\n\\x8c \\x00\\x00\\x00\\x00\\x00\\x00\\x00)\\xed\\xa9P\\t]\\x13\\r\\xb24\\x9e\\xd8A\\x10\\x01*\\x13\\n\\x11\\n\\nLoss/train\\x15\\x81iu?\\xf2X\\xfe\\x81 \\x00\\x00\\x00\\x00\\x00\\x00\\x00)\\xed\\xa9P\\t5\\x97\\x0f\\xb24\\x9e\\xd8A\\x10\\x01*\\x13\\n\\x11\\n\\nLoss/train\\x15\\xb2\\xf0W?\\xe9e\\xc9\\x13 \\x00\\x00\\x00\\x00\\x00\\x00\\x00)\\xed\\xa9P\\t\\xb0\\x01\\x12\\xb24\\x9e\\xd8A\\x10\\x01*\\x13\\n\\x11\\n\\nLoss/train\\x15Z\\xdah?\\xe5\\x90b2 \\x00\\x00\\x00\\x00\\x00\\x00\\x00)\\xed\\xa9P\\t\\xa2&\\x14\\xb24\\x9e\\xd8A\\x10\\x01*\\x13\\n\\x11\\n\\nLoss/train\\x15\\xdd\\x1f[?\\xd6\\x18\\xc7\\xd2 \\x00\\x00\\x00\\x00\\x00\\x00\\x00)\\xed\\xa9P\\t\\xadK\\x16\\xb24\\x9e\\xd8A\\x10\\x01*\\x13\\n\\x11\\n\\nLoss/train\\x15t\\xc5G?\\xdf\\xe7\\xed\\x9d \\x00\\x00\\x00\\x00\\x00\\x00\\x00)\\xed\\xa9P\\to\\xd5\\x18\\xb24\\x9e\\xd8A\\x10\\x01*\\x13\\n\\x11\\n\\nLoss/train\\x15\\x8f\\x94N?O\\xba\\x13\\xa2 \\x00\\x00\\x00\\x00\\x00\\x00\\x00)\\xed\\xa9P\\t\\xb8\\t\\x1b\\xb24\\x9e\\xd8A\\x10\\x01*\\x13\\n\\x11\\n\\nLoss/train\\x15\\xc4\\x1c??\\x8c\\x82=\\x8c \\x00\\x00\\x00\\x00\\x00\\x00\\x00)\\xed\\xa9P\\t\\x88+\\x1d\\xb24\\x9e\\xd8A\\x10\\x01*\\x13\\n\\x11\\n\\nLoss/train\\x15\\xcf6U?@\\xc7\\xb7s \\x00\\x00\\x00\\x00\\x00\\x00\\x00)\\xed\\xa9P\\t\\x1dx\\x1f\\xb24\\x9e\\xd8A\\x10\\x01*\\x13\\n\\x11\\n\\nLoss/train\\x15\\xc6\\xfc^?\\xc76\\xe4\\x1b \\x00\\x00\\x00\\x00\\x00\\x00\\x00)\\xed\\xa9P\\to\\x9e!\\xb24\\x9e\\xd8A\\x10\\x01*\\x13\\n\\x11\\n\\nLoss/train\\x15\\xf8\\xc3`?vn\\x9cz \\x00\\x00\\x00\\x00\\x00\\x00\\x00)\\xed\\xa9P\\t\\xed\\xb9#\\xb24\\x9e\\xd8A\\x10\\x01*\\x13\\n\\x11\\n\\nLoss/train\\x15\\x03Bb?\\x9f\\xa3 F \\x00\\x00\\x00\\x00\\x00\\x00\\x00)\\xed\\xa9P\\t}\\xe9%\\xb24\\x9e\\xd8A\\x10\\x01*\\x13\\n\\x11\\n\\nLoss/train\\x15pal?\\x0c\\x97\\xb4\\xaf \\x00\\x00\\x00\\x00\\x00\\x00\\x00)\\xed\\xa9P\\t\\xf2B(\\xb24\\x9e\\xd8A\\x10\\x01*\\x13\\n\\x11\\n\\nLoss/train\\x15\\\\\\xd6>? \\xe9\\xd7  \\x00\\x00\\x00\\x00\\x00\\x00\\x00)\\xed\\xa9P\\t\\xb6\\xda*\\xb24\\x9e\\xd8A\\x10\\x01*\\x13\\n\\x11\\n\\nLoss/train\\x15\\x8656?\\xd6j\\xe0\\x1f \\x00\\x00\\x00\\x00\\x00\\x00\\x00)\\xed\\xa9P\\tp\\\\-\\xb24\\x9e\\xd8A\\x10\\x01*\\x13\\n\\x11\\n\\nLoss/train\\x15e\\xe0Q?-#Z( \\x00\\x00\\x00\\x00\\x00\\x00\\x00)\\xed\\xa9P\\t(~/\\xb24\\x9e\\xd8A\\x10\\x01*\\x13\\n\\x11\\n\\nLoss/train\\x15\\xf2\\x02a?S\\xe6\\x18\\x98 \\x00\\x00\\x00\\x00\\x00\\x00\\x00)\\xed\\xa9P\\t=\\xd91\\xb24\\x9e\\xd8A\\x10\\x01*\\x13\\n\\x11\\n\\nLoss/train\\x15d\\x86Z?\\x86\\xfc-\\x14 \\x00\\x00\\x00\\x00\\x00\\x00\\x00)\\xed\\xa9P\\t\\xd6\\xfd3\\xb24\\x9e\\xd8A\\x10\\x01*\\x13\\n\\x11\\n\\nLoss/train\\x15\\xd3\\x89Y?!>a\\x14 \\x00\\x00\\x00\\x00\\x00\\x00\\x00)\\xed\\xa9P\\t\\x93\\x1f6\\xb24\\x9e\\xd8A\\x10\\x01*\\x13\\n\\x11\\n\\nLoss/train\\x15\\xbc\\tU?k\\x06)\\xf2 \\x00\\x00\\x00\\x00\\x00\\x00\\x00)\\xed\\xa9P\\t\\xf5\\xa18\\xb24\\x9e\\xd8A\\x10\\x01*\\x13\\n\\x11\\n\\nLoss/train\\x15\\x9aab??%\\x1f\\xce \\x00\\x00\\x00\\x00\\x00\\x00\\x00)\\xed\\xa9P\\tq\\xca:\\xb24\\x9e\\xd8A\\x10\\x01*\\x13\\n\\x11\\n\\nLoss/train\\x15dAI?\\x9d\\xfdL) \\x00\\x00\\x00\\x00\\x00\\x00\\x00)\\xed\\xa9P\\t1\\xec<\\xb24\\x9e\\xd8A\\x10\\x01*\\x13\\n\\x11\\n\\nLoss/train\\x15\\xf4\\x12J?\\x05\\xb6S\\xa0 \\x00\\x00\\x00\\x00\\x00\\x00\\x00)\\xed\\xa9P\\t\\x01\\x8c?\\xb24\\x9e\\xd8A\\x10\\x01*\\x13\\n\\x11\\n\\nLoss/train\\x152\\xd4o?[\\x02\\xf21 \\x00\\x00\\x00\\x00\\x00\\x00\\x00)\\xed\\xa9P\\tM\\x13B\\xb24\\x9e\\xd8A\\x10\\x01*\\x13\\n\\x11\\n\\nLoss/train\\x15\\xee\\xbf0?|av\\xc9 \\x00\\x00\\x00\\x00\\x00\\x00\\x00)\\xed\\xa9P\\t\\xd7\\x8cD\\xb24\\x9e\\xd8A\\x10\\x01*\\x13\\n\\x11\\n\\nLoss/train\\x15s\\xebb?\\x0e\\xb7\\xe0c \\x00\\x00\\x00\\x00\\x00\\x00\\x00)\\xed\\xa9P\\tc\\xb7F\\xb24\\x9e\\xd8A\\x10\\x01*\\x13\\n\\x11\\n\\nLoss/train\\x15\\x8b\\x89Q?\\xc9\\npi \\x00\\x00\\x00\\x00\\x00\\x00\\x00)\\xed\\xa9P\\t\\xac\\xfeH\\xb24\\x9e\\xd8A\\x10\\x01*\\x13\\n\\x11\\n\\nLoss/train\\x15\\xee\\xc8|?\\xfb]\\xfa\\xba \\x00\\x00\\x00\\x00\\x00\\x00\\x00)\\xed\\xa9P\\tJCK\\xb24\\x9e\\xd8A\\x10\\x01*\\x13\\n\\x11\\n\\nLoss/train\\x15%\\xb4O?\\x18l\\xdf$ \\x00\\x00\\x00\\x00\\x00\\x00\\x00)\\xed\\xa9P\\t\\x1e\\xc1M\\xb24\\x9e\\xd8A\\x10\\x01*\\x13\\n\\x11\\n\\nLoss/train\\x158\\xd0 ?\\xe7\\xe8X\\xa7 \\x00\\x00\\x00\\x00\\x00\\x00\\x00)\\xed\\xa9P\\t6\\xe9O\\xb24\\x9e\\xd8A\\x10\\x01*\\x13\\n\\x11\\n\\nLoss/train\\x153$=?\\x84v\\x9c\\xf9 \\x00\\x00\\x00\\x00\\x00\\x00\\x00)\\xed\\xa9P\\t\\x86WR\\xb24\\x9e\\xd8A\\x10\\x01*\\x13\\n\\x11\\n\\nLoss/train\\x15\\x90\\xff\\x8f?f\\xbc\\'\\x1b \\x00\\x00\\x00\\x00\\x00\\x00\\x00)\\xed\\xa9P\\t\\x0c\\xb0T\\xb24\\x9e\\xd8A\\x10\\x01*\\x13\\n\\x11\\n\\nLoss/train\\x15\\xa1\\x89N?\\xafr\\x0eA \\x00\\x00\\x00\\x00\\x00\\x00\\x00)\\xed\\xa9P\\tE\\x0fW\\xb24\\x9e\\xd8A\\x10\\x01*\\x13\\n\\x11\\n\\nLoss/train\\x15\\x1d\\x83C?\\x8c-\\xd8\\xad \\x00\\x00\\x00\\x00\\x00\\x00\\x00)\\xed\\xa9P\\tQ\\xbeY\\xb24\\x9e\\xd8A\\x10\\x01*\\x13\\n\\x11\\n\\nLoss/train\\x15\\xc5w[?dk!f \\x00\\x00\\x00\\x00\\x00\\x00\\x00)\\xed\\xa9P\\t\"W\\\\\\xb24\\x9e\\xd8A\\x10\\x01*\\x13\\n\\x11\\n\\nLoss/train\\x15\\xdcEV?\\x81y\\x89\\x9c \\x00\\x00\\x00\\x00\\x00\\x00\\x00)\\xed\\xa9P\\t\\x8f\\xc2^\\xb24\\x9e\\xd8A\\x10\\x01*\\x13\\n\\x11\\n\\nLoss/train\\x15\\x188N?\\x8co8\\xbe \\x00\\x00\\x00\\x00\\x00\\x00\\x00)\\xed\\xa9P\\t\\xd7\\xf7`\\xb24\\x9e\\xd8A\\x10\\x01*\\x13\\n\\x11\\n\\nLoss/train\\x15\\xc7\\xcfR?\\xfb\\xf3\\r) \\x00\\x00\\x00\\x00\\x00\\x00\\x00)\\xed\\xa9P\\t.!c\\xb24\\x9e\\xd8A\\x10\\x01*\\x13\\n\\x11\\n\\nLoss/train\\x15\\x00\\xad\"?My\\x18} \\x00\\x00\\x00\\x00\\x00\\x00\\x00)\\xed\\xa9P\\tkIe\\xb24\\x9e\\xd8A\\x10\\x01*\\x13\\n\\x11\\n\\nLoss/train\\x15\\xce`P?\\xea\\x97\\xc5\\n \\x00\\x00\\x00\\x00\\x00\\x00\\x00)\\xed\\xa9P\\t\\x08ng\\xb24\\x9e\\xd8A\\x10\\x01*\\x13\\n\\x11\\n\\nLoss/train\\x15\\xf7zA?\\xf6\\\\\\xde% \\x00\\x00\\x00\\x00\\x00\\x00\\x00)\\xed\\xa9P\\t\\xd9\\x97i\\xb24\\x9e\\xd8A\\x10\\x01*\\x13\\n\\x11\\n\\nLoss/train\\x15\\xcc\\x93E?\\x0c\\xfe\\xb8\\xfc \\x00\\x00\\x00\\x00\\x00\\x00\\x00)\\xed\\xa9P\\t\"\\xe3k\\xb24\\x9e\\xd8A\\x10\\x01*\\x13\\n\\x11\\n\\nLoss/train\\x15C8h?\\'G\\xd2\\x89 \\x00\\x00\\x00\\x00\\x00\\x00\\x00)\\xed\\xa9P\\t\\xe3\\xbfn\\xb24\\x9e\\xd8A\\x10\\x01*\\x13\\n\\x11\\n\\nLoss/train\\x15\\x8d\\x0eF?\\x91\\xb6\\x02X \\x00\\x00\\x00\\x00\\x00\\x00\\x00)\\xed\\xa9P\\t\\x13Gq\\xb24\\x9e\\xd8A\\x10\\x01*\\x13\\n\\x11\\n\\nLoss/train\\x15\\x8aJj?o(\\xf2\\xfb \\x00\\x00\\x00\\x00\\x00\\x00\\x00)\\xed\\xa9P\\t\\x90ks\\xb24\\x9e\\xd8A\\x10\\x01*\\x13\\n\\x11\\n\\nLoss/train\\x15\\x94\\x9c5?l1\\xc1\\x10 \\x00\\x00\\x00\\x00\\x00\\x00\\x00)\\xed\\xa9P\\t\\xc9\\xe4u\\xb24\\x9e\\xd8A\\x10\\x01*\\x13\\n\\x11\\n\\nLoss/train\\x15\\x1b\\xbaR?\\xe3XG\\x94 \\x00\\x00\\x00\\x00\\x00\\x00\\x00)\\xed\\xa9P\\t\\xcffx\\xb24\\x9e\\xd8A\\x10\\x01*\\x13\\n\\x11\\n\\nLoss/train\\x15_\\x04`?\\x1dW_\\xa6 \\x00\\x00\\x00\\x00\\x00\\x00\\x00)\\xed\\xa9P\\tF\\xb8z\\xb24\\x9e\\xd8A\\x10\\x01*\\x13\\n\\x11\\n\\nLoss/train\\x15tVK?\\xa4\\xeeo\\xae \\x00\\x00\\x00\\x00\\x00\\x00\\x00)\\xed\\xa9P\\t{\\xd9|\\xb24\\x9e\\xd8A\\x10\\x01*\\x13\\n\\x11\\n\\nLoss/train\\x15\\x87L{?\\xc6\\xbc\\xd5\\xfa \\x00\\x00\\x00\\x00\\x00\\x00\\x00)\\xed\\xa9P\\t\\xbeO\\x7f\\xb24\\x9e\\xd8A\\x10\\x01*\\x13\\n\\x11\\n\\nLoss/train\\x15\\xbc\\xdaA?o\\xf7\\xc7\\xf4 \\x00\\x00\\x00\\x00\\x00\\x00\\x00)\\xed\\xa9P\\t?\\xa9\\x81\\xb24\\x9e\\xd8A\\x10\\x01*\\x13\\n\\x11\\n\\nLoss/train\\x15\\x8b+D?p\\xfc\\xa2\\xb2 \\x00\\x00\\x00\\x00\\x00\\x00\\x00)\\xed\\xa9P\\t\\x81\\xce\\x83\\xb24\\x9e\\xd8A\\x10\\x01*\\x13\\n\\x11\\n\\nLoss/train\\x15\\x18\\xb89?X\\x1d\\xaf/ \\x00\\x00\\x00\\x00\\x00\\x00\\x00)\\xed\\xa9P\\t>_\\x86\\xb24\\x9e\\xd8A\\x10\\x01*\\x13\\n\\x11\\n\\nLoss/train\\x15\\xad`D?7+\\xb4\\xc4 \\x00\\x00\\x00\\x00\\x00\\x00\\x00)\\xed\\xa9P\\tk\\xf5\\x88\\xb24\\x9e\\xd8A\\x10\\x01*\\x13\\n\\x11\\n\\nLoss/train\\x15\\xd2\\xceI?\\x96\\x07j\\xb6 \\x00\\x00\\x00\\x00\\x00\\x00\\x00)\\xed\\xa9P\\t\\xcb\\x7f\\x8b\\xb24\\x9e\\xd8A\\x10\\x01*\\x13\\n\\x11\\n\\nLoss/train\\x15\\x93\\xd38?S\\xc981 \\x00\\x00\\x00\\x00\\x00\\x00\\x00)\\xed\\xa9P\\t\\xb8\\xcd\\x8d\\xb24\\x9e\\xd8A\\x10\\x01*\\x13\\n\\x11\\n\\nLoss/train\\x15g\\xb7Z?\\xd4b\\x9fz \\x00\\x00\\x00\\x00\\x00\\x00\\x00)\\xed\\xa9P\\t\\x10\\x07\\x90\\xb24\\x9e\\xd8A\\x10\\x01*\\x13\\n\\x11\\n\\nLoss/train\\x15\\xa4\\xfcT?\\xf5\\xbdNp \\x00\\x00\\x00\\x00\\x00\\x00\\x00)\\xed\\xa9P\\t\\xfde\\x92\\xb24\\x9e\\xd8A\\x10\\x01*\\x13\\n\\x11\\n\\nLoss/train\\x15/\\xa97?\\x04D~\\x95 \\x00\\x00\\x00\\x00\\x00\\x00\\x00)\\xed\\xa9P\\t\\xa8\\x8c\\x94\\xb24\\x9e\\xd8A\\x10\\x01*\\x13\\n\\x11\\n\\nLoss/train\\x15\\xed]U?\\x1f\\xe3\\n\\x9f \\x00\\x00\\x00\\x00\\x00\\x00\\x00)\\xed\\xa9P\\t\\xed\\xba\\x96\\xb24\\x9e\\xd8A\\x10\\x01*\\x13\\n\\x11\\n\\nLoss/train\\x15\\xb4eG?P\\xffY\\r \\x00\\x00\\x00\\x00\\x00\\x00\\x00)\\xed\\xa9P\\t\\xefU\\x99\\xb24\\x9e\\xd8A\\x10\\x01*\\x13\\n\\x11\\n\\nLoss/train\\x15(U\\x1d?\\xaaNQ\\xfa \\x00\\x00\\x00\\x00\\x00\\x00\\x00)\\xed\\xa9P\\t\\x01\\xc4\\x9b\\xb24\\x9e\\xd8A\\x10\\x01*\\x13\\n\\x11\\n\\nLoss/train\\x15\\xe3$B?g\\xb1\\x06\\x9e \\x00\\x00\\x00\\x00\\x00\\x00\\x00)\\xed\\xa9P\\t6X\\x9e\\xb24\\x9e\\xd8A\\x10\\x01*\\x13\\n\\x11\\n\\nLoss/train\\x15\\x13\"\\x1d?\\xf6\\xd9\\xe1\\x0e \\x00\\x00\\x00\\x00\\x00\\x00\\x00)\\xed\\xa9P\\t\\xd9\\xed\\xa0\\xb24\\x9e\\xd8A\\x10\\x01*\\x13\\n\\x11\\n\\nLoss/train\\x15<UQ?&\\x12\\xcb\\r \\x00\\x00\\x00\\x00\\x00\\x00\\x00)\\xed\\xa9P\\t\\xd8c\\xa3\\xb24\\x9e\\xd8A\\x10\\x01*\\x13\\n\\x11\\n\\nLoss/train\\x15\\xff\\x9f\\x1b?a`\\x08\\xe2 \\x00\\x00\\x00\\x00\\x00\\x00\\x00)\\xed\\xa9P\\t8\\xd4\\xa5\\xb24\\x9e\\xd8A\\x10\\x01*\\x13\\n\\x11\\n\\nLoss/train\\x15\\'OF?\\xeevd\\xb4 \\x00\\x00\\x00\\x00\\x00\\x00\\x00)\\xed\\xa9P\\t!9\\xa8\\xb24\\x9e\\xd8A\\x10\\x01*\\x13\\n\\x11\\n\\nLoss/train\\x15\\x9e\\x88H?)b\\x17@ \\x00\\x00\\x00\\x00\\x00\\x00\\x00)\\xed\\xa9P\\t\\xafv\\xaa\\xb24\\x9e\\xd8A\\x10\\x01*\\x13\\n\\x11\\n\\nLoss/train\\x15$5s?\\xb7\\xbc\\xba\\xc8 \\x00\\x00\\x00\\x00\\x00\\x00\\x00)\\xed\\xa9P\\t\\xef\\xa9\\xac\\xb24\\x9e\\xd8A\\x10\\x01*\\x13\\n\\x11\\n\\nLoss/train\\x15)\\xd3,?\\xbb\\xf0\\xbb\\xfe \\x00\\x00\\x00\\x00\\x00\\x00\\x00)\\xed\\xa9P\\t;\\xfa\\xae\\xb24\\x9e\\xd8A\\x10\\x01*\\x13\\n\\x11\\n\\nLoss/train\\x15\\xb9\\xf8!?s\\xf49\\xf6 \\x00\\x00\\x00\\x00\\x00\\x00\\x00)\\xed\\xa9P\\t\\'*\\xb1\\xb24\\x9e\\xd8A\\x10\\x01*\\x13\\n\\x11\\n\\nLoss/train\\x15\\x8e\\xff0?\\xdci\\xb0\\xbd \\x00\\x00\\x00\\x00\\x00\\x00\\x00)\\xed\\xa9P\\tZb\\xb3\\xb24\\x9e\\xd8A\\x10\\x01*\\x13\\n\\x11\\n\\nLoss/train\\x15*37?P\\x82G\\xf7 \\x00\\x00\\x00\\x00\\x00\\x00\\x00)\\xed\\xa9P\\t`\\x08\\xb6\\xb24\\x9e\\xd8A\\x10\\x01*\\x13\\n\\x11\\n\\nLoss/train\\x15\\x91{H?\\xe5\\xe9\\x8cu \\x00\\x00\\x00\\x00\\x00\\x00\\x00)\\xed\\xa9P\\tm\\xab\\xb8\\xb24\\x9e\\xd8A\\x10\\x01*\\x13\\n\\x11\\n\\nLoss/train\\x15\\xd9\\xaa??\\x02\\x85W, \\x00\\x00\\x00\\x00\\x00\\x00\\x00)\\xed\\xa9P\\tNc\\xbb\\xb24\\x9e\\xd8A\\x10\\x01*\\x13\\n\\x11\\n\\nLoss/train\\x15\\xd6\\\\i?\\xc4\\x1d\\n\\xf9 \\x00\\x00\\x00\\x00\\x00\\x00\\x00)\\xed\\xa9P\\t\\x94\\xc1\\xbd\\xb24\\x9e\\xd8A\\x10\\x01*\\x13\\n\\x11\\n\\nLoss/train\\x15\\xb0\\xe57?J/0\\xbd \\x00\\x00\\x00\\x00\\x00\\x00\\x00)\\xed\\xa9P\\t\\xf2\\xe9\\xbf\\xb24\\x9e\\xd8A\\x10\\x01*\\x13\\n\\x11\\n\\nLoss/train\\x15\\x93sD?\"f\\x19\\xf8 \\x00\\x00\\x00\\x00\\x00\\x00\\x00)\\xed\\xa9P\\tx\\\\\\xc2\\xb24\\x9e\\xd8A\\x10\\x01*\\x13\\n\\x11\\n\\nLoss/train\\x15\\x7fm:?\\x96\\x8f\\xc4\\r \\x00\\x00\\x00\\x00\\x00\\x00\\x00)\\xed\\xa9P\\t\\x85\\x9b\\xc4\\xb24\\x9e\\xd8A\\x10\\x01*\\x13\\n\\x11\\n\\nLoss/train\\x15\\x1c+\\'?\\xce\\x9f|\\xd4 \\x00\\x00\\x00\\x00\\x00\\x00\\x00)\\xed\\xa9P\\t\\x91\\xb9\\xc6\\xb24\\x9e\\xd8A\\x10\\x01*\\x13\\n\\x11\\n\\nLoss/train\\x15>\\xd16?\\x8efM5 \\x00\\x00\\x00\\x00\\x00\\x00\\x00)\\xed\\xa9P\\t\\xc4>\\xc9\\xb24\\x9e\\xd8A\\x10\\x01*\\x13\\n\\x11\\n\\nLoss/train\\x15\\xd3\\xd1f?\\xddM\\xbb\\xb5 \\x00\\x00\\x00\\x00\\x00\\x00\\x00)\\xed\\xa9P\\t\\x16\\xa2\\xcb\\xb24\\x9e\\xd8A\\x10\\x01*\\x13\\n\\x11\\n\\nLoss/train\\x15\\xaeH>?J\\x9a\\\\G \\x00\\x00\\x00\\x00\\x00\\x00\\x00)\\xed\\xa9P\\t\\x82\\x19\\xce\\xb24\\x9e\\xd8A\\x10\\x01*\\x13\\n\\x11\\n\\nLoss/train\\x15\\x0f\\xa6M?\\xb0\\x10\\xcb\\x95 \\x00\\x00\\x00\\x00\\x00\\x00\\x00)\\xed\\xa9P\\tj\\xf9\\xd0\\xb24\\x9e\\xd8A\\x10\\x01*\\x13\\n\\x11\\n\\nLoss/train\\x15?\\x14,?\\x1a-\\xcb\\xd9 \\x00\\x00\\x00\\x00\\x00\\x00\\x00)\\xed\\xa9P\\t\\xa5e\\xd3\\xb24\\x9e\\xd8A\\x10\\x01*\\x13\\n\\x11\\n\\nLoss/train\\x15\\x1b\\x8cB?\\xa9\\x10\\x9f\\x1e \\x00\\x00\\x00\\x00\\x00\\x00\\x00)\\xed\\xa9P\\t\\xbd\\xc3\\xd5\\xb24\\x9e\\xd8A\\x10\\x01*\\x13\\n\\x11\\n\\nLoss/train\\x15+\\x18\\x0b?\\x16\\xb2\\x7f\\x1c \\x00\\x00\\x00\\x00\\x00\\x00\\x00)\\xed\\xa9P\\tx\\x08\\xd8\\xb24\\x9e\\xd8A\\x10\\x01*\\x13\\n\\x11\\n\\nLoss/train\\x15y\\r&?\\xdcF\\x86\\x1f \\x00\\x00\\x00\\x00\\x00\\x00\\x00)\\xed\\xa9P\\t\\xe5_\\xda\\xb24\\x9e\\xd8A\\x10\\x01*\\x13\\n\\x11\\n\\nLoss/train\\x15\\r\\n]?<\\x85\\xecS \\x00\\x00\\x00\\x00\\x00\\x00\\x00)\\xed\\xa9P\\t\\xdc\\x82\\xdc\\xb24\\x9e\\xd8A\\x10\\x01*\\x13\\n\\x11\\n\\nLoss/train\\x15-H\\x19?\\xfeI\\xb7\\xed \\x00\\x00\\x00\\x00\\x00\\x00\\x00)\\xed\\xa9P\\t\\x9f\\xac\\xde\\xb24\\x9e\\xd8A\\x10\\x01*\\x13\\n\\x11\\n\\nLoss/train\\x15x\\xa7F?\\xb1w\\x01\\x8d \\x00\\x00\\x00\\x00\\x00\\x00\\x00)\\xed\\xa9P\\t\\xfc\\xe1\\xe0\\xb24\\x9e\\xd8A\\x10\\x01*\\x13\\n\\x11\\n\\nLoss/train\\x15?\\xdeP?\\xfb\\x94\\x95\\x04 \\x00\\x00\\x00\\x00\\x00\\x00\\x00)\\xed\\xa9P\\tn\\xf6\\xe2\\xb24\\x9e\\xd8A\\x10\\x01*\\x13\\n\\x11\\n\\nLoss/train\\x15\\x16\\xea%?j\\xd0c\\x03 \\x00\\x00\\x00\\x00\\x00\\x00\\x00)\\xed\\xa9P\\tiT\\xe5\\xb24\\x9e\\xd8A\\x10\\x01*\\x13\\n\\x11\\n\\nLoss/train\\x15\\x03\\x82\\x14?id\\x9a\\xbb \\x00\\x00\\x00\\x00\\x00\\x00\\x00)\\xed\\xa9P\\t\\xb6\\xd7\\xe7\\xb24\\x9e\\xd8A\\x10\\x01*\\x13\\n\\x11\\n\\nLoss/train\\x15\\xb0\\x8d.?\\x9d\\x91\\xcc\\x98 \\x00\\x00\\x00\\x00\\x00\\x00\\x00)\\xed\\xa9P\\tv0\\xea\\xb24\\x9e\\xd8A\\x10\\x01*\\x13\\n\\x11\\n\\nLoss/train\\x15\\x83P\\t?,=D\\x8f \\x00\\x00\\x00\\x00\\x00\\x00\\x00)\\xed\\xa9P\\t\\nl\\xec\\xb24\\x9e\\xd8A\\x10\\x01*\\x13\\n\\x11\\n\\nLoss/train\\x15\\x85fB?0l\\x07i \\x00\\x00\\x00\\x00\\x00\\x00\\x00)\\xed\\xa9P\\t\\n\\x9e\\xee\\xb24\\x9e\\xd8A\\x10\\x01*\\x13\\n\\x11\\n\\nLoss/train\\x15\\xb7l>?\\x18\\xe3\\x19\\xb0 \\x00\\x00\\x00\\x00\\x00\\x00\\x00)\\xed\\xa9P\\t\\x06\\xd9\\xf0\\xb24\\x9e\\xd8A\\x10\\x01*\\x13\\n\\x11\\n\\nLoss/train\\x15\\x97YH?H[\\xf1\\x80 \\x00\\x00\\x00\\x00\\x00\\x00\\x00)\\xed\\xa9P\\t{\\xf6\\xf2\\xb24\\x9e\\xd8A\\x10\\x01*\\x13\\n\\x11\\n\\nLoss/train\\x15\\xb6\\xb6\\xff>y\\x8f\\xc6\\xfe \\x00\\x00\\x00\\x00\\x00\\x00\\x00)\\xed\\xa9P\\t\\x1d!\\xf5\\xb24\\x9e\\xd8A\\x10\\x01*\\x13\\n\\x11\\n\\nLoss/train\\x15\\xed\\xafC?/\\xe3\\x9f\\xe2 \\x00\\x00\\x00\\x00\\x00\\x00\\x00)\\xed\\xa9P\\t\\xdeW\\xf7\\xb24\\x9e\\xd8A\\x10\\x01*\\x13\\n\\x11\\n\\nLoss/train\\x15\\x10\\xc5C?\\x1d\\xd7\\x90\\xf2 \\x00\\x00\\x00\\x00\\x00\\x00\\x00)\\xed\\xa9P\\t\\x04\\xe4\\xf9\\xb24\\x9e\\xd8A\\x10\\x01*\\x13\\n\\x11\\n\\nLoss/train\\x15\\xb3\\x84&?\\xd1\\xd0k\\xb9 \\x00\\x00\\x00\\x00\\x00\\x00\\x00)\\xed\\xa9P\\tl\\x9a\\xfc\\xb24\\x9e\\xd8A\\x10\\x01*\\x13\\n\\x11\\n\\nLoss/train\\x15qiU?\\x96\\x93\\\\- \\x00\\x00\\x00\\x00\\x00\\x00\\x00)\\xed\\xa9P\\t\\xb12\\xff\\xb24\\x9e\\xd8A\\x10\\x01*\\x13\\n\\x11\\n\\nLoss/train\\x15n\\x14c?\\x84\\x8bG0 \\x00\\x00\\x00\\x00\\x00\\x00\\x00)\\xed\\xa9P\\ty\\xb1\\x01\\xb34\\x9e\\xd8A\\x10\\x01*\\x13\\n\\x11\\n\\nLoss/train\\x15M\\xb8R?\\n,\\xad\\x1e \\x00\\x00\\x00\\x00\\x00\\x00\\x00)\\xed\\xa9P\\t.\\xe8\\x03\\xb34\\x9e\\xd8A\\x10\\x01*\\x13\\n\\x11\\n\\nLoss/train\\x15\\xf5\\xa8\\'?\\xd5\\xad\\xb6\\xc5 \\x00\\x00\\x00\\x00\\x00\\x00\\x00)\\xed\\xa9P\\t\\xcb*\\x06\\xb34\\x9e\\xd8A\\x10\\x01*\\x13\\n\\x11\\n\\nLoss/train\\x15\\x00\\x9f??\\x0egS\\xa8 \\x00\\x00\\x00\\x00\\x00\\x00\\x00)\\xed\\xa9P\\t\\xdat\\x08\\xb34\\x9e\\xd8A\\x10\\x01*\\x13\\n\\x11\\n\\nLoss/train\\x15Q\\x08\\x12?c\\x14\\xb3\\xc2 \\x00\\x00\\x00\\x00\\x00\\x00\\x00)\\xed\\xa9P\\t\\xdd\\x93\\n\\xb34\\x9e\\xd8A\\x10\\x01*\\x13\\n\\x11\\n\\nLoss/train\\x15\\xdeP\\x04?\\xc85\\xce\\x7f \\x00\\x00\\x00\\x00\\x00\\x00\\x00)\\xed\\xa9P\\t\\xb9\\xc4\\x0c\\xb34\\x9e\\xd8A\\x10\\x01*\\x13\\n\\x11\\n\\nLoss/train\\x15\"-\\x1b?>\\x0f\\xb2\\x1b \\x00\\x00\\x00\\x00\\x00\\x00\\x00)\\xed\\xa9P\\t\\t\\xf9\\x0e\\xb34\\x9e\\xd8A\\x10\\x01*\\x13\\n\\x11\\n\\nLoss/train\\x15sfC?\\xe3\\x1f@$ \\x00\\x00\\x00\\x00\\x00\\x00\\x00)\\xed\\xa9P\\tR&\\x11\\xb34\\x9e\\xd8A\\x10\\x01*\\x13\\n\\x11\\n\\nLoss/train\\x15T\\x9a\\x16?\\x17\\x1e\\xc1\\xf6 \\x00\\x00\\x00\\x00\\x00\\x00\\x00)\\xed\\xa9P\\toI\\x13\\xb34\\x9e\\xd8A\\x10\\x01*\\x13\\n\\x11\\n\\nLoss/train\\x15\\xb7`B?\\x98\\xe2#) \\x00\\x00\\x00\\x00\\x00\\x00\\x00)\\xed\\xa9P\\t\\xfc\\x1a\\x16\\xb34\\x9e\\xd8A\\x10\\x01*\\x13\\n\\x11\\n\\nLoss/train\\x15\\x13\\x83@?\\xde\\x85\\r\\xd2 \\x00\\x00\\x00\\x00\\x00\\x00\\x00)\\xed\\xa9P\\t\\x93\\xc5\\x18\\xb34\\x9e\\xd8A\\x10\\x01*\\x13\\n\\x11\\n\\nLoss/train\\x15\\xb59Q?P\\x03\\x87\\xd9 \\x00\\x00\\x00\\x00\\x00\\x00\\x00)\\xed\\xa9P\\t\\x9e(\\x1b\\xb34\\x9e\\xd8A\\x10\\x01*\\x13\\n\\x11\\n\\nLoss/train\\x15\\x9b.3?\\x99\\x93\\x8af \\x00\\x00\\x00\\x00\\x00\\x00\\x00)\\xed\\xa9P\\tZa\\x1d\\xb34\\x9e\\xd8A\\x10\\x01*\\x13\\n\\x11\\n\\nLoss/train\\x15D|.?@\\xdc\\x16\\xf6 \\x00\\x00\\x00\\x00\\x00\\x00\\x00)\\xed\\xa9P\\t\\xae\\x81\\x1f\\xb34\\x9e\\xd8A\\x10\\x01*\\x13\\n\\x11\\n\\nLoss/train\\x15\\xc5\\x7fN?)\\x93G  \\x00\\x00\\x00\\x00\\x00\\x00\\x00)\\xed\\xa9P\\t\\xb4\\xad!\\xb34\\x9e\\xd8A\\x10\\x01*\\x13\\n\\x11\\n\\nLoss/train\\x15U\\xf7\\x12?\\x84\\xab\\xbe= \\x00\\x00\\x00\\x00\\x00\\x00\\x00)\\xed\\xa9P\\t\\xce\\xe0#\\xb34\\x9e\\xd8A\\x10\\x01*\\x13\\n\\x11\\n\\nLoss/train\\x15;w.?\\x81\\xee\\x9c@ \\x00\\x00\\x00\\x00\\x00\\x00\\x00)\\xed\\xa9P\\ti\\x00&\\xb34\\x9e\\xd8A\\x10\\x01*\\x13\\n\\x11\\n\\nLoss/train\\x15\\xc7\\x03\\xfa>g\\xa9\\xd0\\xb6 \\x00\\x00\\x00\\x00\\x00\\x00\\x00)\\xed\\xa9P\\t\\x1bJ(\\xb34\\x9e\\xd8A\\x10\\x01*\\x13\\n\\x11\\n\\nLoss/train\\x15\\x88\\x95E?\\x84\\xe6\\x11\\xd7 \\x00\\x00\\x00\\x00\\x00\\x00\\x00)\\xed\\xa9P\\t\\x82\\x8b*\\xb34\\x9e\\xd8A\\x10\\x01*\\x13\\n\\x11\\n\\nLoss/train\\x15\\xf3\\x08\\x08?:\\xbd\\xaf3 \\x00\\x00\\x00\\x00\\x00\\x00\\x00)\\xed\\xa9P\\tc\\xba-\\xb34\\x9e\\xd8A\\x10\\x01*\\x13\\n\\x11\\n\\nLoss/train\\x15n3\"?\\xf4=\\x90\\xb1 \\x00\\x00\\x00\\x00\\x00\\x00\\x00)\\xed\\xa9P\\t\\xf6(0\\xb34\\x9e\\xd8A\\x10\\x01*\\x13\\n\\x11\\n\\nLoss/train\\x15\\xa2\\x84a?\\x02`\\xb8l \\x00\\x00\\x00\\x00\\x00\\x00\\x00)\\xed\\xa9P\\t\\xa3\\x962\\xb34\\x9e\\xd8A\\x10\\x01*\\x13\\n\\x11\\n\\nLoss/train\\x15\\x82+N?\\x8a_\\xd4- \\x00\\x00\\x00\\x00\\x00\\x00\\x00)\\xed\\xa9P\\t\\xb4\\xcb4\\xb34\\x9e\\xd8A\\x10\\x01*\\x13\\n\\x11\\n\\nLoss/train\\x15Z\\xd76?.\\t1\\x89 \\x00\\x00\\x00\\x00\\x00\\x00\\x00)\\xed\\xa9P\\t\\xcf\\xf06\\xb34\\x9e\\xd8A\\x10\\x01*\\x13\\n\\x11\\n\\nLoss/train\\x15L\\x1a&?\\xb2\\xe8\\x89\\xa6 \\x00\\x00\\x00\\x00\\x00\\x00\\x00)\\xed\\xa9P\\t\\x18C9\\xb34\\x9e\\xd8A\\x10\\x01*\\x13\\n\\x11\\n\\nLoss/train\\x15y\\xd8-?\\xd5R\\xd9\\xb9 \\x00\\x00\\x00\\x00\\x00\\x00\\x00)\\xed\\xa9P\\t.r;\\xb34\\x9e\\xd8A\\x10\\x01*\\x13\\n\\x11\\n\\nLoss/train\\x15\\x10\\xce$?\\x06\\xcd~6 \\x00\\x00\\x00\\x00\\x00\\x00\\x00)\\xed\\xa9P\\t\\xf7\\xae=\\xb34\\x9e\\xd8A\\x10\\x01*\\x13\\n\\x11\\n\\nLoss/train\\x15W9\\x1f?`\\xd4\\x08\\x01 \\x00\\x00\\x00\\x00\\x00\\x00\\x00)\\xed\\xa9P\\tz\\xcb?\\xb34\\x9e\\xd8A\\x10\\x01*\\x13\\n\\x11\\n\\nLoss/train\\x15\\x0c\\x10\\x19?J\\xef\\xeb\\x18 \\x00\\x00\\x00\\x00\\x00\\x00\\x00)\\xed\\xa9P\\t\\xdc.B\\xb34\\x9e\\xd8A\\x10\\x01*\\x13\\n\\x11\\n\\nLoss/train\\x15\\xbb\\xa2&?\\x16Zl| \\x00\\x00\\x00\\x00\\x00\\x00\\x00)\\xed\\xa9P\\t\\x08\\xc9D\\xb34\\x9e\\xd8A\\x10\\x01*\\x13\\n\\x11\\n\\nLoss/train\\x15\\xb7\\xc54?\\xfc.\\xecq \\x00\\x00\\x00\\x00\\x00\\x00\\x00)\\xed\\xa9P\\t\\xc8#G\\xb34\\x9e\\xd8A\\x10\\x01*\\x13\\n\\x11\\n\\nLoss/train\\x15[($?\\xb4\\xe7\\x124 \\x00\\x00\\x00\\x00\\x00\\x00\\x00)\\xed\\xa9P\\t\\x0fGI\\xb34\\x9e\\xd8A\\x10\\x01*\\x13\\n\\x11\\n\\nLoss/train\\x15\\x88mJ?UD\\xd8\\xbc \\x00\\x00\\x00\\x00\\x00\\x00\\x00)\\xed\\xa9P\\t2\\x92K\\xb34\\x9e\\xd8A\\x10\\x01*\\x13\\n\\x11\\n\\nLoss/train\\x15\\xe82+?\\xe5IY\\xb9 \\x00\\x00\\x00\\x00\\x00\\x00\\x00)\\xed\\xa9P\\t\\xf0\\xc6M\\xb34\\x9e\\xd8A\\x10\\x01*\\x13\\n\\x11\\n\\nLoss/train\\x15\\xf0,-?\\x9fO \\x84 \\x00\\x00\\x00\\x00\\x00\\x00\\x00)\\xed\\xa9P\\t\\xa6\\xefO\\xb34\\x9e\\xd8A\\x10\\x01*\\x13\\n\\x11\\n\\nLoss/train\\x15\\xb3\\x17;?&m\\xad\\xf9 \\x00\\x00\\x00\\x00\\x00\\x00\\x00)\\xed\\xa9P\\t)AR\\xb34\\x9e\\xd8A\\x10\\x01*\\x13\\n\\x11\\n\\nLoss/train\\x15\\x10\\xc1/?j{\\xa9\\x9f \\x00\\x00\\x00\\x00\\x00\\x00\\x00)\\xed\\xa9P\\t\\x9dgT\\xb34\\x9e\\xd8A\\x10\\x01*\\x13\\n\\x11\\n\\nLoss/train\\x15\\x07-;?\\x9f\\xb7F\\xfd \\x00\\x00\\x00\\x00\\x00\\x00\\x00)\\xed\\xa9P\\t\\xd1\\x92V\\xb34\\x9e\\xd8A\\x10\\x01*\\x13\\n\\x11\\n\\nLoss/train\\x15[G4?K^\\xfb\\xcb \\x00\\x00\\x00\\x00\\x00\\x00\\x00)\\xed\\xa9P\\t[\\'Y\\xb34\\x9e\\xd8A\\x10\\x01*\\x13\\n\\x11\\n\\nLoss/train\\x15\\xf0e7?\\x8bR\\x13I \\x00\\x00\\x00\\x00\\x00\\x00\\x00)\\xed\\xa9P\\tX\\xe5[\\xb34\\x9e\\xd8A\\x10\\x01*\\x13\\n\\x11\\n\\nLoss/train\\x15\\x125.?\\xcf\\xbd\\xcfP \\x00\\x00\\x00\\x00\\x00\\x00\\x00)\\xed\\xa9P\\tvO^\\xb34\\x9e\\xd8A\\x10\\x01*\\x13\\n\\x11\\n\\nLoss/train\\x15\\x1c\\xe00?%\\x06\\x10E \\x00\\x00\\x00\\x00\\x00\\x00\\x00)\\xed\\xa9P\\thu`\\xb34\\x9e\\xd8A\\x10\\x01*\\x13\\n\\x11\\n\\nLoss/train\\x15\\xf8\\x8aB?\\x05\\xe0\\x87F \\x00\\x00\\x00\\x00\\x00\\x00\\x00)\\xed\\xa9P\\tv\\xa4b\\xb34\\x9e\\xd8A\\x10\\x01*\\x13\\n\\x11\\n\\nLoss/train\\x15\\x9b\\xf03?\\x10\\xf2\\x11\\xe8 \\x00\\x00\\x00\\x00\\x00\\x00\\x00)\\xed\\xa9P\\t\\xee\\xd0d\\xb34\\x9e\\xd8A\\x10\\x01*\\x13\\n\\x11\\n\\nLoss/train\\x15n\\x8aJ?:\\x8eZ\\r \\x00\\x00\\x00\\x00\\x00\\x00\\x00)\\xed\\xa9P\\t\\xec\\xf8f\\xb34\\x9e\\xd8A\\x10\\x01*\\x13\\n\\x11\\n\\nLoss/train\\x15\\xc0\\x83>?\\xea\\xeb\\x82m \\x00\\x00\\x00\\x00\\x00\\x00\\x00)\\xed\\xa9P\\t\\x9b\\x1fi\\xb34\\x9e\\xd8A\\x10\\x01*\\x13\\n\\x11\\n\\nLoss/train\\x15\\x12m>?\\x08\\x80S\\xfb \\x00\\x00\\x00\\x00\\x00\\x00\\x00)\\xed\\xa9P\\tZIk\\xb34\\x9e\\xd8A\\x10\\x01*\\x13\\n\\x11\\n\\nLoss/train\\x15!\\xb52?\\x89k\\x04A \\x00\\x00\\x00\\x00\\x00\\x00\\x00)\\xed\\xa9P\\t\\x90\\x85m\\xb34\\x9e\\xd8A\\x10\\x01*\\x13\\n\\x11\\n\\nLoss/train\\x15U\\xd6\\x0f?U\\x8dD\\x8d \\x00\\x00\\x00\\x00\\x00\\x00\\x00)\\xed\\xa9P\\ta2p\\xb34\\x9e\\xd8A\\x10\\x01*\\x13\\n\\x11\\n\\nLoss/train\\x15\\xaa\\xcbd?\\xf0(}\\xfd \\x00\\x00\\x00\\x00\\x00\\x00\\x00)\\xed\\xa9P\\t\\xb0\\xe6r\\xb34\\x9e\\xd8A\\x10\\x01*\\x13\\n\\x11\\n\\nLoss/train\\x15\\xdeP ?P>\\xa5\\x93 \\x00\\x00\\x00\\x00\\x00\\x00\\x00)\\xed\\xa9P\\twOu\\xb34\\x9e\\xd8A\\x10\\x01*\\x13\\n\\x11\\n\\nLoss/train\\x15Bc\\x1a?\\xd4\\x05\\xbf\\x05 \\x00\\x00\\x00\\x00\\x00\\x00\\x00)\\xed\\xa9P\\t\\xcf\\x7fw\\xb34\\x9e\\xd8A\\x10\\x01*\\x13\\n\\x11\\n\\nLoss/train\\x15GX\\n?/E\\xb9( \\x00\\x00\\x00\\x00\\x00\\x00\\x00)\\xed\\xa9P\\t\\x16\\xc0y\\xb34\\x9e\\xd8A\\x10\\x01*\\x13\\n\\x11\\n\\nLoss/train\\x15\\xe2Q\\x1b?\\xea\\r\\xd1K \\x00\\x00\\x00\\x00\\x00\\x00\\x00)\\xed\\xa9P\\t\\x00;|\\xb34\\x9e\\xd8A\\x10\\x01*\\x13\\n\\x11\\n\\nLoss/train\\x15\\xda\\xfbW?\\xd2i\\xbe0 \\x00\\x00\\x00\\x00\\x00\\x00\\x00)\\xed\\xa9P\\t\\x01o~\\xb34\\x9e\\xd8A\\x10\\x01*\\x13\\n\\x11\\n\\nLoss/train\\x15si\\x18?K\\xa2\\x1b8 \\x00\\x00\\x00\\x00\\x00\\x00\\x00)\\xed\\xa9P\\t\\xa3\\x93\\x80\\xb34\\x9e\\xd8A\\x10\\x01*\\x13\\n\\x11\\n\\nLoss/train\\x159D\\x1e?\\xa2\\x85\\x92\\x96 \\x00\\x00\\x00\\x00\\x00\\x00\\x00)\\xed\\xa9P\\tG\\xcd\\x82\\xb34\\x9e\\xd8A\\x10\\x01*\\x13\\n\\x11\\n\\nLoss/train\\x15\\xff\\xbe!?\\xda\\xc2pT \\x00\\x00\\x00\\x00\\x00\\x00\\x00)\\xed\\xa9P\\tF\\x0c\\x85\\xb34\\x9e\\xd8A\\x10\\x01*\\x13\\n\\x11\\n\\nLoss/train\\x15i\\xda%? n\\xa1\\xa3 \\x00\\x00\\x00\\x00\\x00\\x00\\x00)\\xed\\xa9P\\tod\\x87\\xb34\\x9e\\xd8A\\x10\\x01*\\x13\\n\\x11\\n\\nLoss/train\\x15B\\xce$?&\\xbdf} \\x00\\x00\\x00\\x00\\x00\\x00\\x00)\\xed\\xa9P\\t\\x8d(\\x8a\\xb34\\x9e\\xd8A\\x10\\x01*\\x13\\n\\x11\\n\\nLoss/train\\x15\\xdb\\xbf%?\\n\\xec\\x03\\xf2 \\x00\\x00\\x00\\x00\\x00\\x00\\x00)\\xed\\xa9P\\tM\\xbc\\x8c\\xb34\\x9e\\xd8A\\x10\\x01*\\x13\\n\\x11\\n\\nLoss/train\\x15\\xc2\\xc18?\\xc5\\xf4\\xecp \\x00\\x00\\x00\\x00\\x00\\x00\\x00)\\xed\\xa9P\\t\\x18\\x08\\x8f\\xb34\\x9e\\xd8A\\x10\\x01*\\x13\\n\\x11\\n\\nLoss/train\\x15\\xd5\\'$?x\\xe4{\\xbe \\x00\\x00\\x00\\x00\\x00\\x00\\x00)\\xed\\xa9P\\t\\xef6\\x91\\xb34\\x9e\\xd8A\\x10\\x01*\\x13\\n\\x11\\n\\nLoss/train\\x15\\xa7Q\\t?\\x1c\\x1b,\\xa2 \\x00\\x00\\x00\\x00\\x00\\x00\\x00)\\xed\\xa9P\\tOy\\x93\\xb34\\x9e\\xd8A\\x10\\x01*\\x13\\n\\x11\\n\\nLoss/train\\x159\\x9c2?\\x8e\\x88E= \\x00\\x00\\x00\\x00\\x00\\x00\\x00)\\xed\\xa9P\\t\\xd1\\xe8\\x95\\xb34\\x9e\\xd8A\\x10\\x01*\\x13\\n\\x11\\n\\nLoss/train\\x15\\xec\\x86h?{\\x80\\x02\\x8a \\x00\\x00\\x00\\x00\\x00\\x00\\x00)\\xed\\xa9P\\tn\\x16\\x98\\xb34\\x9e\\xd8A\\x10\\x01*\\x13\\n\\x11\\n\\nLoss/train\\x15\\x85>)?\\x86\\xde,\\x82 \\x00\\x00\\x00\\x00\\x00\\x00\\x00)\\xed\\xa9P\\t\\xff\\xb5\\x9a\\xb34\\x9e\\xd8A\\x10\\x01*\\x13\\n\\x11\\n\\nLoss/train\\x15-\\x9d\\x1a?9\\x04\\x10\\xed \\x00\\x00\\x00\\x00\\x00\\x00\\x00)\\xed\\xa9P\\t\\xdb\\xfa\\x9c\\xb34\\x9e\\xd8A\\x10\\x01*\\x13\\n\\x11\\n\\nLoss/train\\x15\\x92\\xd4D?\\xa37\\x97\\x9f \\x00\\x00\\x00\\x00\\x00\\x00\\x00)\\xed\\xa9P\\t\\xbc\\xae\\x9f\\xb34\\x9e\\xd8A\\x10\\x01*\\x13\\n\\x11\\n\\nLoss/train\\x15\\x91/H?S\\xdd\\xb4\\xa7 \\x00\\x00\\x00\\x00\\x00\\x00\\x00)\\xed\\xa9P\\tp?\\xa2\\xb34\\x9e\\xd8A\\x10\\x01*\\x13\\n\\x11\\n\\nLoss/train\\x153\\xb6\\x11?\\x1f\\xb6RT \\x00\\x00\\x00\\x00\\x00\\x00\\x00)\\xed\\xa9P\\tl\\x93\\xa4\\xb34\\x9e\\xd8A\\x10\\x01*\\x13\\n\\x11\\n\\nLoss/train\\x15\\xf7\\xaa\\x1f?\\x98\\xe9\\'d \\x00\\x00\\x00\\x00\\x00\\x00\\x00)\\xed\\xa9P\\t\\xd0\\xb9\\xa6\\xb34\\x9e\\xd8A\\x10\\x01*\\x13\\n\\x11\\n\\nLoss/train\\x15\\xb9\\xcc(?\\x17\\xc8\\xccb \\x00\\x00\\x00\\x00\\x00\\x00\\x00)\\xed\\xa9P\\t\\x0f\\xf0\\xa8\\xb34\\x9e\\xd8A\\x10\\x01*\\x13\\n\\x11\\n\\nLoss/train\\x150c9?\\xc4i\\xa9i \\x00\\x00\\x00\\x00\\x00\\x00\\x00)\\xed\\xa9P\\t\\xa5\\x13\\xab\\xb34\\x9e\\xd8A\\x10\\x01*\\x13\\n\\x11\\n\\nLoss/train\\x15\\xeb\\xad+?\\x8e\\x03\\xecn \\x00\\x00\\x00\\x00\\x00\\x00\\x00)\\xed\\xa9P\\t\\xf5O\\xad\\xb34\\x9e\\xd8A\\x10\\x01*\\x13\\n\\x11\\n\\nLoss/train\\x15xf\\x1f?Y\\xcb\\x06\\x18 \\x00\\x00\\x00\\x00\\x00\\x00\\x00)\\xed\\xa9P\\t\\x99~\\xaf\\xb34\\x9e\\xd8A\\x10\\x01*\\x13\\n\\x11\\n\\nLoss/train\\x15O\\xbeM?t\\xb1\\xdda \\x00\\x00\\x00\\x00\\x00\\x00\\x00)\\xed\\xa9P\\t\\xeb\\xc5\\xb1\\xb34\\x9e\\xd8A\\x10\\x01*\\x13\\n\\x11\\n\\nLoss/train\\x15\\x97\\xe5\\x1f?\\xd8\\xa2W\\xb6 \\x00\\x00\\x00\\x00\\x00\\x00\\x00)\\xed\\xa9P\\t\\xc8\\x03\\xb4\\xb34\\x9e\\xd8A\\x10\\x01*\\x13\\n\\x11\\n\\nLoss/train\\x15\\x9c\\x05/?X\\x8b9\\x85 \\x00\\x00\\x00\\x00\\x00\\x00\\x00)\\xed\\xa9P\\t\\xa6y\\xb6\\xb34\\x9e\\xd8A\\x10\\x01*\\x13\\n\\x11\\n\\nLoss/train\\x15XK#?]\\xb8\\x9d\\xc7 \\x00\\x00\\x00\\x00\\x00\\x00\\x00)\\xed\\xa9P\\tqX\\xb9\\xb34\\x9e\\xd8A\\x10\\x01*\\x13\\n\\x11\\n\\nLoss/train\\x15q\\xf29?\\xd2\\xc1\\xa9I\\x94s.'\n",
      "Key: epochs, Value: 1\n",
      "Key: train_loss, Value: (2.358812093734741, 2.285137891769409, 2.3066349029541016, 2.2708795070648193, 2.2367401123046875, 2.24334716796875, 2.1832549571990967, 2.1485116481781006, 2.1049115657806396, 2.0044069290161133, 1.8622523546218872, 1.8843708038330078, 1.7973158359527588, 1.6879109144210815, 1.508046269416809, 1.764279842376709, 1.4700727462768555, 1.3514467477798462, 1.2905819416046143, 1.0177571773529053, 1.042162299156189, 1.0987662076950073, 1.2285516262054443, 1.1495932340621948, 0.8452475070953369, 0.9741130471229553, 0.8569056987762451, 0.9234588146209717, 1.0218565464019775, 0.8069543242454529, 0.8789511919021606, 0.8185049891471863, 0.8055434226989746, 0.8231522440910339, 0.8543609976768494, 0.7746452689170837, 0.718348503112793, 0.5433375239372253, 0.7593768239021301, 0.65492182970047, 0.6999298930168152, 0.8053513765335083, 0.790733814239502, 0.7599329948425293, 0.540409505367279, 0.6412327885627747, 0.6593738198280334)\n"
     ]
    }
   ],
   "source": [
    "# Check array directory\n",
    "pprint(glob.glob('tiledb-pytorch-mnist-1/*'))\n",
    "\n",
    "# Open in write mode in order to add metadata\n",
    "print()\n",
    "model_array_1 = tiledb.open('tiledb-pytorch-mnist-1')\n",
    "for key, value in model_array_1.meta.items():\n",
    "    if isinstance(value, bytes)  and key != '__TENSORBOARD__':\n",
    "        value = json.loads(value)\n",
    "    print(\"Key: {}, Value: {}\".format(key, value))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, in array's metadata we have by default information about the backend we used for training (pytorch),\n",
    "pytorch version, python version and the extra metadata about epochs and training loss that we added.\n",
    "We can load and check any of the aforementioned without having to load the entire model in memory.\n",
    "Moreover, we can add any kind of extra information in model's metadata also by opening the TileDB array and adding new keys."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Key: TILEDB_ML_MODEL_ML_FRAMEWORK, Value: PYTORCH\n",
      "Key: TILEDB_ML_MODEL_ML_FRAMEWORK_VERSION, Value: 1.10.2\n",
      "Key: TILEDB_ML_MODEL_PREVIEW, Value: Net(\n",
      "  (conv1): Conv2d(1, 10, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2): Conv2d(10, 20, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2_drop): Dropout2d(p=0.5, inplace=False)\n",
      "  (fc1): Linear(in_features=320, out_features=50, bias=True)\n",
      "  (fc2): Linear(in_features=50, out_features=10, bias=True)\n",
      ")\n",
      "Key: TILEDB_ML_MODEL_PYTHON_VERSION, Value: 3.9.5\n",
      "Key: TILEDB_ML_MODEL_STAGE, Value: STAGING\n",
      "Key: epochs, Value: 1\n",
      "Key: new_meta, Value: [\"Any kind of info\"]\n",
      "Key: train_loss, Value: (2.358812093734741, 2.285137891769409, 2.3066349029541016, 2.2708795070648193, 2.2367401123046875, 2.24334716796875, 2.1832549571990967, 2.1485116481781006, 2.1049115657806396, 2.0044069290161133, 1.8622523546218872, 1.8843708038330078, 1.7973158359527588, 1.6879109144210815, 1.508046269416809, 1.764279842376709, 1.4700727462768555, 1.3514467477798462, 1.2905819416046143, 1.0177571773529053, 1.042162299156189, 1.0987662076950073, 1.2285516262054443, 1.1495932340621948, 0.8452475070953369, 0.9741130471229553, 0.8569056987762451, 0.9234588146209717, 1.0218565464019775, 0.8069543242454529, 0.8789511919021606, 0.8185049891471863, 0.8055434226989746, 0.8231522440910339, 0.8543609976768494, 0.7746452689170837, 0.718348503112793, 0.5433375239372253, 0.7593768239021301, 0.65492182970047, 0.6999298930168152, 0.8053513765335083, 0.790733814239502, 0.7599329948425293, 0.540409505367279, 0.6412327885627747, 0.6593738198280334)\n"
     ]
    }
   ],
   "source": [
    "# Open the array in write mode\n",
    "with tiledb.Array('tiledb-pytorch-mnist-1', \"w\") as A:\n",
    "    # Keep all history\n",
    "    A.meta['new_meta'] = json.dumps(['Any kind of info'])\n",
    "\n",
    "# Check that everything is there\n",
    "model_array_1 = tiledb.open('tiledb-pytorch-mnist-1')\n",
    "for key, value in model_array_1.meta.items():\n",
    "    if isinstance(value, bytes):\n",
    "        value = json.loads(value)\n",
    "    print(\"Key: {}, Value: {}\".format(key, value))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the case of PyTorch models, internally, we save model's state_dict and optimizer's state_dict,\n",
    "as [variable sized attributes)](https://docs.tiledb.com/main/how-to/arrays/writing-arrays/var-length-attributes)\n",
    "(pickled), i.e., we can open the TileDB and get only the state_dict of the model or optimizer,\n",
    "without bringing the whole model in memory. For example, we can load model's and optimizer's state_dict\n",
    "for model tiledb-pytorch-mnist-1 as follows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OrderedDict([('conv1.weight', tensor([[[[ 1.2094e-01, -8.5941e-02, -6.2444e-02,  5.8970e-02, -2.0762e-01],\n",
      "          [ 1.5059e-01, -2.9233e-02,  9.7367e-02,  2.8207e-02, -1.6851e-02],\n",
      "          [ 9.4352e-02,  4.5804e-02,  1.0320e-01, -3.8633e-02,  1.3617e-02],\n",
      "          [ 1.9258e-04,  5.2862e-02,  2.5137e-02,  2.0193e-01,  8.8988e-02],\n",
      "          [-8.0275e-02, -1.3042e-01, -4.8912e-02, -9.3391e-02, -5.5979e-02]]],\n",
      "\n",
      "\n",
      "        [[[ 5.8930e-02,  1.9274e-01,  1.1763e-01, -2.8203e-01,  1.3414e-02],\n",
      "          [ 1.3715e-01,  2.8225e-01,  1.3725e-01, -2.5948e-01, -2.7058e-01],\n",
      "          [ 1.2401e-02,  2.8841e-01, -1.5566e-02,  3.6678e-02, -1.4582e-01],\n",
      "          [ 2.9282e-01,  1.6428e-02,  1.9005e-01,  6.6314e-03, -1.0706e-01],\n",
      "          [ 1.8062e-01, -2.4322e-02,  1.2149e-01, -7.4748e-03,  2.0982e-02]]],\n",
      "\n",
      "\n",
      "        [[[-2.2462e-01, -1.0825e-01,  1.6450e-01,  5.5100e-02,  2.8822e-01],\n",
      "          [ 1.5019e-01,  2.7266e-02, -2.1150e-02,  2.5508e-01,  1.4146e-01],\n",
      "          [-1.0837e-01,  2.0830e-01,  2.4816e-01,  2.9263e-01, -7.8893e-02],\n",
      "          [ 8.3816e-03,  7.3567e-02,  1.2236e-01, -9.8346e-02, -1.5182e-01],\n",
      "          [ 1.3673e-01,  5.5743e-03,  1.4508e-01,  2.1074e-02,  4.4479e-02]]],\n",
      "\n",
      "\n",
      "        [[[ 7.3997e-02,  1.1141e-01,  1.3513e-01, -1.5937e-01,  1.2615e-01],\n",
      "          [-1.3540e-01, -1.3885e-01,  2.4412e-02, -1.8662e-02,  1.3792e-01],\n",
      "          [-1.2008e-01, -1.8048e-01,  1.4064e-01, -2.8908e-02,  1.2296e-01],\n",
      "          [ 5.7939e-02, -1.5646e-01,  2.9573e-02,  3.9015e-02, -1.3845e-01],\n",
      "          [-1.0179e-01,  5.8935e-02,  2.1090e-02, -1.0104e-01, -7.2820e-02]]],\n",
      "\n",
      "\n",
      "        [[[ 2.8265e-01,  9.7737e-02,  2.4348e-01,  1.5866e-01, -1.2060e-01],\n",
      "          [-2.6344e-02,  4.2090e-01,  2.2141e-01,  2.5185e-01,  8.7390e-02],\n",
      "          [ 2.5203e-01,  4.2776e-01,  4.2293e-01, -1.4474e-02,  9.2655e-02],\n",
      "          [ 3.0842e-01,  3.6310e-01,  2.8501e-01, -6.0262e-02, -7.3063e-02],\n",
      "          [-3.4303e-02,  1.4463e-01,  2.8786e-01, -6.6910e-02,  1.8372e-01]]],\n",
      "\n",
      "\n",
      "        [[[-1.0966e-01, -1.9262e-01, -3.4474e-03,  2.3162e-01,  3.1775e-01],\n",
      "          [-9.2638e-02,  8.5335e-02,  2.7551e-01,  2.9201e-01,  2.6148e-01],\n",
      "          [ 1.8237e-01,  1.7741e-01,  1.4328e-01, -1.4558e-02, -8.7815e-02],\n",
      "          [-8.2011e-02, -1.2618e-03, -6.1363e-02,  1.6021e-01,  1.1372e-01],\n",
      "          [-6.4378e-02,  7.5386e-02,  1.0978e-01,  1.8243e-01,  1.6508e-01]]],\n",
      "\n",
      "\n",
      "        [[[-7.3436e-02,  1.8606e-01, -2.0623e-02,  1.0143e-02, -1.7651e-01],\n",
      "          [-1.4443e-01, -1.3213e-01, -8.3286e-02,  1.6307e-01, -5.4090e-02],\n",
      "          [-1.6180e-01,  7.8273e-02,  8.6733e-02,  1.5925e-01, -8.7825e-02],\n",
      "          [ 4.7167e-02, -5.2079e-02,  1.2832e-03, -1.1566e-01, -6.0104e-02],\n",
      "          [-4.5096e-02,  1.1652e-01,  8.5698e-02,  9.0698e-02,  2.3069e-01]]],\n",
      "\n",
      "\n",
      "        [[[ 1.8333e-01,  1.3493e-01,  1.0797e-02, -1.3546e-01,  1.4426e-01],\n",
      "          [-6.6069e-02, -2.6858e-02, -1.7451e-01, -5.0471e-02, -1.4550e-01],\n",
      "          [ 1.6927e-01, -1.7851e-01, -1.5688e-01, -1.2383e-02, -1.1069e-01],\n",
      "          [ 8.0756e-02, -5.0298e-02, -5.7470e-02,  4.9647e-02,  1.8387e-01],\n",
      "          [-1.1187e-01,  1.5371e-01,  2.0994e-01,  2.7118e-01, -8.7066e-02]]],\n",
      "\n",
      "\n",
      "        [[[ 4.8640e-02, -1.5364e-01,  1.8229e-01, -2.7139e-02,  2.0005e-01],\n",
      "          [ 9.8679e-03, -8.5198e-03,  2.7613e-02,  1.1363e-01,  2.0136e-01],\n",
      "          [ 3.7913e-02,  7.0007e-02, -1.4183e-01,  2.2730e-01,  2.7914e-01],\n",
      "          [ 1.5599e-01,  1.3730e-01, -1.3076e-01, -1.9008e-02,  2.5420e-01],\n",
      "          [-1.9463e-01,  1.4922e-01, -1.6149e-01, -1.1318e-01,  1.3929e-01]]],\n",
      "\n",
      "\n",
      "        [[[-1.2871e-01, -9.2169e-03,  2.2126e-01,  2.5386e-01,  2.7312e-01],\n",
      "          [-3.9777e-02, -1.3743e-01,  2.6556e-01,  4.5070e-02, -8.3500e-03],\n",
      "          [ 3.5777e-02, -1.3642e-01,  1.7272e-01,  1.8111e-01, -1.1670e-01],\n",
      "          [-1.8199e-01, -1.7659e-02,  1.0660e-01,  1.9154e-02, -8.8221e-02],\n",
      "          [-1.1774e-02, -1.8913e-01, -1.3152e-01, -1.3019e-01,  1.5553e-01]]]])), ('conv1.bias', tensor([ 0.1877, -0.0415,  0.0473,  0.0980,  0.1126, -0.0456,  0.1228,  0.1315,\n",
      "         0.0150,  0.0093])), ('conv2.weight', tensor([[[[ 5.5688e-02,  5.0323e-02, -4.1645e-02,  5.8802e-02,  1.7461e-02],\n",
      "          [-4.6975e-02,  2.6073e-02, -5.8855e-02,  5.9749e-02, -2.2266e-02],\n",
      "          [ 1.2298e-02,  9.4340e-03,  4.4289e-03,  5.2572e-02, -1.8529e-02],\n",
      "          [-3.7877e-02, -6.5628e-02, -6.3345e-03, -2.5061e-02,  4.2432e-04],\n",
      "          [-1.6862e-02,  2.2800e-02,  6.5047e-02,  3.2745e-02, -1.3664e-02]],\n",
      "\n",
      "         [[-3.7478e-02, -6.9350e-02,  3.3418e-02, -6.0245e-03,  4.6770e-02],\n",
      "          [-1.1183e-02, -6.7169e-02,  1.3454e-02,  4.4195e-02,  2.6528e-02],\n",
      "          [-5.1710e-02, -5.3922e-02,  8.9637e-03, -1.2199e-02, -3.8623e-02],\n",
      "          [-1.8659e-02, -1.9495e-02,  5.1787e-02, -6.4121e-03, -2.9470e-02],\n",
      "          [-2.2463e-02, -3.9903e-02,  8.0596e-02, -1.6780e-03,  3.9917e-02]],\n",
      "\n",
      "         [[-1.5541e-02, -3.6429e-02, -6.9846e-02, -5.2136e-03,  6.0159e-02],\n",
      "          [-8.1789e-02, -3.9465e-02,  6.4421e-02,  7.5873e-02,  3.4211e-02],\n",
      "          [-2.7526e-02,  1.3951e-02,  4.2703e-02, -6.0232e-03,  1.5761e-02],\n",
      "          [-2.6497e-02, -5.2499e-03,  2.8947e-02, -6.8603e-02, -3.4084e-02],\n",
      "          [ 2.9113e-02,  3.0167e-02,  3.3005e-02, -6.2668e-02, -1.7476e-03]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-5.4886e-02,  3.1016e-02,  8.8663e-03,  3.8638e-02,  5.0861e-02],\n",
      "          [-4.9791e-02,  7.7306e-02, -6.1382e-02,  2.1049e-02,  6.3304e-03],\n",
      "          [ 2.3999e-02, -2.9574e-02,  2.7499e-02, -2.5450e-02, -4.1954e-02],\n",
      "          [ 1.8014e-02,  3.0905e-02,  6.8800e-02,  2.3851e-02,  7.9991e-02],\n",
      "          [ 3.3960e-02, -3.9802e-02, -3.7312e-02,  5.1859e-02, -2.9487e-02]],\n",
      "\n",
      "         [[ 7.4754e-03, -5.4621e-02,  5.3075e-02,  2.5734e-02, -1.0738e-02],\n",
      "          [ 2.2757e-03, -4.6360e-02, -7.0277e-03,  1.8788e-02, -2.8173e-02],\n",
      "          [-2.6094e-02, -1.7771e-02,  5.7014e-02,  5.5233e-02,  5.0262e-02],\n",
      "          [ 5.3179e-02, -5.2183e-02,  3.8744e-03,  5.8395e-03, -6.1096e-02],\n",
      "          [ 2.9442e-02, -2.6877e-02,  4.5059e-03,  1.8200e-02,  5.4210e-02]],\n",
      "\n",
      "         [[-2.8648e-02, -5.6071e-02,  5.7827e-02,  3.2404e-02,  2.7678e-02],\n",
      "          [ 1.7313e-02, -4.1857e-02,  4.1774e-02, -4.0967e-02,  8.4482e-03],\n",
      "          [ 1.4055e-02,  4.3988e-02, -3.0981e-03, -3.0161e-02,  2.8427e-02],\n",
      "          [-6.1764e-02, -3.1399e-02, -4.0380e-02, -4.1164e-02, -1.8116e-02],\n",
      "          [-4.9330e-02, -9.6725e-03, -5.1042e-02,  3.2385e-02, -1.7551e-02]]],\n",
      "\n",
      "\n",
      "        [[[-4.3452e-02,  1.5854e-02, -5.0899e-02,  3.4294e-02,  2.7940e-02],\n",
      "          [-7.4259e-02, -6.0277e-02, -5.0499e-02, -3.6630e-02,  6.0665e-03],\n",
      "          [-7.4253e-02, -5.5786e-02,  2.8466e-02, -9.9154e-03, -1.3602e-02],\n",
      "          [-6.1736e-02,  1.4597e-02,  1.2049e-03, -2.3759e-02, -3.4440e-02],\n",
      "          [ 1.7737e-02,  1.3214e-02,  2.0545e-02,  3.9243e-03, -1.9609e-02]],\n",
      "\n",
      "         [[-5.2470e-02,  3.4858e-02, -1.3617e-02, -6.5290e-02,  5.2350e-02],\n",
      "          [ 1.6406e-02, -7.2818e-02, -3.7993e-02, -8.2651e-03, -1.4540e-02],\n",
      "          [ 6.2362e-03, -5.9854e-02,  6.1123e-02, -2.1097e-02,  4.8519e-02],\n",
      "          [-1.3449e-02, -2.5081e-02,  3.8062e-02, -7.5380e-02, -5.3551e-02],\n",
      "          [ 4.1617e-03,  4.9241e-02, -5.1492e-02, -6.9348e-02, -6.2666e-02]],\n",
      "\n",
      "         [[-3.6324e-02, -4.1899e-02, -3.4289e-02, -1.1614e-02,  6.6807e-02],\n",
      "          [-2.0730e-02, -3.9922e-02, -3.5429e-03,  5.8353e-02,  2.6961e-02],\n",
      "          [ 3.8102e-02,  2.2424e-02, -2.5756e-02, -3.7780e-02, -2.3828e-02],\n",
      "          [ 8.7532e-03,  2.7832e-02, -2.4130e-02,  1.4462e-02, -2.6144e-02],\n",
      "          [ 4.0355e-02, -1.1793e-03, -4.1850e-02,  4.1885e-02, -3.6663e-03]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-7.0607e-02,  2.0968e-02, -1.1691e-02, -1.9715e-02, -6.5066e-02],\n",
      "          [ 2.8988e-02, -3.4248e-02, -5.5552e-02, -2.3410e-02, -1.9724e-02],\n",
      "          [-5.7045e-02,  1.1606e-02, -3.2236e-02,  4.7339e-02,  5.8537e-02],\n",
      "          [-4.5309e-02, -4.1538e-02, -1.1850e-02,  3.5893e-02, -5.0144e-03],\n",
      "          [-2.6272e-02,  4.3843e-02,  7.0995e-02,  4.6005e-02, -4.8562e-02]],\n",
      "\n",
      "         [[-1.6963e-02, -2.4871e-02,  4.4745e-02, -2.3616e-02, -1.8955e-02],\n",
      "          [-1.1253e-02, -1.9488e-02,  3.6824e-02,  2.1252e-02, -3.3756e-02],\n",
      "          [ 3.9704e-02,  4.0178e-02,  3.3367e-02, -4.6919e-02,  2.7889e-02],\n",
      "          [ 4.0036e-02,  6.2874e-03, -2.8772e-02,  1.2537e-03, -2.3577e-02],\n",
      "          [ 5.2498e-02,  3.8492e-02, -2.5590e-02, -5.5962e-02,  2.3836e-02]],\n",
      "\n",
      "         [[ 3.2959e-02,  2.7211e-02, -7.1565e-02,  4.2409e-03, -1.8724e-02],\n",
      "          [-6.3511e-02, -5.0197e-02,  2.6501e-02,  4.0345e-02, -4.6381e-02],\n",
      "          [ 1.3889e-02,  5.8676e-02,  6.7019e-02,  4.6219e-02, -1.0121e-02],\n",
      "          [ 4.4267e-02,  6.2342e-02,  3.7357e-02, -3.8083e-02, -1.2602e-02],\n",
      "          [ 5.5295e-02,  4.1193e-02,  2.7278e-02, -2.7366e-02, -1.7019e-02]]],\n",
      "\n",
      "\n",
      "        [[[-3.0022e-02, -1.5105e-02,  6.6623e-02, -3.6930e-02, -7.3034e-03],\n",
      "          [ 2.9494e-02, -1.7090e-02,  3.8153e-02,  1.7798e-02,  4.9250e-02],\n",
      "          [-5.5555e-02, -1.5927e-03, -6.8216e-02,  3.3606e-02,  6.5747e-02],\n",
      "          [-2.8826e-02,  5.7012e-04, -4.7133e-02, -5.6715e-02, -3.9384e-02],\n",
      "          [-3.6551e-02,  5.6376e-03, -1.6357e-02, -2.7725e-03, -4.9651e-02]],\n",
      "\n",
      "         [[-2.5514e-02, -3.6148e-02, -3.4350e-02,  5.8074e-02,  5.1768e-02],\n",
      "          [-5.8352e-02, -1.0072e-02, -3.2340e-03,  3.1245e-02,  9.1773e-02],\n",
      "          [ 2.3100e-03,  3.0584e-02, -5.3055e-02, -3.7134e-02, -2.5670e-02],\n",
      "          [-2.3990e-03,  3.9612e-02,  9.3327e-04,  2.6196e-02, -2.5319e-02],\n",
      "          [-2.1766e-02,  1.3584e-02,  3.9344e-02, -6.8363e-02, -2.5131e-02]],\n",
      "\n",
      "         [[ 1.5613e-02, -3.3368e-02,  3.0923e-02,  2.6269e-02,  5.6894e-03],\n",
      "          [ 6.7097e-02,  8.3595e-02,  8.4585e-02,  5.2235e-02,  8.9738e-03],\n",
      "          [ 5.1056e-02,  3.6040e-02, -5.8222e-02,  2.1493e-02,  2.9130e-03],\n",
      "          [-6.3701e-02, -5.7063e-02, -4.4526e-02, -4.2057e-02, -1.3934e-02],\n",
      "          [ 1.0640e-02,  3.4856e-02,  5.5287e-03, -4.3096e-02,  4.5817e-02]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 4.2895e-02,  2.8940e-02,  2.3935e-03, -2.0773e-03,  3.4044e-02],\n",
      "          [-5.5163e-02,  2.6301e-02,  2.8581e-02,  2.1541e-02,  6.3038e-02],\n",
      "          [ 1.8737e-03, -2.9279e-02,  1.7323e-02, -1.5889e-02,  1.6505e-02],\n",
      "          [ 5.7673e-02, -2.5437e-02,  9.0059e-03,  5.6819e-02, -5.4477e-02],\n",
      "          [-5.3645e-03,  6.9887e-02,  5.3072e-02,  1.9318e-02, -2.7225e-02]],\n",
      "\n",
      "         [[ 1.4167e-02, -3.0600e-02, -2.1561e-02, -2.8249e-02,  6.1998e-02],\n",
      "          [ 8.2970e-02,  6.4997e-02,  3.8081e-02,  9.8764e-02, -8.2151e-03],\n",
      "          [ 2.7356e-02, -2.0417e-02, -2.5145e-02, -7.8068e-04, -8.8840e-03],\n",
      "          [-9.8527e-02, -2.4700e-02, -3.3825e-02,  2.8046e-02,  3.7247e-02],\n",
      "          [-2.8050e-02,  1.3485e-02,  1.3458e-02,  3.9041e-02, -4.6902e-02]],\n",
      "\n",
      "         [[-1.6335e-02, -3.7480e-02,  4.0901e-02, -5.5388e-02, -4.3102e-02],\n",
      "          [ 4.4437e-02,  9.6758e-02,  8.6377e-02, -2.0423e-02, -3.9329e-02],\n",
      "          [ 9.6477e-02,  4.1098e-02, -2.4671e-02,  1.1741e-02,  4.1939e-02],\n",
      "          [-2.9733e-02, -7.3394e-02, -5.0284e-02, -1.8883e-02, -1.7289e-03],\n",
      "          [ 3.1744e-03,  4.6827e-03, -4.1093e-02, -7.0043e-03, -5.3609e-02]]],\n",
      "\n",
      "\n",
      "        ...,\n",
      "\n",
      "\n",
      "        [[[ 8.3277e-02,  3.6890e-02,  1.8119e-02, -3.0209e-02, -4.9738e-02],\n",
      "          [-4.3396e-02,  3.3674e-03, -5.2423e-02, -6.7803e-02,  2.7803e-02],\n",
      "          [-4.0240e-02, -6.8270e-02, -5.4117e-02,  4.0483e-02, -3.1872e-02],\n",
      "          [-4.8515e-02, -6.2948e-02, -3.4390e-02,  5.5129e-02,  7.9916e-02],\n",
      "          [-1.4520e-03, -2.0981e-02, -9.8406e-03, -4.7773e-02, -2.0590e-02]],\n",
      "\n",
      "         [[ 4.2671e-02, -3.0699e-02,  6.9804e-02, -1.0883e-02, -4.7450e-02],\n",
      "          [-1.3815e-02,  4.8303e-02,  1.2898e-01,  4.0659e-02, -2.4439e-02],\n",
      "          [-2.3547e-02, -7.7007e-02,  1.1054e-01,  1.4921e-01,  2.7429e-02],\n",
      "          [-1.6096e-02, -7.5879e-02,  8.2524e-02,  7.4598e-02,  6.4061e-02],\n",
      "          [-1.6626e-02,  2.1861e-02, -2.8795e-02, -1.6534e-02,  1.1863e-02]],\n",
      "\n",
      "         [[ 7.9089e-03,  2.0441e-02,  3.4478e-02,  8.0293e-02, -3.6534e-02],\n",
      "          [ 6.4225e-03, -4.7762e-02,  3.2168e-02,  4.6198e-02, -7.8083e-02],\n",
      "          [-8.0853e-02, -4.7434e-02,  6.5746e-02, -3.3948e-02, -6.7171e-02],\n",
      "          [-4.3018e-02,  2.0986e-03, -3.4692e-02, -4.2691e-02, -4.2879e-02],\n",
      "          [ 2.8360e-02,  3.7412e-02,  1.0797e-02, -3.1601e-02, -3.6011e-02]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-7.4323e-02,  2.3514e-02,  2.8120e-02, -3.4444e-02,  2.0240e-02],\n",
      "          [-1.6704e-02, -4.8874e-02, -2.3001e-02, -2.3052e-02, -2.4977e-03],\n",
      "          [ 1.7580e-02, -2.8186e-02, -9.6625e-03,  3.8570e-02,  6.0247e-02],\n",
      "          [-2.5835e-02, -3.2005e-02,  6.6707e-03, -3.3212e-02, -5.4322e-03],\n",
      "          [ 1.0559e-02, -1.2809e-02, -1.7153e-02,  3.3424e-02,  1.7789e-02]],\n",
      "\n",
      "         [[ 1.4336e-03,  2.3133e-02, -2.7413e-02,  3.3489e-02,  2.3936e-03],\n",
      "          [ 4.0719e-02,  8.0677e-02,  3.7392e-03,  1.9926e-02, -5.3278e-02],\n",
      "          [ 3.4773e-02,  7.3169e-02,  8.0197e-02, -1.4426e-02, -1.8952e-02],\n",
      "          [-8.3511e-02,  5.8287e-02,  1.6465e-02,  3.1923e-02, -6.5599e-02],\n",
      "          [-4.2686e-02,  2.0655e-02, -5.0714e-02,  2.0228e-02,  1.4534e-02]],\n",
      "\n",
      "         [[ 2.7775e-02,  4.3326e-02, -5.6377e-02, -4.8590e-02,  2.9876e-02],\n",
      "          [ 1.6133e-02, -2.3314e-02, -5.1664e-02,  4.8778e-02, -5.7037e-02],\n",
      "          [ 5.2648e-02, -2.3763e-02,  7.0552e-02, -2.6327e-02,  4.2132e-03],\n",
      "          [ 3.4014e-02,  7.9176e-02,  1.0076e-02,  4.6820e-03, -4.3330e-02],\n",
      "          [-4.9744e-02,  1.6529e-02,  4.9492e-02, -5.8161e-03,  5.2731e-02]]],\n",
      "\n",
      "\n",
      "        [[[-1.9338e-02, -9.6606e-04, -4.2074e-02,  1.0414e-02,  5.1378e-03],\n",
      "          [ 3.2916e-02,  3.7042e-02,  4.2324e-02,  3.2243e-02, -2.6132e-03],\n",
      "          [ 4.1480e-02,  4.3427e-02,  3.7974e-02,  4.4343e-02, -1.6073e-02],\n",
      "          [ 1.4071e-02, -2.6600e-02,  2.8199e-02, -9.7928e-03, -6.0696e-02],\n",
      "          [-1.6549e-02, -4.1310e-02,  2.0404e-02,  6.1281e-02,  6.4857e-03]],\n",
      "\n",
      "         [[-3.3950e-02, -1.5466e-02, -5.5534e-02,  5.3051e-02,  6.3665e-02],\n",
      "          [-4.4447e-02,  3.8480e-02,  1.0806e-02, -1.8571e-02,  7.0318e-02],\n",
      "          [ 1.4206e-02, -3.3643e-02,  3.8725e-02,  1.4963e-02,  3.6013e-02],\n",
      "          [ 7.7154e-02,  5.0835e-02,  7.5755e-02,  5.5606e-03, -3.7373e-02],\n",
      "          [ 1.5683e-02,  2.1561e-02,  8.6117e-02,  6.1160e-02,  4.1893e-02]],\n",
      "\n",
      "         [[ 3.1305e-02, -5.1487e-02, -5.0632e-03,  9.5190e-03, -5.4894e-02],\n",
      "          [-1.7844e-02, -2.1254e-02, -4.5451e-02,  1.5600e-02, -8.2639e-02],\n",
      "          [ 2.2997e-02, -3.2974e-02,  5.2778e-02, -6.9690e-04, -6.9729e-02],\n",
      "          [ 1.8696e-02, -2.3886e-02,  7.0037e-02, -6.1049e-03, -1.4696e-03],\n",
      "          [ 4.0309e-02,  6.4744e-02,  5.6297e-02,  3.9013e-02, -4.1965e-03]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 1.6885e-02, -6.1368e-02, -5.6470e-02, -2.6741e-02, -6.8191e-02],\n",
      "          [ 4.4768e-03,  4.9708e-02,  6.0977e-02, -1.6171e-02, -4.4969e-02],\n",
      "          [ 4.6102e-02, -3.6203e-02,  2.7477e-02, -2.8119e-02, -1.2248e-02],\n",
      "          [ 4.3254e-02,  1.7085e-02, -2.3136e-02,  6.1859e-02, -3.2702e-02],\n",
      "          [-2.2756e-02, -4.4126e-02, -4.9386e-02, -4.3294e-02, -3.7409e-02]],\n",
      "\n",
      "         [[ 1.7293e-02, -2.3817e-02,  6.2928e-02, -2.3484e-02, -8.3615e-04],\n",
      "          [ 2.4658e-02,  1.5330e-02, -4.1019e-02, -3.0920e-02, -3.7113e-02],\n",
      "          [-2.9193e-02,  2.0407e-02, -4.4766e-02, -8.2219e-03, -7.3293e-02],\n",
      "          [ 6.6241e-02,  4.7269e-03, -5.9276e-02, -6.6090e-02, -7.7270e-02],\n",
      "          [ 2.5021e-02,  1.2794e-02, -3.1807e-02, -2.7548e-02, -4.7251e-02]],\n",
      "\n",
      "         [[-1.9367e-02, -1.6620e-02, -3.0591e-02,  1.8364e-02,  1.6226e-02],\n",
      "          [-3.7912e-02, -1.4789e-02, -1.6854e-02, -1.6120e-02, -2.1135e-02],\n",
      "          [-3.9185e-02,  3.3734e-02,  1.6282e-02,  3.1442e-02,  2.2967e-02],\n",
      "          [-9.2874e-03,  9.2374e-03,  3.3128e-02,  4.8171e-02, -7.2117e-02],\n",
      "          [ 1.1281e-02,  2.0008e-02,  2.2592e-02,  3.2063e-03,  5.1954e-02]]],\n",
      "\n",
      "\n",
      "        [[[ 5.9991e-02, -5.4231e-02, -4.5536e-02,  2.3540e-02,  1.1555e-02],\n",
      "          [ 3.4607e-02,  4.5185e-02,  5.8081e-02,  2.2030e-02, -1.2349e-03],\n",
      "          [ 5.1897e-02, -1.6887e-03, -8.1748e-03, -4.5904e-02,  5.3274e-02],\n",
      "          [-5.4784e-02, -1.5433e-02,  6.3656e-02, -5.9552e-02,  2.3722e-02],\n",
      "          [-1.5593e-02,  6.5349e-03,  3.1639e-02,  3.6559e-02,  2.8092e-02]],\n",
      "\n",
      "         [[ 1.2763e-03, -1.2872e-02,  5.2114e-02,  1.5032e-02, -1.7647e-02],\n",
      "          [-2.6048e-03,  3.8619e-02, -2.6600e-02, -5.9680e-02, -7.1203e-02],\n",
      "          [-1.0221e-02, -1.2485e-02,  2.8465e-02, -5.8682e-02,  3.3715e-02],\n",
      "          [ 2.7378e-02,  2.2546e-03,  2.3179e-02, -1.7939e-02, -1.9545e-02],\n",
      "          [ 6.6928e-02,  7.6206e-02,  4.1887e-02, -6.4758e-02,  5.4992e-02]],\n",
      "\n",
      "         [[ 3.4233e-02, -1.4922e-02,  2.2285e-02,  1.7263e-03, -3.8386e-02],\n",
      "          [-4.6064e-02, -1.8892e-02,  1.8039e-02, -2.3864e-02, -2.3662e-02],\n",
      "          [ 5.8448e-02,  4.7967e-02,  3.9555e-03, -7.2130e-02,  4.0005e-02],\n",
      "          [ 8.2568e-02, -1.1495e-02, -5.4155e-02, -1.9160e-02,  2.3540e-03],\n",
      "          [ 6.8304e-02,  4.7106e-02,  2.2225e-02, -3.4687e-02, -1.4680e-02]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 9.5131e-03, -4.8124e-02, -3.3598e-02,  3.9273e-02,  3.1625e-02],\n",
      "          [ 3.8291e-02, -1.6454e-02,  5.3250e-02,  4.1788e-02,  3.0442e-02],\n",
      "          [ 2.4323e-02, -7.1931e-02, -4.1177e-02, -1.0253e-02,  1.2542e-04],\n",
      "          [ 5.2099e-02,  3.6753e-02, -3.7681e-02,  3.4476e-02,  2.0076e-02],\n",
      "          [-2.3042e-02, -4.8089e-02,  3.8296e-02,  1.9068e-02,  1.7965e-02]],\n",
      "\n",
      "         [[ 5.6605e-02, -2.5973e-02, -3.5838e-02,  9.3149e-03, -1.1911e-02],\n",
      "          [-3.1516e-02, -7.7478e-02, -7.2859e-02, -9.4961e-03,  6.6282e-02],\n",
      "          [-3.2830e-02, -8.7030e-04,  1.6331e-02,  1.5938e-02,  2.5990e-02],\n",
      "          [ 3.9815e-02, -7.9145e-02,  3.0474e-02,  5.3497e-02,  7.8176e-02],\n",
      "          [ 2.7994e-02, -3.5282e-02, -7.4082e-02, -3.9905e-02, -3.7181e-02]],\n",
      "\n",
      "         [[-2.3574e-02, -5.4620e-02,  4.1302e-02,  3.3771e-02, -1.3920e-02],\n",
      "          [ 7.3658e-03,  5.3518e-03, -1.2001e-02,  3.1115e-02,  3.1655e-02],\n",
      "          [-4.3660e-02,  5.9753e-02, -4.1402e-02, -3.7035e-02,  8.0813e-02],\n",
      "          [-2.6652e-02, -8.3527e-03, -6.6319e-03, -6.4132e-02,  6.8581e-02],\n",
      "          [ 5.2492e-02, -6.7979e-02, -2.7970e-02, -8.0654e-04,  5.3618e-02]]]])), ('conv2.bias', tensor([ 0.0248, -0.0148,  0.0498, -0.0081, -0.0075, -0.0324, -0.0339, -0.0140,\n",
      "        -0.0517, -0.0182,  0.0125, -0.0430, -0.0512,  0.0703, -0.0312, -0.0518,\n",
      "        -0.0098, -0.0439, -0.0555,  0.0041])), ('fc1.weight', tensor([[-5.0389e-02, -2.2066e-02,  1.7721e-02,  ..., -2.3435e-02,\n",
      "          5.4974e-02,  1.5509e-02],\n",
      "        [-3.8942e-02, -8.8542e-02, -7.0218e-03,  ...,  3.9788e-02,\n",
      "         -3.3045e-02, -8.6664e-03],\n",
      "        [-4.0431e-02, -3.4863e-05, -2.6978e-02,  ...,  4.4620e-02,\n",
      "         -5.6651e-02,  5.4016e-02],\n",
      "        ...,\n",
      "        [ 8.1766e-03, -4.1001e-05,  2.9171e-02,  ..., -3.9311e-02,\n",
      "         -2.6783e-03, -3.5160e-02],\n",
      "        [ 3.1894e-02,  3.9430e-02,  2.9696e-02,  ..., -3.7023e-03,\n",
      "         -3.1485e-03,  6.3415e-03],\n",
      "        [ 6.1915e-02,  1.7205e-02, -1.1150e-02,  ...,  3.7018e-02,\n",
      "         -1.9638e-04, -1.8389e-03]])), ('fc1.bias', tensor([ 0.0460,  0.0312, -0.0076, -0.0048,  0.0141, -0.0137,  0.0018, -0.0409,\n",
      "         0.0100,  0.0080,  0.0261,  0.0465, -0.0189, -0.0034, -0.0120, -0.0431,\n",
      "         0.0499,  0.0147,  0.0573, -0.0467,  0.0091,  0.0176, -0.0292,  0.0365,\n",
      "         0.0319,  0.0011,  0.0615, -0.0251, -0.0436, -0.0344, -0.0366,  0.0440,\n",
      "         0.0050, -0.0457, -0.0466,  0.0661,  0.0026, -0.0557,  0.0308, -0.0318,\n",
      "         0.0095,  0.0652,  0.0320, -0.0397, -0.0244,  0.0130, -0.0475, -0.0529,\n",
      "         0.0506,  0.0300])), ('fc2.weight', tensor([[ 1.1329e-01, -4.9277e-02,  1.0808e-01,  3.2998e-02, -5.1854e-02,\n",
      "         -5.2151e-02, -1.5911e-01, -1.2682e-01, -6.0331e-02, -9.5980e-02,\n",
      "         -1.8736e-01, -1.0472e-01, -4.5661e-02,  1.0853e-01,  1.4338e-01,\n",
      "          1.0916e-01,  2.0733e-02,  1.5855e-01,  8.9660e-02, -1.3606e-02,\n",
      "         -3.9632e-03, -1.2448e-01, -4.7867e-02, -1.6723e-01,  4.6486e-03,\n",
      "         -1.8062e-01, -1.4716e-01, -2.1614e-01,  1.0487e-01,  2.1111e-01,\n",
      "         -1.6761e-01,  1.9804e-02,  2.0156e-01,  9.7869e-02, -1.0254e-01,\n",
      "          1.6511e-01,  1.4513e-01,  4.8654e-02, -1.0066e-01, -1.2394e-01,\n",
      "         -1.2176e-01, -2.1524e-01,  6.8837e-02, -6.5217e-02, -6.2767e-03,\n",
      "          1.5323e-01, -7.3099e-02, -3.9144e-02,  1.0364e-01,  2.8738e-02],\n",
      "        [ 2.0434e-02,  1.2708e-01, -1.4209e-01,  1.0773e-01,  9.3752e-02,\n",
      "          1.0387e-01, -1.7779e-01,  5.1364e-02,  7.5435e-03, -1.1745e-01,\n",
      "          1.3459e-01,  1.7600e-01, -8.6039e-02, -1.2488e-01,  2.2548e-02,\n",
      "         -2.1913e-01,  1.5436e-01, -1.3927e-02, -1.4040e-01,  2.9331e-02,\n",
      "         -9.7013e-02,  5.2065e-02,  9.6782e-02,  1.5242e-01,  1.4130e-01,\n",
      "         -2.4710e-01, -2.1954e-01,  1.3901e-01,  8.4520e-02,  7.5603e-02,\n",
      "          1.7768e-01, -3.7554e-02, -1.7958e-01, -1.1477e-01,  1.6770e-01,\n",
      "          2.8387e-02, -1.1817e-01, -4.1141e-02,  1.2721e-01,  1.0890e-01,\n",
      "         -2.0214e-02,  1.2324e-01, -4.5969e-02, -1.8321e-01,  1.3157e-01,\n",
      "         -1.2097e-01, -1.4182e-01,  1.0118e-01, -1.7533e-01,  7.8801e-02],\n",
      "        [-4.3322e-02, -5.2308e-02, -8.1031e-02, -6.5818e-02, -8.2730e-02,\n",
      "          1.4568e-01,  9.3820e-02, -2.6014e-02,  1.8428e-01,  2.4338e-02,\n",
      "          5.3856e-02,  1.8062e-01,  5.4525e-02,  1.5821e-01,  1.4423e-01,\n",
      "          9.6118e-02, -6.0475e-02,  1.9119e-02, -1.1301e-01, -1.1059e-01,\n",
      "         -5.7030e-02,  3.2274e-02, -1.2624e-01, -1.2234e-01, -5.8389e-02,\n",
      "         -8.8818e-02, -1.4993e-01,  5.6047e-02, -1.7134e-01, -2.3792e-02,\n",
      "         -1.0263e-01, -5.9447e-02, -2.5908e-02, -1.3088e-01,  7.1328e-02,\n",
      "          1.6464e-01, -9.1488e-02, -1.4225e-01,  1.5668e-01,  1.8799e-02,\n",
      "          1.1613e-01,  1.2893e-01, -5.9570e-02, -3.8996e-02,  1.1327e-02,\n",
      "          1.6906e-01,  1.0385e-01,  4.6251e-02, -1.0178e-01,  1.3085e-01],\n",
      "        [ 9.5788e-03,  1.6852e-01,  1.1110e-01, -8.5674e-02, -5.3882e-03,\n",
      "         -1.8486e-01,  6.1248e-02,  5.2668e-02,  4.7841e-02, -3.6299e-02,\n",
      "         -3.0851e-02,  2.1104e-01, -8.3559e-02, -3.0019e-02,  6.2176e-02,\n",
      "         -1.1657e-02, -1.1138e-01, -1.4426e-03, -1.2079e-01,  1.3146e-01,\n",
      "          1.3465e-01,  1.4191e-01,  7.5799e-02, -4.3849e-02, -1.0114e-01,\n",
      "         -1.5214e-01,  8.9260e-02, -9.6968e-02,  7.9148e-02, -1.1888e-01,\n",
      "         -1.9057e-01,  1.2911e-01, -1.0427e-01,  8.8596e-02,  7.2434e-02,\n",
      "          8.2009e-02,  9.9676e-02,  6.9669e-02, -2.0140e-01,  2.3941e-02,\n",
      "          9.6935e-02,  1.4097e-01, -1.3517e-01,  5.2466e-02,  8.4699e-02,\n",
      "          3.8861e-02, -9.2184e-02, -3.1131e-02, -6.7337e-02,  1.3185e-01],\n",
      "        [ 1.2385e-02, -2.6049e-02, -2.2740e-02,  5.9395e-02,  1.5643e-02,\n",
      "          1.2214e-01,  1.2966e-01, -9.4065e-02, -4.1866e-02, -1.2759e-01,\n",
      "          1.7667e-01,  1.5954e-02,  1.7815e-01, -1.8080e-01, -2.8281e-02,\n",
      "          8.1924e-02, -9.1864e-02, -9.3606e-02,  5.8841e-02, -8.0877e-02,\n",
      "         -1.1424e-01, -1.2965e-01,  1.7479e-01,  1.0463e-01,  3.8096e-02,\n",
      "          1.5461e-01,  4.4775e-02, -2.4116e-02,  7.8765e-02, -1.6952e-02,\n",
      "          3.8997e-02,  9.0082e-02, -1.5115e-01, -1.5956e-01, -7.5876e-02,\n",
      "         -8.1925e-02, -5.2436e-03,  1.9835e-01,  8.7058e-02, -2.0398e-02,\n",
      "         -1.5305e-01, -5.5359e-02,  2.1174e-02,  7.9143e-02, -1.2925e-01,\n",
      "          1.8099e-02, -9.1407e-02, -1.2877e-01,  1.6970e-01, -1.0876e-01],\n",
      "        [ 6.3108e-02, -6.6444e-02, -4.2356e-02, -1.2443e-01,  2.6209e-02,\n",
      "         -3.9322e-02, -1.4557e-01, -7.4897e-02,  8.2360e-02, -9.9916e-02,\n",
      "          1.3040e-01,  2.0497e-02, -1.4823e-01, -1.7967e-01, -1.4129e-01,\n",
      "          9.9937e-03, -7.4570e-02,  1.6031e-01,  1.7097e-01,  9.3536e-02,\n",
      "          2.0270e-01, -1.1737e-01, -8.0334e-02,  3.5116e-02, -2.1944e-02,\n",
      "         -1.1571e-01,  7.3290e-02, -1.5101e-01,  1.7630e-01,  1.0601e-01,\n",
      "         -4.2692e-02,  1.0160e-01, -8.9883e-02,  9.6224e-02, -4.5115e-02,\n",
      "          1.0808e-01,  1.6810e-01,  7.2629e-02, -1.2076e-01, -8.7162e-02,\n",
      "         -9.5677e-02,  1.1006e-02,  1.1293e-01, -8.2854e-02, -3.0842e-02,\n",
      "         -1.3778e-02,  1.4202e-01, -1.3063e-02,  1.2681e-01,  2.5435e-02],\n",
      "        [-2.1366e-01, -1.4309e-01,  2.8044e-02, -4.8394e-02, -5.1112e-03,\n",
      "          8.0516e-02, -1.8648e-01,  6.8483e-02,  1.4983e-01, -1.1179e-01,\n",
      "          1.7012e-01, -4.5306e-02, -1.6713e-02, -1.6615e-01,  1.0774e-01,\n",
      "          1.0649e-01, -1.9343e-01,  1.6015e-01,  1.0752e-01, -3.6167e-02,\n",
      "         -1.9240e-02, -9.3073e-02, -3.4891e-02,  1.1017e-04, -1.2943e-01,\n",
      "         -3.5737e-02, -1.4748e-01, -1.1261e-01,  9.1107e-02, -1.2189e-01,\n",
      "          1.5608e-01,  5.0552e-02, -7.1142e-02, -2.8527e-02, -7.3392e-02,\n",
      "          1.3127e-01, -1.5734e-01,  1.7765e-01,  1.5573e-01,  1.5243e-01,\n",
      "         -2.1977e-01, -2.0289e-01,  1.5289e-01,  2.3262e-02, -3.5810e-02,\n",
      "          1.2944e-01,  2.0181e-01,  5.3761e-02,  8.9058e-02, -7.7019e-03],\n",
      "        [ 1.6765e-01,  1.1528e-01,  8.0093e-02, -2.5310e-02,  1.2207e-01,\n",
      "         -1.2488e-01,  1.2480e-01, -2.8346e-02, -2.3160e-02, -4.0579e-02,\n",
      "         -2.2885e-02,  3.3628e-02,  1.1887e-01,  1.1587e-01,  2.9698e-02,\n",
      "         -1.5940e-01,  3.1749e-02, -1.7011e-01,  4.2473e-02, -1.8962e-01,\n",
      "         -8.3081e-02, -8.5114e-02, -1.4596e-01,  6.2214e-02, -9.5328e-02,\n",
      "          1.3708e-01, -1.3487e-01,  1.7868e-01,  7.2906e-02,  8.1681e-02,\n",
      "         -7.2876e-02, -5.4429e-02,  1.3629e-01,  2.8440e-02,  8.7442e-02,\n",
      "         -1.9752e-01, -5.1323e-02,  9.2613e-03, -1.6843e-01, -3.2622e-02,\n",
      "          9.7304e-02,  1.2135e-01, -1.1120e-01,  2.0989e-01,  8.3647e-02,\n",
      "         -1.6593e-02, -3.2424e-02,  6.4727e-02,  9.3573e-02, -1.3790e-01],\n",
      "        [ 2.4006e-02, -1.0594e-01, -1.3918e-01, -9.9842e-02, -2.7026e-02,\n",
      "          3.6958e-02,  8.9777e-03,  3.9072e-02,  8.2450e-02, -1.3243e-01,\n",
      "         -4.5892e-02, -8.7366e-02, -3.8825e-02, -1.1203e-01,  1.2417e-01,\n",
      "         -2.1194e-02,  6.7045e-02,  1.8617e-02,  1.4258e-01,  1.3730e-01,\n",
      "          2.0116e-01, -1.0053e-01, -1.1520e-01,  1.4579e-01,  1.1032e-01,\n",
      "         -3.2347e-03,  9.8058e-02, -3.0530e-02, -1.6233e-01, -2.8639e-02,\n",
      "          1.5683e-01, -6.6692e-02, -1.0367e-01, -4.7446e-02, -3.0383e-02,\n",
      "          1.7264e-01, -1.1332e-01, -1.4409e-01, -4.5395e-02,  8.1815e-02,\n",
      "          1.1130e-01,  1.1704e-01, -4.1612e-02, -7.4955e-02, -1.5046e-01,\n",
      "          1.5265e-01, -1.1518e-01, -1.0108e-01,  1.4899e-01,  4.8859e-03],\n",
      "        [ 1.2572e-01, -1.0222e-01, -4.4766e-02,  2.2561e-02,  4.9607e-02,\n",
      "         -9.5624e-02,  1.3857e-01,  1.7242e-02, -1.3880e-01, -1.6652e-02,\n",
      "          1.0146e-01, -2.4892e-02,  6.3580e-02,  1.6538e-02, -7.9181e-03,\n",
      "         -1.1572e-02,  3.2834e-02, -8.4497e-02,  1.0907e-01, -7.4708e-02,\n",
      "         -1.6268e-02, -5.3936e-02,  2.2602e-02,  7.8637e-02, -2.6085e-02,\n",
      "          1.4748e-01,  8.7984e-02,  6.0788e-02,  8.1826e-02,  5.3797e-02,\n",
      "         -6.6489e-03,  1.5258e-01, -5.6476e-02,  3.1634e-02, -1.1020e-01,\n",
      "         -1.2116e-01, -6.3880e-02,  4.4707e-02,  1.6100e-02,  2.8136e-02,\n",
      "          4.8066e-02,  8.7091e-02, -1.1421e-01,  2.1019e-01, -1.1483e-01,\n",
      "         -1.2560e-01, -4.2111e-02,  1.0333e-01,  1.5443e-01, -1.5270e-01]])), ('fc2.bias', tensor([-0.0910,  0.0674, -0.0465, -0.1390,  0.0332,  0.0468, -0.0558, -0.0567,\n",
      "        -0.0181, -0.0386]))])\n",
      "{'state': {0: {'momentum_buffer': tensor([[[[ 4.4572e-03,  5.9092e-03,  1.3747e-02,  1.5993e-02,  1.3422e-02],\n",
      "          [ 8.7904e-04,  2.7949e-03,  1.6703e-02,  1.9541e-02,  2.0271e-02],\n",
      "          [-5.4636e-03, -9.9470e-04,  7.3423e-03,  7.8441e-03, -6.3248e-05],\n",
      "          [-4.9739e-03, -1.0705e-02, -4.3088e-03,  7.7827e-04, -2.9581e-03],\n",
      "          [-1.4367e-04,  5.2202e-04,  7.3892e-03,  3.1877e-03, -1.3338e-03]]],\n",
      "\n",
      "\n",
      "        [[[ 9.4095e-03, -1.3845e-02, -1.3278e-02,  8.3173e-03,  1.3730e-02],\n",
      "          [ 6.5575e-03, -2.3069e-02, -9.0404e-03,  9.2064e-03,  6.1318e-03],\n",
      "          [-1.4798e-02, -3.1486e-02, -1.2201e-02,  2.7547e-04,  4.5446e-03],\n",
      "          [-3.9191e-02, -3.8294e-02, -2.4901e-02, -5.1683e-03, -1.2332e-03],\n",
      "          [-4.9811e-02, -3.8884e-02, -2.4957e-02, -2.0174e-02, -9.7825e-03]]],\n",
      "\n",
      "\n",
      "        [[[ 2.7230e-02,  3.1071e-02,  3.4787e-02,  1.2243e-02,  7.8198e-03],\n",
      "          [ 3.2617e-02,  3.1770e-02,  2.6065e-02,  1.8859e-02,  1.0730e-02],\n",
      "          [ 1.8028e-02,  1.6714e-02,  1.4264e-02,  1.2983e-02,  9.7653e-03],\n",
      "          [-1.1584e-02, -1.2764e-02, -8.1976e-03,  8.8387e-03,  7.6350e-03],\n",
      "          [-1.8178e-02, -2.2261e-02, -9.0089e-03,  5.3154e-03,  7.8199e-03]]],\n",
      "\n",
      "\n",
      "        [[[ 1.3690e-03, -2.5408e-03, -8.5600e-03, -1.9876e-02, -1.6688e-02],\n",
      "          [-9.0841e-05, -4.5122e-03, -1.1600e-02, -1.3883e-02, -8.5139e-03],\n",
      "          [-4.8617e-04, -3.2638e-03, -5.1984e-03, -6.4652e-03, -3.7061e-03],\n",
      "          [-3.9965e-03, -5.8915e-03, -8.5517e-03, -1.2037e-02, -6.1129e-03],\n",
      "          [-6.1352e-03, -4.9812e-03, -5.3016e-03, -4.6472e-03, -3.5000e-03]]],\n",
      "\n",
      "\n",
      "        [[[ 1.2981e-02,  1.6814e-02,  1.5802e-02,  4.0404e-03, -3.0231e-05],\n",
      "          [-6.9166e-03, -1.8213e-02, -2.5391e-02, -4.5621e-03, -3.8202e-03],\n",
      "          [-1.9070e-02, -4.9241e-02, -3.8623e-02, -1.7657e-02,  1.6853e-03],\n",
      "          [-3.4692e-02, -4.5210e-02, -4.7534e-02, -3.4870e-02, -9.4240e-03],\n",
      "          [-3.5341e-02, -4.6627e-02, -5.0359e-02, -5.0017e-02, -2.5811e-02]]],\n",
      "\n",
      "\n",
      "        [[[ 2.2655e-02,  3.2155e-02,  3.6699e-02,  1.4077e-02,  1.0197e-02],\n",
      "          [ 2.5577e-02,  2.8854e-02,  2.4062e-02,  4.5747e-03,  4.9820e-03],\n",
      "          [ 9.7163e-03,  6.7705e-03,  9.2528e-03,  1.6565e-03,  9.7750e-03],\n",
      "          [ 4.1616e-04, -2.3298e-03,  1.9330e-03,  3.6471e-03,  6.1266e-03],\n",
      "          [-9.5380e-03, -1.2040e-04,  8.1960e-03,  3.4192e-03,  6.8847e-04]]],\n",
      "\n",
      "\n",
      "        [[[-8.8641e-03,  8.7777e-04,  4.0274e-03, -2.1549e-05,  1.4768e-03],\n",
      "          [-2.0023e-03,  6.5997e-03,  4.6323e-03, -1.5142e-03,  3.6519e-03],\n",
      "          [ 3.7340e-03,  3.7466e-03, -8.0678e-04, -1.0663e-02,  1.0974e-03],\n",
      "          [ 7.0950e-03,  2.9331e-03, -1.2110e-02, -2.2270e-02, -7.5629e-03],\n",
      "          [ 1.2801e-02,  5.4109e-03, -1.6563e-02, -2.9883e-02, -1.0359e-02]]],\n",
      "\n",
      "\n",
      "        [[[ 2.1083e-02,  2.4958e-02,  2.4553e-02,  2.8947e-02,  1.8870e-02],\n",
      "          [ 2.4931e-02,  2.3368e-02,  1.9944e-02,  2.8772e-02,  1.8473e-02],\n",
      "          [ 1.6184e-02,  1.5782e-02,  1.8994e-02,  2.1696e-02,  1.4745e-02],\n",
      "          [ 5.5030e-03,  5.3810e-03,  1.4586e-02,  1.6864e-02,  7.0376e-03],\n",
      "          [ 4.4955e-03,  8.7601e-03,  1.9770e-02,  2.3473e-02,  1.0086e-02]]],\n",
      "\n",
      "\n",
      "        [[[-1.1138e-02, -6.9780e-03,  8.7066e-03,  5.1353e-03, -4.6561e-03],\n",
      "          [-7.6100e-03, -1.7051e-03,  7.3865e-03,  9.2877e-04, -7.4033e-03],\n",
      "          [-1.5890e-02, -9.4590e-03, -1.1541e-02, -1.7293e-02, -2.0344e-02],\n",
      "          [-4.5519e-03, -1.3021e-03, -5.2445e-03, -1.1035e-02, -1.7547e-02],\n",
      "          [ 7.0183e-03,  7.3809e-03,  9.0176e-04,  4.1334e-03,  2.8008e-03]]],\n",
      "\n",
      "\n",
      "        [[[ 3.4368e-03,  5.5280e-03, -2.3703e-03, -1.9837e-02, -7.9476e-03],\n",
      "          [ 1.8506e-02,  1.5220e-02, -8.7057e-04, -1.1067e-02, -4.8154e-03],\n",
      "          [ 1.6053e-02, -6.6788e-04, -1.5707e-02, -1.8350e-02, -8.4765e-03],\n",
      "          [ 2.1689e-04, -1.3886e-02, -2.4221e-02, -2.2598e-02, -8.6157e-03],\n",
      "          [-3.9921e-04, -6.8745e-03, -9.2428e-03, -2.4704e-03,  7.7036e-03]]]])}, 1: {'momentum_buffer': tensor([ 9.8395e-03, -1.6430e-02,  6.8424e-03,  7.6912e-03, -2.0002e-02,\n",
      "         7.9094e-03, -8.5494e-03,  5.3187e-03,  6.6364e-05, -1.1770e-03])}, 2: {'momentum_buffer': tensor([[[[-2.9543e-03, -1.0493e-03, -1.5350e-03,  2.8225e-03,  4.0028e-03],\n",
      "          [-1.1874e-04,  2.7500e-03,  5.6288e-03,  7.0372e-03,  4.1703e-03],\n",
      "          [-5.3400e-03, -4.6387e-03, -5.5499e-03, -2.7509e-03, -1.1757e-03],\n",
      "          [-2.4300e-03, -1.6694e-03, -3.7970e-03, -4.2937e-03,  2.2613e-03],\n",
      "          [ 3.1349e-03,  3.8933e-03, -4.4875e-04, -2.1036e-05,  3.6175e-03]],\n",
      "\n",
      "         [[-8.5810e-03, -9.9940e-03, -3.1869e-03,  4.8675e-03,  9.1348e-03],\n",
      "          [-7.7294e-03,  2.6154e-03, -1.1913e-03, -3.3089e-03,  9.3674e-03],\n",
      "          [-5.6308e-03,  4.7934e-03, -8.5938e-03, -2.1209e-02,  7.8435e-03],\n",
      "          [ 4.5675e-03,  5.1070e-03, -2.0146e-02, -2.0024e-02,  6.4533e-03],\n",
      "          [ 5.9529e-03, -4.4495e-03, -2.4997e-02, -9.5025e-03,  9.9415e-03]],\n",
      "\n",
      "         [[-1.4826e-02, -6.7586e-05,  1.5099e-02,  1.4773e-02,  1.5888e-02],\n",
      "          [ 1.5566e-03,  1.8709e-02,  3.9243e-05,  1.0023e-02, -1.0187e-03],\n",
      "          [ 8.6312e-03, -1.3652e-02, -1.9748e-02, -1.4957e-02, -9.1324e-03],\n",
      "          [-1.0826e-02, -3.2338e-02, -3.0104e-02, -6.1610e-03,  6.1442e-03],\n",
      "          [-9.4204e-03, -1.3605e-02, -2.5116e-03,  1.8886e-02,  1.6116e-02]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 7.4867e-03,  4.3029e-03, -1.4535e-04, -1.3080e-03, -5.1144e-04],\n",
      "          [-4.9981e-03, -1.1120e-02, -1.4159e-02,  1.1780e-03,  3.8791e-03],\n",
      "          [-7.3996e-03, -8.2635e-03,  5.7842e-03,  3.8538e-03,  8.0526e-03],\n",
      "          [-2.2076e-03,  6.8020e-03,  1.1111e-02,  5.5997e-03,  7.0945e-03],\n",
      "          [-3.5788e-03,  2.0680e-04, -7.3361e-03, -1.0623e-02, -6.5544e-03]],\n",
      "\n",
      "         [[-8.2364e-03,  2.8665e-04,  1.3491e-02,  1.1691e-02,  1.1179e-02],\n",
      "          [ 3.4822e-03,  2.8687e-03,  1.4471e-02,  1.1434e-02, -7.1223e-03],\n",
      "          [-6.2767e-03, -1.7216e-02, -3.9757e-03, -2.3237e-03, -1.6534e-02],\n",
      "          [-1.7545e-02, -2.1757e-02, -1.1145e-02,  2.4713e-03, -7.3564e-04],\n",
      "          [-1.2428e-02, -6.3669e-03,  6.8810e-03,  1.3998e-02,  1.7194e-02]],\n",
      "\n",
      "         [[-1.4268e-02, -2.8540e-03,  6.7405e-03,  8.7867e-03,  5.5827e-03],\n",
      "          [-3.2981e-03,  4.0122e-03,  1.2306e-02,  1.6286e-02,  6.3144e-03],\n",
      "          [ 1.0731e-02,  1.5023e-03,  1.3762e-03,  8.7295e-03,  2.1112e-03],\n",
      "          [-4.0661e-03, -1.6039e-02, -1.4218e-02,  2.1486e-03, -2.3446e-03],\n",
      "          [-1.2198e-02, -1.6296e-02, -9.0259e-03,  4.9573e-03,  4.8049e-03]]],\n",
      "\n",
      "\n",
      "        [[[-4.4320e-03, -8.1424e-03, -2.1175e-03,  2.6529e-03, -8.7750e-04],\n",
      "          [-5.8779e-03, -9.3869e-03, -1.0537e-02, -7.5603e-03, -6.1166e-03],\n",
      "          [-5.3252e-03, -9.1597e-03, -1.4691e-02, -1.3679e-02, -1.1530e-02],\n",
      "          [ 4.5617e-03,  2.0165e-03, -3.6872e-03, -1.7303e-03, -2.5276e-03],\n",
      "          [ 1.3967e-03,  1.5857e-03,  2.1689e-03,  5.0643e-03,  3.8570e-03]],\n",
      "\n",
      "         [[-1.0093e-02, -1.4270e-02, -6.2934e-03,  1.5267e-03, -1.7580e-02],\n",
      "          [-9.2225e-03, -1.0826e-02, -1.8908e-02, -2.4298e-02, -1.7511e-02],\n",
      "          [ 1.5317e-03, -8.6991e-03, -7.3830e-03, -1.9150e-02, -3.2397e-03],\n",
      "          [ 1.0167e-02, -7.1681e-03, -2.0723e-03,  5.5146e-03,  1.2123e-02],\n",
      "          [ 4.6512e-03, -6.7952e-03,  3.7987e-03,  7.8490e-03,  1.5885e-02]],\n",
      "\n",
      "         [[-1.2681e-02, -9.7649e-03, -1.0526e-02, -2.7341e-02, -1.8925e-02],\n",
      "          [-1.7140e-02, -1.7124e-02, -2.5025e-02, -2.6913e-02, -6.6658e-03],\n",
      "          [-1.8928e-02, -3.1306e-02, -3.6294e-02, -9.2332e-03,  1.1387e-03],\n",
      "          [-1.9212e-02, -1.5795e-02,  2.1386e-03,  1.1751e-02,  9.2806e-04],\n",
      "          [-5.9833e-03,  8.2703e-03,  1.0129e-02,  1.1688e-02,  1.8974e-02]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-3.5361e-03, -8.3985e-03, -2.1624e-02, -1.1236e-02, -6.4863e-03],\n",
      "          [-1.1209e-02, -1.7035e-02, -1.7911e-02, -7.0656e-03, -1.3908e-02],\n",
      "          [-3.5092e-03, -5.9895e-05,  6.4477e-03, -2.2275e-03, -7.0334e-03],\n",
      "          [ 1.0654e-03,  1.8095e-03, -7.1161e-03, -6.0073e-03,  1.5956e-03],\n",
      "          [-2.5003e-03, -7.3496e-03, -1.9497e-03,  3.4337e-03,  8.0236e-03]],\n",
      "\n",
      "         [[-1.4379e-02, -9.9900e-04,  6.3593e-05, -5.8533e-03, -6.1298e-03],\n",
      "          [-1.6543e-02, -1.8174e-02, -1.3909e-02, -6.3202e-03, -5.5065e-03],\n",
      "          [-1.7371e-02, -2.7128e-02, -2.0059e-02, -9.5839e-03, -6.5824e-03],\n",
      "          [-8.4461e-03, -4.7162e-03,  3.0905e-03,  8.6544e-03, -1.0149e-03],\n",
      "          [ 4.9775e-03,  1.0868e-02,  1.3154e-02,  1.5892e-02,  1.4147e-02]],\n",
      "\n",
      "         [[-3.9726e-03, -1.8554e-04,  6.0081e-03, -6.0746e-03, -4.5747e-03],\n",
      "          [-9.4225e-03, -8.3347e-03, -2.5077e-03, -7.9292e-03, -1.3782e-03],\n",
      "          [-1.2826e-02, -1.8189e-02, -2.5937e-02, -6.9214e-03, -5.0837e-03],\n",
      "          [-1.7775e-02, -1.9275e-02, -1.7408e-02, -2.5996e-03, -6.8724e-03],\n",
      "          [-6.9520e-03,  3.2258e-03,  6.2707e-03,  5.8583e-03,  1.5986e-03]]],\n",
      "\n",
      "\n",
      "        [[[ 2.3986e-03,  5.3166e-04, -2.3535e-03, -5.9320e-03, -4.7468e-03],\n",
      "          [ 2.8626e-03,  7.3206e-04,  1.0055e-03,  5.5552e-03,  7.8497e-03],\n",
      "          [-9.7313e-03, -9.1779e-03, -8.7340e-03, -1.0745e-02, -2.7915e-03],\n",
      "          [ 1.4774e-03, -2.9772e-03,  1.5859e-03,  4.2676e-03, -2.9692e-03],\n",
      "          [ 3.9132e-03, -3.9251e-03, -1.5994e-03,  1.0664e-03,  1.9761e-03]],\n",
      "\n",
      "         [[ 5.9191e-03,  7.5496e-03, -4.6738e-03,  1.4395e-02,  1.2533e-02],\n",
      "          [ 4.2171e-04, -9.7639e-03, -1.3569e-02,  7.8446e-03,  5.7001e-03],\n",
      "          [-1.5347e-03, -1.3903e-02, -1.2127e-02,  1.3625e-02, -3.7475e-03],\n",
      "          [ 1.1299e-02, -1.4049e-02,  7.4119e-03,  3.2127e-02,  1.2612e-04],\n",
      "          [ 1.1577e-02,  9.8748e-03,  1.4178e-02,  2.3951e-02,  2.3515e-03]],\n",
      "\n",
      "         [[-4.5352e-03, -1.1055e-02, -1.2239e-02, -8.8044e-03, -3.9610e-02],\n",
      "          [-1.7676e-02, -2.3792e-02, -3.4457e-03, -9.7516e-03, -1.2852e-02],\n",
      "          [-1.6892e-02, -2.3949e-02,  6.8701e-03,  1.8911e-02,  1.4407e-02],\n",
      "          [-1.4484e-02,  1.9438e-04,  1.1626e-02, -1.4681e-04, -6.6562e-03],\n",
      "          [ 1.7880e-03, -6.5856e-04,  9.9231e-03, -2.0049e-03,  8.0555e-03]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-6.7122e-03, -4.2302e-03, -2.1216e-03,  7.3907e-03,  2.4879e-02],\n",
      "          [-1.5106e-02, -6.0318e-03, -3.7330e-03, -3.6061e-03, -9.1995e-03],\n",
      "          [ 3.1212e-03,  5.1415e-04, -1.6837e-03,  1.9486e-03, -3.3522e-03],\n",
      "          [ 3.7525e-03, -6.5870e-03, -1.5899e-02, -3.9335e-03,  6.7013e-03],\n",
      "          [ 3.4742e-03,  7.0159e-03,  8.2180e-03,  1.1364e-02,  8.4086e-03]],\n",
      "\n",
      "         [[-3.6575e-03,  3.5438e-03,  1.9375e-03, -1.4799e-02, -2.5230e-02],\n",
      "          [-1.4076e-02, -4.1179e-03,  5.2800e-04, -1.3717e-02,  8.6698e-03],\n",
      "          [-1.5029e-02, -3.2266e-03,  5.9102e-03, -1.0591e-02,  1.4576e-02],\n",
      "          [-8.0884e-03,  6.2744e-03,  8.4515e-03, -9.4480e-03, -1.0429e-04],\n",
      "          [ 7.8814e-04,  5.3071e-03,  1.0671e-02, -5.5833e-03, -5.9989e-03]],\n",
      "\n",
      "         [[-2.7157e-04, -3.7418e-03,  1.4480e-02,  4.3833e-03, -1.3471e-02],\n",
      "          [-3.0676e-03, -9.5242e-03,  4.7271e-03, -1.2380e-02, -2.4728e-02],\n",
      "          [-1.5718e-02, -9.5558e-03,  6.5024e-03,  6.2501e-03,  1.0373e-02],\n",
      "          [-2.0779e-02, -6.3457e-03,  1.3916e-02, -2.9676e-04, -2.2102e-03],\n",
      "          [-4.1625e-03, -1.3656e-03,  1.1172e-02, -5.0689e-03, -4.4298e-03]]],\n",
      "\n",
      "\n",
      "        ...,\n",
      "\n",
      "\n",
      "        [[[-1.8571e-02, -1.2457e-02, -1.4227e-03, -9.0217e-04, -1.9327e-03],\n",
      "          [-9.8200e-03, -8.0331e-03, -8.1169e-03, -1.5714e-04,  6.8577e-03],\n",
      "          [-1.0874e-02, -1.4752e-02, -2.0120e-02, -8.6719e-03,  7.5190e-03],\n",
      "          [ 6.8951e-03,  2.5231e-04, -5.6181e-03, -8.5463e-03, -9.0210e-04],\n",
      "          [ 4.7487e-03,  1.1735e-02,  8.5783e-03, -1.7575e-03, -1.0584e-02]],\n",
      "\n",
      "         [[-1.3230e-02,  1.2232e-02, -5.7320e-03,  2.4933e-02,  1.1790e-02],\n",
      "          [-9.6580e-04, -9.3515e-03, -2.8497e-02,  4.1951e-03,  1.3023e-02],\n",
      "          [-2.2692e-03, -1.3827e-02, -1.7987e-02, -3.1061e-03, -2.3617e-03],\n",
      "          [ 9.4879e-03,  1.8826e-03,  1.9548e-02,  3.8077e-03, -2.0168e-02],\n",
      "          [ 1.3470e-03,  1.0945e-02,  3.2439e-02,  5.6508e-03, -2.3677e-02]],\n",
      "\n",
      "         [[ 3.2262e-04, -1.8014e-02, -1.7984e-02, -1.7061e-02, -1.6767e-02],\n",
      "          [-2.4439e-02, -3.5217e-02, -9.3386e-03,  1.3795e-02, -6.0123e-04],\n",
      "          [-2.4093e-02, -7.4781e-03,  3.0333e-02,  1.6283e-02, -5.5653e-03],\n",
      "          [-1.2613e-02,  1.8250e-02,  2.6285e-02,  2.5114e-03, -6.9747e-03],\n",
      "          [ 2.4991e-02,  2.9340e-02,  8.2110e-03, -7.3423e-03, -8.5176e-03]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-1.3124e-02, -8.1059e-03, -7.4760e-03,  2.6893e-03,  8.8126e-03],\n",
      "          [-1.3209e-02,  2.7956e-03, -1.6209e-03,  4.4534e-03, -1.1559e-02],\n",
      "          [ 5.1671e-03,  1.4876e-02, -2.9908e-03, -6.2712e-03,  6.3992e-03],\n",
      "          [ 4.6658e-03, -6.7423e-03, -9.5431e-04,  9.0379e-03, -2.8573e-03],\n",
      "          [-1.3166e-02, -1.2615e-02, -6.8928e-03,  3.0942e-03, -3.8319e-03]],\n",
      "\n",
      "         [[-1.7462e-02,  1.2540e-02,  5.4073e-03, -1.0554e-02, -1.1243e-02],\n",
      "          [-3.3250e-02, -1.3980e-02,  1.4671e-02,  5.4953e-03, -3.1296e-03],\n",
      "          [-2.0577e-02, -2.2600e-02,  3.4469e-03, -5.9846e-04, -1.0682e-02],\n",
      "          [ 7.1659e-03,  8.2904e-03, -7.2008e-03, -2.2248e-04, -6.5585e-03],\n",
      "          [ 3.2347e-02,  3.9342e-02, -1.4516e-02, -7.6606e-03, -5.2493e-03]],\n",
      "\n",
      "         [[ 3.6029e-02,  1.6851e-02,  1.5146e-02, -6.3458e-03, -7.9147e-03],\n",
      "          [-9.3814e-03, -1.3804e-02,  1.0198e-02,  1.0466e-03,  2.7665e-03],\n",
      "          [-2.1553e-02, -2.2866e-02, -1.4415e-03,  8.4715e-03, -1.2442e-03],\n",
      "          [-2.2229e-02,  3.7443e-03, -2.8167e-03, -1.3365e-02, -5.5468e-03],\n",
      "          [ 4.6584e-03,  1.2254e-02, -1.4628e-03, -1.2428e-02, -5.4998e-03]]],\n",
      "\n",
      "\n",
      "        [[[-1.1074e-02, -1.7371e-02, -1.0706e-02, -4.6045e-03, -2.4582e-03],\n",
      "          [-7.6114e-03, -6.2379e-03, -1.7769e-03,  5.5376e-04,  4.8656e-03],\n",
      "          [-5.0612e-03, -3.9875e-03,  2.3602e-03, -3.8219e-03, -2.5689e-03],\n",
      "          [ 4.3990e-03, -1.4588e-03, -6.3751e-03, -2.2995e-03, -6.0175e-03],\n",
      "          [-6.0608e-03, -4.9243e-03, -4.9291e-03, -5.2477e-03, -5.7349e-03]],\n",
      "\n",
      "         [[-1.7799e-02, -1.8602e-02, -1.0635e-02,  5.1140e-03, -7.2148e-03],\n",
      "          [-1.3055e-02,  1.0642e-03, -2.0280e-03, -1.0671e-02, -5.3543e-03],\n",
      "          [-2.3214e-03,  2.4947e-03,  3.6683e-04, -8.4940e-03, -2.1496e-02],\n",
      "          [ 1.0102e-02,  5.6742e-04, -7.8403e-03, -9.2702e-03, -9.7419e-03],\n",
      "          [ 3.4257e-03, -1.4191e-03, -8.4458e-03, -9.6951e-03, -1.1325e-02]],\n",
      "\n",
      "         [[-2.6934e-02, -8.1536e-03,  1.0750e-02,  4.6411e-03,  1.2123e-02],\n",
      "          [-1.5025e-02, -5.5004e-03,  7.4688e-03,  1.1932e-02, -2.9019e-04],\n",
      "          [ 1.0344e-02,  8.2272e-03, -1.7459e-03, -8.3189e-03, -2.1495e-03],\n",
      "          [-5.1585e-04, -1.2872e-02, -2.0943e-02, -2.1374e-02, -3.0373e-02],\n",
      "          [-1.1541e-02, -1.7369e-02, -1.9576e-02, -2.3155e-02, -2.6566e-02]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-3.8873e-03, -8.3506e-04,  5.6586e-03,  8.8179e-03,  7.6342e-03],\n",
      "          [-1.1508e-03, -4.3679e-03,  7.0156e-03,  1.3454e-03, -3.8283e-03],\n",
      "          [-6.0357e-03, -1.5375e-02, -1.2149e-02, -5.0764e-03, -1.2399e-02],\n",
      "          [-6.7002e-03, -5.9768e-03, -8.4512e-03, -7.2696e-03, -7.7705e-03],\n",
      "          [-3.7207e-03,  6.6078e-04, -2.2973e-03, -9.9018e-03, -7.9252e-03]],\n",
      "\n",
      "         [[-1.9672e-02, -1.5213e-02, -7.3527e-03, -3.4865e-03,  1.1916e-03],\n",
      "          [-1.1678e-02, -9.2898e-03, -5.2898e-03,  3.1627e-03, -2.5506e-03],\n",
      "          [-9.4721e-04,  4.7888e-04, -1.7639e-03, -5.5617e-03, -6.5093e-03],\n",
      "          [ 6.1791e-04, -2.5510e-03, -9.9803e-03, -1.6360e-02, -2.0619e-02],\n",
      "          [-1.1900e-02, -1.8787e-02, -1.7039e-02, -1.9019e-02, -1.9213e-02]],\n",
      "\n",
      "         [[-1.1616e-02, -3.0031e-03,  7.9968e-03,  1.0047e-02,  1.3127e-02],\n",
      "          [-1.2888e-02, -3.7994e-03, -1.7905e-03,  1.8244e-03,  3.2862e-03],\n",
      "          [-3.2414e-03, -1.2928e-04,  1.1041e-02,  1.5714e-04,  1.8576e-03],\n",
      "          [-1.8203e-03,  5.9354e-03,  8.1424e-04, -2.9222e-03, -6.6726e-03],\n",
      "          [-1.7348e-02, -1.0941e-02, -6.9883e-03, -1.0375e-02, -1.6123e-02]]],\n",
      "\n",
      "\n",
      "        [[[ 3.9844e-03,  4.2765e-03,  8.7651e-03,  1.3860e-02,  8.8478e-03],\n",
      "          [ 2.5884e-03,  4.0506e-03,  4.4273e-03,  7.1849e-03,  5.4572e-03],\n",
      "          [ 2.5528e-03,  8.3730e-03,  1.0499e-02,  9.1109e-03,  6.4614e-03],\n",
      "          [ 4.4617e-03,  4.2921e-03,  9.2701e-03,  5.5607e-03,  2.7722e-03],\n",
      "          [ 1.1516e-02,  6.9987e-03,  5.9670e-03,  1.8191e-03,  2.8933e-03]],\n",
      "\n",
      "         [[-7.2434e-04,  6.4067e-03,  2.0449e-02,  2.7069e-02,  1.3165e-02],\n",
      "          [ 5.5763e-03,  1.4851e-02,  1.7502e-02,  1.4501e-02,  1.0193e-02],\n",
      "          [ 4.2987e-03,  1.5437e-02,  1.6014e-02,  7.1948e-03,  4.5726e-03],\n",
      "          [ 9.1828e-03,  1.4628e-02,  5.3432e-03,  1.8691e-03,  3.7828e-04],\n",
      "          [ 7.8794e-03,  9.0898e-03, -2.8276e-03, -2.4919e-03, -9.2977e-03]],\n",
      "\n",
      "         [[ 2.4980e-02,  3.6459e-02,  3.3636e-02,  1.6282e-02,  9.5033e-03],\n",
      "          [ 2.6677e-02,  2.9037e-02,  2.3770e-02,  1.3518e-02,  1.0561e-02],\n",
      "          [ 2.3557e-02,  2.2200e-02,  1.2869e-02,  1.5226e-02,  9.2736e-03],\n",
      "          [ 2.0518e-02,  1.2963e-02,  5.9138e-03,  1.4968e-02,  5.2095e-03],\n",
      "          [ 9.0163e-03,  7.0413e-03,  4.5056e-03, -3.2368e-04, -1.0944e-02]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 6.8198e-03,  6.8691e-03,  4.7791e-03,  1.1654e-02,  7.3726e-03],\n",
      "          [ 4.5447e-03,  8.8624e-03,  1.6357e-02,  1.5757e-02,  6.6674e-03],\n",
      "          [ 2.6600e-04,  3.4428e-03,  9.8907e-03,  9.2990e-03,  4.7492e-03],\n",
      "          [ 4.2348e-03,  5.6028e-03,  9.3373e-03,  4.3611e-03,  1.4290e-03],\n",
      "          [-2.1070e-03, -1.0726e-03, -9.8458e-04, -6.5798e-03,  2.7670e-03]],\n",
      "\n",
      "         [[ 1.7507e-02,  2.6003e-02,  1.8610e-02,  1.1892e-02,  1.0818e-02],\n",
      "          [ 2.0796e-02,  1.7092e-02,  1.3960e-02,  7.3466e-03,  9.8717e-03],\n",
      "          [ 2.2035e-02,  1.3056e-02,  1.3522e-02,  7.3774e-03,  8.4608e-03],\n",
      "          [ 1.5241e-02,  9.5934e-03,  1.1046e-02,  3.1993e-03,  6.3687e-03],\n",
      "          [ 1.1272e-02,  2.4494e-03,  4.9360e-03, -3.6860e-03,  1.7664e-03]],\n",
      "\n",
      "         [[ 1.3012e-02,  1.3523e-02,  1.8122e-02,  7.5997e-03,  5.6765e-03],\n",
      "          [ 1.2874e-02,  1.1063e-02,  1.4482e-02,  1.0150e-02,  2.5685e-03],\n",
      "          [ 1.2785e-02,  1.0119e-02,  4.9487e-03,  7.4184e-03,  5.6763e-03],\n",
      "          [ 7.8856e-03,  9.1250e-03,  9.1801e-03,  1.2378e-02,  1.9382e-03],\n",
      "          [ 4.9912e-03,  6.0796e-03,  9.0614e-03,  5.4848e-03, -1.9069e-03]]]])}, 3: {'momentum_buffer': tensor([ 0.0027, -0.0042,  0.0009,  0.0038, -0.0114,  0.0025, -0.0042,  0.0117,\n",
      "        -0.0037, -0.0124,  0.0071,  0.0021, -0.0103,  0.0012, -0.0058,  0.0087,\n",
      "        -0.0066, -0.0032, -0.0018,  0.0024])}, 4: {'momentum_buffer': tensor([[ 2.1491e-02,  4.8160e-02,  2.1655e-02,  ..., -8.4055e-03,\n",
      "         -5.3537e-04,  3.7869e-02],\n",
      "        [-1.6645e-02, -2.0983e-02, -2.8439e-02,  ...,  5.9244e-03,\n",
      "         -4.7024e-02, -6.9408e-02],\n",
      "        [ 9.6979e-04, -2.3240e-02, -2.3467e-02,  ...,  7.6564e-03,\n",
      "         -2.3006e-04, -6.1351e-03],\n",
      "        ...,\n",
      "        [ 7.7318e-04,  4.2932e-03,  9.4237e-04,  ...,  8.8402e-05,\n",
      "         -6.3488e-05,  2.7124e-04],\n",
      "        [ 2.7324e-02, -9.6276e-03,  1.2119e-02,  ...,  3.0111e-03,\n",
      "         -3.3469e-03, -2.4955e-03],\n",
      "        [-9.5620e-03, -9.7903e-03, -8.3581e-03,  ...,  2.1878e-02,\n",
      "          5.9153e-03,  3.4976e-03]])}, 5: {'momentum_buffer': tensor([ 7.6317e-03, -1.3063e-02, -4.9985e-03, -2.0846e-03, -1.2788e-03,\n",
      "         4.4178e-03,  3.8667e-04, -4.4637e-03, -2.6392e-03, -1.0459e-04,\n",
      "         5.3889e-03,  2.2457e-03, -5.4713e-03,  5.7754e-03,  2.4209e-03,\n",
      "        -1.1061e-02, -2.7006e-03,  3.1020e-03,  6.0094e-03, -4.4586e-03,\n",
      "         1.4411e-02, -4.9290e-03, -1.1890e-02,  4.5398e-03, -7.1498e-03,\n",
      "        -1.0650e-02,  9.0875e-03, -3.2168e-03, -5.1489e-03, -8.7079e-05,\n",
      "        -2.0115e-02,  2.1580e-03,  3.3802e-03,  5.6070e-03, -1.0090e-02,\n",
      "        -1.2573e-04, -5.1982e-03,  4.3321e-03, -6.1763e-03, -3.8278e-03,\n",
      "         1.9942e-03, -4.9811e-03,  1.5660e-03, -4.1182e-03, -1.2591e-02,\n",
      "        -5.9389e-03,  9.9016e-03,  4.7649e-04,  1.0691e-02, -4.1579e-04])}, 6: {'momentum_buffer': tensor([[ 1.3200e-02,  5.4191e-03,  2.1400e-02, -4.2285e-02,  6.2312e-03,\n",
      "          7.7253e-02,  1.0569e-02,  8.9871e-03,  8.6163e-03, -1.3105e-05,\n",
      "         -2.9247e-03,  1.1033e-02,  1.8687e-02,  5.7007e-03,  9.6111e-02,\n",
      "         -3.2995e-02,  1.5686e-02,  4.5301e-02,  4.8877e-02,  3.4133e-03,\n",
      "         -7.7582e-03,  3.9622e-04,  1.7936e-03,  2.7488e-02,  1.0289e-02,\n",
      "          1.4451e-02,  2.7192e-02,  8.0098e-03, -4.6415e-02,  1.5806e-02,\n",
      "          4.7187e-03,  3.1326e-02,  1.1774e-01, -2.3684e-02,  4.1983e-03,\n",
      "         -1.2843e-01,  2.3353e-02,  8.4522e-03,  9.4103e-03,  5.0331e-03,\n",
      "          1.1790e-02,  1.2082e-02,  9.9591e-03,  3.6508e-02,  1.2252e-02,\n",
      "         -3.8331e-02,  6.5079e-03,  4.6472e-04, -3.5416e-02, -2.7673e-02],\n",
      "        [-1.8338e-02, -1.1384e-01,  5.5080e-03,  2.0283e-03, -4.7121e-02,\n",
      "         -1.5606e-01, -2.2132e-02, -4.8563e-02,  3.0125e-02,  6.4868e-04,\n",
      "         -1.0525e-01, -6.5428e-02,  1.1124e-03, -2.3547e-02, -7.5118e-03,\n",
      "          7.9767e-03, -8.6821e-02,  1.2980e-02,  1.6930e-02, -7.5470e-04,\n",
      "         -4.2701e-03, -1.1076e-02, -7.5589e-02, -8.0210e-02, -1.1511e-03,\n",
      "         -1.2523e-02,  1.4744e-02, -5.8204e-02, -3.3904e-02,  4.7376e-03,\n",
      "         -3.8544e-02,  1.0378e-02,  2.7398e-03,  7.9814e-03, -9.9163e-02,\n",
      "         -1.7566e-02,  2.8170e-03, -1.8522e-02, -9.6269e-02, -5.1063e-03,\n",
      "         -6.0424e-02, -3.2637e-01,  5.5544e-03,  6.4051e-03, -1.0127e-01,\n",
      "          1.0192e-02, -1.5411e-02,  3.2396e-03, -3.3252e-02, -1.3068e-01],\n",
      "        [-7.4787e-02,  5.9518e-02, -1.6728e-02, -5.4854e-03, -3.2869e-02,\n",
      "          1.1773e-01, -1.4895e-01, -5.7277e-04, -2.1594e-01, -8.5884e-03,\n",
      "         -5.2758e-02,  7.3380e-02,  1.7808e-02, -1.7870e-01,  4.1426e-02,\n",
      "         -4.9964e-02, -5.6484e-02,  1.1550e-02,  1.1836e-02,  4.5269e-02,\n",
      "         -2.1864e-02,  3.3508e-03,  1.7861e-02, -1.0702e-01, -1.6341e-02,\n",
      "          2.7338e-03,  6.6377e-04,  1.2383e-02,  3.1345e-03,  3.9993e-03,\n",
      "          2.7937e-02, -4.5865e-02, -7.8657e-02, -1.2986e-01,  2.4243e-02,\n",
      "          1.5955e-02, -2.4870e-02,  1.5374e-02,  4.8302e-03, -2.2508e-02,\n",
      "         -4.5356e-02,  9.2736e-02, -6.1339e-03,  7.5792e-03, -1.2517e-01,\n",
      "         -2.2178e-03,  2.9938e-02,  1.5095e-02, -2.0774e-02,  4.8639e-02],\n",
      "        [-2.0820e-02, -1.0280e-02,  1.0059e-02, -3.0003e-04, -1.7713e-02,\n",
      "         -3.1507e-02,  6.5155e-02,  2.7420e-02, -5.3325e-02,  1.4764e-03,\n",
      "          7.1332e-03,  2.8005e-02, -2.0804e-02,  8.3893e-03,  1.4194e-02,\n",
      "          1.7857e-02,  2.9473e-02, -2.2789e-02, -1.0397e-02, -1.6361e-01,\n",
      "         -6.2404e-02,  5.4046e-03, -3.0989e-02, -1.6178e-02, -3.9664e-04,\n",
      "          8.5014e-03, -9.4905e-02,  5.8985e-03, -4.7891e-02,  4.5060e-03,\n",
      "          1.4009e-02, -2.7503e-02,  9.3763e-03, -7.3379e-02, -5.2412e-03,\n",
      "         -3.2100e-02,  1.0948e-02,  5.6562e-03,  1.7701e-02,  5.0711e-03,\n",
      "          4.9354e-02, -4.3617e-02, -1.9680e-03, -5.8709e-03, -1.7690e-04,\n",
      "          2.3190e-02,  7.3618e-04, -5.7231e-03,  2.3076e-02,  6.5379e-02],\n",
      "        [ 9.0131e-03,  1.7412e-03,  1.9088e-02, -1.0301e-02, -2.8324e-02,\n",
      "          2.0618e-02, -1.2203e-01,  3.3220e-02,  1.1764e-02, -2.5668e-03,\n",
      "          6.7835e-02,  2.2175e-02, -9.0807e-02,  1.0397e-02, -2.6420e-02,\n",
      "          4.3943e-02,  3.0832e-03,  1.3114e-02, -1.3721e-01,  8.5241e-03,\n",
      "          1.0424e-02,  3.6762e-04, -9.6403e-02,  5.0804e-02,  2.2591e-02,\n",
      "         -1.1045e-01, -1.3992e-01,  8.5862e-03, -4.3280e-02,  2.9360e-02,\n",
      "         -9.2984e-03, -7.2532e-02, -7.9919e-03, -2.7792e-02,  1.4715e-02,\n",
      "          2.5139e-02,  9.8484e-03, -3.8766e-02,  3.9611e-02,  8.3577e-03,\n",
      "          7.7111e-03,  2.9021e-03, -5.3382e-03, -1.0347e-01,  1.9862e-02,\n",
      "          3.9668e-03, -1.2276e-02,  9.8039e-04, -2.5974e-01,  2.2140e-02],\n",
      "        [-2.4901e-02,  2.7799e-02,  1.5285e-02,  1.6228e-02,  2.0942e-02,\n",
      "          9.2039e-03,  1.8715e-02, -4.2286e-03,  4.2811e-02,  2.0000e-03,\n",
      "          1.2020e-02,  2.2332e-02,  1.3070e-02,  7.3919e-03,  8.7862e-03,\n",
      "          7.6227e-02,  1.6148e-02,  7.4596e-02,  9.3433e-02,  9.5657e-02,\n",
      "          6.3016e-02,  4.2923e-03,  2.6695e-02,  2.3240e-02, -9.9731e-02,\n",
      "          9.2355e-03,  2.9683e-02,  4.5115e-03,  3.4701e-02,  2.1277e-02,\n",
      "          3.6347e-02,  8.9849e-02,  5.4993e-03,  1.2148e-01,  5.5147e-03,\n",
      "          5.8965e-02, -7.1045e-02,  1.0548e-02, -4.2264e-03, -3.8322e-03,\n",
      "         -9.9742e-03,  6.6446e-02, -7.6243e-04,  2.1389e-02,  2.1765e-02,\n",
      "         -4.9052e-03,  1.2829e-02,  8.2935e-04,  6.2679e-02,  6.9355e-03],\n",
      "        [ 1.8104e-03, -5.5815e-03,  6.6174e-04, -3.0990e-03,  4.0514e-03,\n",
      "         -1.3260e-01,  1.8385e-02, -3.8254e-03,  1.3586e-01,  7.3896e-04,\n",
      "          6.4635e-02, -1.5850e-02,  1.2769e-02,  1.0307e-02, -1.4047e-01,\n",
      "         -1.7092e-01,  5.9919e-03, -1.5382e-01, -1.7361e-02, -3.9733e-02,\n",
      "         -8.8774e-03, -2.3665e-03, -6.4238e-03, -7.4682e-03,  2.6162e-02,\n",
      "          2.6248e-02, -2.1503e-02, -1.1305e-02, -3.2004e-03, -1.1655e-02,\n",
      "         -1.0263e-01,  7.0755e-02,  1.4440e-02,  1.3164e-02,  1.3938e-02,\n",
      "          1.1955e-01,  2.9682e-02, -2.5665e-02, -5.0590e-03, -1.5178e-02,\n",
      "          1.1239e-02,  2.2591e-02, -1.3530e-02,  3.7053e-03,  2.4508e-02,\n",
      "          1.0139e-02,  2.1997e-02,  8.2189e-04,  1.3209e-01,  2.4871e-02],\n",
      "        [ 2.2187e-02, -1.1770e-03, -7.8119e-02,  6.4738e-03,  6.5645e-02,\n",
      "         -9.3681e-02, -1.1189e-03, -3.3755e-03, -2.6125e-02, -5.1821e-04,\n",
      "         -4.4975e-02, -7.3045e-02, -4.0557e-02,  1.5736e-01, -2.4808e-02,\n",
      "          4.0243e-02,  1.0248e-01,  1.0985e-03, -2.0769e-01, -1.3600e-03,\n",
      "         -1.9455e-02,  5.1583e-03,  3.2149e-02, -7.1598e-02,  1.2509e-02,\n",
      "          6.3693e-03,  6.6714e-03, -1.3122e-02,  1.4305e-02, -6.4013e-02,\n",
      "         -7.6918e-03,  2.4432e-02, -6.8233e-02,  1.0349e-01, -1.4729e-02,\n",
      "         -2.5520e-02,  6.8678e-03, -1.2110e-03,  7.1554e-04,  3.6470e-03,\n",
      "          6.0337e-02,  7.7613e-02,  1.7295e-03, -1.6930e-01,  1.1298e-01,\n",
      "          6.6967e-02,  4.8379e-03, -1.8723e-02, -1.3941e-02,  2.2383e-02],\n",
      "        [-3.3267e-03,  2.8217e-02,  1.0493e-02,  4.5522e-03,  2.8347e-02,\n",
      "          1.4125e-01, -4.2834e-03,  6.5832e-03,  4.7364e-02,  3.1239e-03,\n",
      "         -8.9912e-03,  5.0993e-03,  1.1869e-02,  4.6565e-03,  5.4829e-02,\n",
      "          8.1522e-02, -5.8661e-02,  1.8452e-02, -1.5666e-02,  2.0175e-02,\n",
      "          3.3456e-02, -8.6067e-03,  2.1117e-02,  6.8790e-02,  1.2474e-02,\n",
      "          1.2403e-02, -3.1509e-02,  2.6232e-02,  3.0295e-02,  3.6454e-03,\n",
      "          9.0636e-03, -4.8350e-02,  1.7492e-02,  4.1032e-04,  2.8886e-02,\n",
      "         -3.9810e-02, -7.9893e-03, -3.5713e-02, -2.7935e-02,  1.8188e-02,\n",
      "          4.1882e-02,  9.1248e-02, -5.2596e-03, -9.9755e-03,  8.4341e-03,\n",
      "         -8.9149e-02, -7.3741e-02,  1.8090e-03, -1.7958e-02,  2.7272e-02],\n",
      "        [ 9.5962e-02,  8.1837e-03,  1.2352e-02,  3.2188e-02,  8.0947e-04,\n",
      "          4.7800e-02,  1.8569e-01, -1.5644e-02,  1.8847e-02,  3.6986e-03,\n",
      "          6.3272e-02, -7.7014e-03,  7.6852e-02, -1.9483e-03, -1.6140e-02,\n",
      "         -1.3890e-02,  2.9100e-02, -4.8768e-04,  2.1725e-01,  3.2419e-02,\n",
      "          1.7734e-02,  3.0795e-03,  1.0979e-01,  1.1215e-01,  3.3594e-02,\n",
      "          4.3032e-02,  2.0889e-01,  1.7009e-02,  9.2254e-02, -7.6632e-03,\n",
      "          6.6088e-02, -3.2492e-02, -1.2403e-02,  8.1980e-03,  2.7639e-02,\n",
      "          2.3811e-02,  2.0388e-02,  7.9847e-02,  6.1220e-02,  6.3273e-03,\n",
      "         -6.6559e-02,  4.3699e-03,  1.5749e-02,  2.1303e-01,  2.6811e-02,\n",
      "          2.0147e-02,  2.4582e-02,  1.2059e-03,  1.6323e-01, -5.9265e-02]])}, 7: {'momentum_buffer': tensor([ 0.0103, -0.0365, -0.0121, -0.0028, -0.0060,  0.0146, -0.0044, -0.0058,\n",
      "         0.0078,  0.0348])}}, 'param_groups': [{'lr': 0.01, 'momentum': 0.5, 'dampening': 0, 'weight_decay': 0, 'nesterov': False, 'params': [0, 1, 2, 3, 4, 5, 6, 7]}]}\n"
     ]
    }
   ],
   "source": [
    "# First open arrays\n",
    "model_array_1 = tiledb.open('tiledb-pytorch-mnist-1')[:]\n",
    "\n",
    "# Load model state_dict\n",
    "model_1_state_dict = pickle.loads(model_array_1['model_state_dict'].item(0))\n",
    "\n",
    "# Load optimizer state_dict\n",
    "optimizer_1_state_dict = pickle.loads(model_array_1['optimizer_state_dict'].item(0))\n",
    "\n",
    "print(model_1_state_dict)\n",
    "print(optimizer_1_state_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Moving on, we can load the trained models for prediction, evaluation or retraining, as usual with\n",
    "PyTorch models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Place holder for the loaded model\n",
    "network = Net()\n",
    "optimizer = optim.SGD(network.parameters(), lr=learning_rate, momentum=momentum)\n",
    "\n",
    "# Load returns possible extra attributes, other than model's and optimizer's state dicts. In case there were\n",
    "# no extra attributes it will return an empty dict\n",
    "_ = tiledb_model_1.load(model=network, optimizer=optimizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What is really nice with saving models as TileDB array, is native versioning based on fragments as described\n",
    "[here](https://docs.tiledb.com/main/concepts/data-format#immutable-fragments). We can load a model, retrain it\n",
    "with new data and update the already existing TileDB model array with new model parameters and metadata. All information, old\n",
    "and new will be there and accessible. This is extremely useful when you retrain with new data or trying different architectures for the same\n",
    "problem, and you want to keep track of all your experiments without having to store different model instances. In our case,\n",
    "let's continue training model_1 with the rest of our dataset and for 2 more epochs. After training is done, you will\n",
    "notice the extra directories and files (fragments) added to tiledb-keras-mnist-sequential-1 TileDB array directory,\n",
    "which keep all versions of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1 [0/60000 (0%)]\tLoss: 0.576272\n",
      "Train Epoch: 1 [1280/60000 (2%)]\tLoss: 0.600712\n",
      "Train Epoch: 1 [2560/60000 (4%)]\tLoss: 0.750536\n",
      "Train Epoch: 1 [3840/60000 (6%)]\tLoss: 0.668426\n",
      "Train Epoch: 1 [5120/60000 (9%)]\tLoss: 0.645401\n",
      "Train Epoch: 1 [6400/60000 (11%)]\tLoss: 0.544283\n",
      "Train Epoch: 1 [7680/60000 (13%)]\tLoss: 0.664902\n",
      "Train Epoch: 1 [8960/60000 (15%)]\tLoss: 0.463885\n",
      "Train Epoch: 1 [10240/60000 (17%)]\tLoss: 0.633202\n",
      "Train Epoch: 1 [11520/60000 (19%)]\tLoss: 0.620702\n",
      "Train Epoch: 1 [12800/60000 (21%)]\tLoss: 0.638514\n",
      "Train Epoch: 1 [14080/60000 (23%)]\tLoss: 0.521937\n",
      "Train Epoch: 1 [15360/60000 (26%)]\tLoss: 0.568641\n",
      "Train Epoch: 1 [16640/60000 (28%)]\tLoss: 0.403443\n",
      "Train Epoch: 1 [17920/60000 (30%)]\tLoss: 0.734374\n",
      "Train Epoch: 1 [19200/60000 (32%)]\tLoss: 0.519801\n",
      "Train Epoch: 1 [20480/60000 (34%)]\tLoss: 0.616732\n",
      "Train Epoch: 1 [21760/60000 (36%)]\tLoss: 0.627395\n",
      "Train Epoch: 1 [23040/60000 (38%)]\tLoss: 0.622394\n",
      "Train Epoch: 1 [24320/60000 (41%)]\tLoss: 0.427548\n",
      "Train Epoch: 1 [25600/60000 (43%)]\tLoss: 0.572651\n",
      "Train Epoch: 1 [26880/60000 (45%)]\tLoss: 0.436058\n",
      "Train Epoch: 1 [28160/60000 (47%)]\tLoss: 0.502791\n",
      "Train Epoch: 1 [29440/60000 (49%)]\tLoss: 0.477583\n",
      "Train Epoch: 1 [30720/60000 (51%)]\tLoss: 0.555044\n",
      "Train Epoch: 1 [32000/60000 (53%)]\tLoss: 0.510869\n",
      "Train Epoch: 1 [33280/60000 (55%)]\tLoss: 0.548664\n",
      "Train Epoch: 1 [34560/60000 (58%)]\tLoss: 0.530004\n",
      "Train Epoch: 1 [35840/60000 (60%)]\tLoss: 0.411406\n",
      "Train Epoch: 1 [37120/60000 (62%)]\tLoss: 0.554757\n",
      "Train Epoch: 1 [38400/60000 (64%)]\tLoss: 0.505362\n",
      "Train Epoch: 1 [39680/60000 (66%)]\tLoss: 0.437215\n",
      "Train Epoch: 1 [40960/60000 (68%)]\tLoss: 0.423777\n",
      "Train Epoch: 1 [42240/60000 (70%)]\tLoss: 0.358317\n",
      "Train Epoch: 1 [43520/60000 (72%)]\tLoss: 0.378239\n",
      "Train Epoch: 1 [44800/60000 (75%)]\tLoss: 0.427609\n",
      "Train Epoch: 1 [46080/60000 (77%)]\tLoss: 0.643378\n",
      "Train Epoch: 1 [47360/60000 (79%)]\tLoss: 0.595157\n",
      "Train Epoch: 1 [48640/60000 (81%)]\tLoss: 0.643645\n",
      "Train Epoch: 1 [49920/60000 (83%)]\tLoss: 0.420812\n",
      "Train Epoch: 1 [51200/60000 (85%)]\tLoss: 0.633425\n",
      "Train Epoch: 1 [52480/60000 (87%)]\tLoss: 0.566453\n",
      "Train Epoch: 1 [53760/60000 (90%)]\tLoss: 0.431015\n",
      "Train Epoch: 1 [55040/60000 (92%)]\tLoss: 0.635182\n",
      "Train Epoch: 1 [56320/60000 (94%)]\tLoss: 0.452681\n",
      "Train Epoch: 1 [57600/60000 (96%)]\tLoss: 0.506694\n",
      "Train Epoch: 1 [58880/60000 (98%)]\tLoss: 0.671234\n",
      "Train Epoch: 2 [0/60000 (0%)]\tLoss: 0.426433\n",
      "Train Epoch: 2 [1280/60000 (2%)]\tLoss: 0.542953\n",
      "Train Epoch: 2 [2560/60000 (4%)]\tLoss: 0.431448\n",
      "Train Epoch: 2 [3840/60000 (6%)]\tLoss: 0.333652\n",
      "Train Epoch: 2 [5120/60000 (9%)]\tLoss: 0.367727\n",
      "Train Epoch: 2 [6400/60000 (11%)]\tLoss: 0.578225\n",
      "Train Epoch: 2 [7680/60000 (13%)]\tLoss: 0.576888\n",
      "Train Epoch: 2 [8960/60000 (15%)]\tLoss: 0.269501\n",
      "Train Epoch: 2 [10240/60000 (17%)]\tLoss: 0.382370\n",
      "Train Epoch: 2 [11520/60000 (19%)]\tLoss: 0.395886\n",
      "Train Epoch: 2 [12800/60000 (21%)]\tLoss: 0.374322\n",
      "Train Epoch: 2 [14080/60000 (23%)]\tLoss: 0.297448\n",
      "Train Epoch: 2 [15360/60000 (26%)]\tLoss: 0.475420\n",
      "Train Epoch: 2 [16640/60000 (28%)]\tLoss: 0.296931\n",
      "Train Epoch: 2 [17920/60000 (30%)]\tLoss: 0.549110\n",
      "Train Epoch: 2 [19200/60000 (32%)]\tLoss: 0.354616\n",
      "Train Epoch: 2 [20480/60000 (34%)]\tLoss: 0.432848\n",
      "Train Epoch: 2 [21760/60000 (36%)]\tLoss: 0.382388\n",
      "Train Epoch: 2 [23040/60000 (38%)]\tLoss: 0.276277\n",
      "Train Epoch: 2 [24320/60000 (41%)]\tLoss: 0.454842\n",
      "Train Epoch: 2 [25600/60000 (43%)]\tLoss: 0.353321\n",
      "Train Epoch: 2 [26880/60000 (45%)]\tLoss: 0.451096\n",
      "Train Epoch: 2 [28160/60000 (47%)]\tLoss: 0.316580\n",
      "Train Epoch: 2 [29440/60000 (49%)]\tLoss: 0.364130\n",
      "Train Epoch: 2 [30720/60000 (51%)]\tLoss: 0.431465\n",
      "Train Epoch: 2 [32000/60000 (53%)]\tLoss: 0.524982\n",
      "Train Epoch: 2 [33280/60000 (55%)]\tLoss: 0.438477\n",
      "Train Epoch: 2 [34560/60000 (58%)]\tLoss: 0.454826\n",
      "Train Epoch: 2 [35840/60000 (60%)]\tLoss: 0.518262\n",
      "Train Epoch: 2 [37120/60000 (62%)]\tLoss: 0.471013\n",
      "Train Epoch: 2 [38400/60000 (64%)]\tLoss: 0.274934\n",
      "Train Epoch: 2 [39680/60000 (66%)]\tLoss: 0.398929\n",
      "Train Epoch: 2 [40960/60000 (68%)]\tLoss: 0.395629\n",
      "Train Epoch: 2 [42240/60000 (70%)]\tLoss: 0.404211\n",
      "Train Epoch: 2 [43520/60000 (72%)]\tLoss: 0.466308\n",
      "Train Epoch: 2 [44800/60000 (75%)]\tLoss: 0.306582\n",
      "Train Epoch: 2 [46080/60000 (77%)]\tLoss: 0.306776\n",
      "Train Epoch: 2 [47360/60000 (79%)]\tLoss: 0.521016\n",
      "Train Epoch: 2 [48640/60000 (81%)]\tLoss: 0.333933\n",
      "Train Epoch: 2 [49920/60000 (83%)]\tLoss: 0.384981\n",
      "Train Epoch: 2 [51200/60000 (85%)]\tLoss: 0.307746\n",
      "Train Epoch: 2 [52480/60000 (87%)]\tLoss: 0.305437\n",
      "Train Epoch: 2 [53760/60000 (90%)]\tLoss: 0.244203\n",
      "Train Epoch: 2 [55040/60000 (92%)]\tLoss: 0.301768\n",
      "Train Epoch: 2 [56320/60000 (94%)]\tLoss: 0.211211\n",
      "Train Epoch: 2 [57600/60000 (96%)]\tLoss: 0.319129\n",
      "Train Epoch: 2 [58880/60000 (98%)]\tLoss: 0.281229\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:root:Internal Python error in the inspect module.\n",
      "Below is the traceback from this internal error.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/Users/konstantinostsitsimpikos/tileroot/TileDB-ML/venv2/lib/python3.9/site-packages/IPython/core/interactiveshell.py\", line 3441, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"<ipython-input-12-b8e59f468cc8>\", line 10, in <module>\n",
      "    tiledb_model_1.save(update=True,\n",
      "  File \"/Users/konstantinostsitsimpikos/tileroot/TileDB-ML/tiledb/ml/models/pytorch.py\", line 95, in save\n",
      "    self._write_array(\n",
      "  File \"/Users/konstantinostsitsimpikos/tileroot/TileDB-ML/tiledb/ml/models/pytorch.py\", line 232, in _write_array\n",
      "    with tiledb.open(\n",
      "  File \"/Users/konstantinostsitsimpikos/tileroot/TileDB-ML/venv2/lib/python3.9/site-packages/tiledb/highlevel.py\", line 23, in open\n",
      "    return tiledb.Array.load_typed(\n",
      "  File \"tiledb/libtiledb.pyx\", line 3447, in tiledb.libtiledb.Array.load_typed\n",
      "  File \"tiledb/libtiledb.pyx\", line 3273, in tiledb.libtiledb.preload_array\n",
      "  File \"tiledb/libtiledb.pyx\", line 526, in tiledb.libtiledb._raise_ctx_err\n",
      "  File \"tiledb/libtiledb.pyx\", line 511, in tiledb.libtiledb._raise_tiledb_error\n",
      "tiledb.cc.TileDBError: [TileDB::Array] Error: Cannot open array; Array does not exist\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/konstantinostsitsimpikos/tileroot/TileDB-ML/venv2/lib/python3.9/site-packages/IPython/core/interactiveshell.py\", line 2061, in showtraceback\n",
      "    stb = value._render_traceback_()\n",
      "AttributeError: 'TileDBError' object has no attribute '_render_traceback_'\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/konstantinostsitsimpikos/tileroot/TileDB-ML/venv2/lib/python3.9/site-packages/IPython/core/ultratb.py\", line 1101, in get_records\n",
      "    return _fixed_getinnerframes(etb, number_of_lines_of_context, tb_offset)\n",
      "  File \"/Users/konstantinostsitsimpikos/tileroot/TileDB-ML/venv2/lib/python3.9/site-packages/IPython/core/ultratb.py\", line 248, in wrapped\n",
      "    return f(*args, **kwargs)\n",
      "  File \"/Users/konstantinostsitsimpikos/tileroot/TileDB-ML/venv2/lib/python3.9/site-packages/IPython/core/ultratb.py\", line 281, in _fixed_getinnerframes\n",
      "    records = fix_frame_records_filenames(inspect.getinnerframes(etb, context))\n",
      "  File \"/usr/local/Cellar/python@3.9/3.9.5/Frameworks/Python.framework/Versions/3.9/lib/python3.9/inspect.py\", line 1541, in getinnerframes\n",
      "    frameinfo = (tb.tb_frame,) + getframeinfo(tb, context)\n",
      "  File \"/usr/local/Cellar/python@3.9/3.9.5/Frameworks/Python.framework/Versions/3.9/lib/python3.9/inspect.py\", line 1499, in getframeinfo\n",
      "    filename = getsourcefile(frame) or getfile(frame)\n",
      "  File \"/usr/local/Cellar/python@3.9/3.9.5/Frameworks/Python.framework/Versions/3.9/lib/python3.9/inspect.py\", line 709, in getsourcefile\n",
      "    if getattr(getmodule(object, filename), '__loader__', None) is not None:\n",
      "  File \"/usr/local/Cellar/python@3.9/3.9.5/Frameworks/Python.framework/Versions/3.9/lib/python3.9/inspect.py\", line 755, in getmodule\n",
      "    os.path.realpath(f)] = module.__name__\n",
      "  File \"/usr/local/Cellar/python@3.9/3.9.5/Frameworks/Python.framework/Versions/3.9/lib/python3.9/posixpath.py\", line 391, in realpath\n",
      "    path, ok = _joinrealpath(filename[:0], filename, {})\n",
      "  File \"/usr/local/Cellar/python@3.9/3.9.5/Frameworks/Python.framework/Versions/3.9/lib/python3.9/posixpath.py\", line 425, in _joinrealpath\n",
      "    if not islink(newpath):\n",
      "  File \"/usr/local/Cellar/python@3.9/3.9.5/Frameworks/Python.framework/Versions/3.9/lib/python3.9/posixpath.py\", line 167, in islink\n",
      "    st = os.lstat(path)\n",
      "KeyboardInterrupt\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "object of type 'NoneType' has no len()",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mTileDBError\u001B[0m                               Traceback (most recent call last)",
      "    \u001B[0;31m[... skipping hidden 1 frame]\u001B[0m\n",
      "\u001B[0;32m<ipython-input-12-b8e59f468cc8>\u001B[0m in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[1;32m      9\u001B[0m \u001B[0mtiledb_model_1\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mPyTorchTileDBModel\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0muri\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0;34m'tiledb-pytorch-mnist-1'\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mmodel\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mnetwork\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0moptimizer\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0moptimizer\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m---> 10\u001B[0;31m tiledb_model_1.save(update=True, \n\u001B[0m\u001B[1;32m     11\u001B[0m                     meta={'epochs': epochs,\n",
      "\u001B[0;32m~/tileroot/TileDB-ML/tiledb/ml/models/pytorch.py\u001B[0m in \u001B[0;36msave\u001B[0;34m(self, update, meta, model_info, summary_writer)\u001B[0m\n\u001B[1;32m     94\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m---> 95\u001B[0;31m         self._write_array(\n\u001B[0m\u001B[1;32m     96\u001B[0m             {\n",
      "\u001B[0;32m~/tileroot/TileDB-ML/tiledb/ml/models/pytorch.py\u001B[0m in \u001B[0;36m_write_array\u001B[0;34m(self, serialized_model_dict, meta)\u001B[0m\n\u001B[1;32m    231\u001B[0m         \u001B[0;31m# TODO: Change timestamp when issue in core is resolved\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 232\u001B[0;31m         with tiledb.open(\n\u001B[0m\u001B[1;32m    233\u001B[0m             \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0muri\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m\"w\"\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mtimestamp\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mcurrent_milli_time\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mctx\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mctx\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/tileroot/TileDB-ML/venv2/lib/python3.9/site-packages/tiledb/highlevel.py\u001B[0m in \u001B[0;36mopen\u001B[0;34m(uri, mode, key, attr, config, timestamp, ctx)\u001B[0m\n\u001B[1;32m     22\u001B[0m     \"\"\"\n\u001B[0;32m---> 23\u001B[0;31m     return tiledb.Array.load_typed(\n\u001B[0m\u001B[1;32m     24\u001B[0m         \u001B[0muri\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32mtiledb/libtiledb.pyx\u001B[0m in \u001B[0;36mtiledb.libtiledb.Array.load_typed\u001B[0;34m()\u001B[0m\n",
      "\u001B[0;32mtiledb/libtiledb.pyx\u001B[0m in \u001B[0;36mtiledb.libtiledb.preload_array\u001B[0;34m()\u001B[0m\n",
      "\u001B[0;32mtiledb/libtiledb.pyx\u001B[0m in \u001B[0;36mtiledb.libtiledb._raise_ctx_err\u001B[0;34m()\u001B[0m\n",
      "\u001B[0;32mtiledb/libtiledb.pyx\u001B[0m in \u001B[0;36mtiledb.libtiledb._raise_tiledb_error\u001B[0;34m()\u001B[0m\n",
      "\u001B[0;31mTileDBError\u001B[0m: [TileDB::Array] Error: Cannot open array; Array does not exist",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001B[0;31mAttributeError\u001B[0m                            Traceback (most recent call last)",
      "\u001B[0;32m~/tileroot/TileDB-ML/venv2/lib/python3.9/site-packages/IPython/core/interactiveshell.py\u001B[0m in \u001B[0;36mshowtraceback\u001B[0;34m(self, exc_tuple, filename, tb_offset, exception_only, running_compiled_code)\u001B[0m\n\u001B[1;32m   2060\u001B[0m                         \u001B[0;31m# in the engines. This should return a list of strings.\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m-> 2061\u001B[0;31m                         \u001B[0mstb\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mvalue\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_render_traceback_\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m   2062\u001B[0m                     \u001B[0;32mexcept\u001B[0m \u001B[0mException\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;31mAttributeError\u001B[0m: 'TileDBError' object has no attribute '_render_traceback_'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001B[0;31mTypeError\u001B[0m                                 Traceback (most recent call last)",
      "    \u001B[0;31m[... skipping hidden 1 frame]\u001B[0m\n",
      "\u001B[0;32m~/tileroot/TileDB-ML/venv2/lib/python3.9/site-packages/IPython/core/interactiveshell.py\u001B[0m in \u001B[0;36mshowtraceback\u001B[0;34m(self, exc_tuple, filename, tb_offset, exception_only, running_compiled_code)\u001B[0m\n\u001B[1;32m   2061\u001B[0m                         \u001B[0mstb\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mvalue\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_render_traceback_\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   2062\u001B[0m                     \u001B[0;32mexcept\u001B[0m \u001B[0mException\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m-> 2063\u001B[0;31m                         stb = self.InteractiveTB.structured_traceback(etype,\n\u001B[0m\u001B[1;32m   2064\u001B[0m                                             value, tb, tb_offset=tb_offset)\n\u001B[1;32m   2065\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/tileroot/TileDB-ML/venv2/lib/python3.9/site-packages/IPython/core/ultratb.py\u001B[0m in \u001B[0;36mstructured_traceback\u001B[0;34m(self, etype, value, tb, tb_offset, number_of_lines_of_context)\u001B[0m\n\u001B[1;32m   1365\u001B[0m         \u001B[0;32melse\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1366\u001B[0m             \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mtb\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mtb\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m-> 1367\u001B[0;31m         return FormattedTB.structured_traceback(\n\u001B[0m\u001B[1;32m   1368\u001B[0m             self, etype, value, tb, tb_offset, number_of_lines_of_context)\n\u001B[1;32m   1369\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/tileroot/TileDB-ML/venv2/lib/python3.9/site-packages/IPython/core/ultratb.py\u001B[0m in \u001B[0;36mstructured_traceback\u001B[0;34m(self, etype, value, tb, tb_offset, number_of_lines_of_context)\u001B[0m\n\u001B[1;32m   1265\u001B[0m         \u001B[0;32mif\u001B[0m \u001B[0mmode\u001B[0m \u001B[0;32min\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mverbose_modes\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1266\u001B[0m             \u001B[0;31m# Verbose modes need a full traceback\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m-> 1267\u001B[0;31m             return VerboseTB.structured_traceback(\n\u001B[0m\u001B[1;32m   1268\u001B[0m                 \u001B[0mself\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0metype\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mvalue\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mtb\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mtb_offset\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mnumber_of_lines_of_context\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1269\u001B[0m             )\n",
      "\u001B[0;32m~/tileroot/TileDB-ML/venv2/lib/python3.9/site-packages/IPython/core/ultratb.py\u001B[0m in \u001B[0;36mstructured_traceback\u001B[0;34m(self, etype, evalue, etb, tb_offset, number_of_lines_of_context)\u001B[0m\n\u001B[1;32m   1122\u001B[0m         \u001B[0;34m\"\"\"Return a nice text document describing the traceback.\"\"\"\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1123\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m-> 1124\u001B[0;31m         formatted_exception = self.format_exception_as_a_whole(etype, evalue, etb, number_of_lines_of_context,\n\u001B[0m\u001B[1;32m   1125\u001B[0m                                                                tb_offset)\n\u001B[1;32m   1126\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/tileroot/TileDB-ML/venv2/lib/python3.9/site-packages/IPython/core/ultratb.py\u001B[0m in \u001B[0;36mformat_exception_as_a_whole\u001B[0;34m(self, etype, evalue, etb, number_of_lines_of_context, tb_offset)\u001B[0m\n\u001B[1;32m   1080\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1081\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m-> 1082\u001B[0;31m         \u001B[0mlast_unique\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mrecursion_repeat\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mfind_recursion\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0morig_etype\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mevalue\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mrecords\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m   1083\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1084\u001B[0m         \u001B[0mframes\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mformat_records\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mrecords\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mlast_unique\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mrecursion_repeat\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/tileroot/TileDB-ML/venv2/lib/python3.9/site-packages/IPython/core/ultratb.py\u001B[0m in \u001B[0;36mfind_recursion\u001B[0;34m(etype, value, records)\u001B[0m\n\u001B[1;32m    380\u001B[0m     \u001B[0;31m# first frame (from in to out) that looks different.\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    381\u001B[0m     \u001B[0;32mif\u001B[0m \u001B[0;32mnot\u001B[0m \u001B[0mis_recursion_error\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0metype\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mvalue\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mrecords\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 382\u001B[0;31m         \u001B[0;32mreturn\u001B[0m \u001B[0mlen\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mrecords\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;36m0\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    383\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    384\u001B[0m     \u001B[0;31m# Select filename, lineno, func_name to track frames with\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;31mTypeError\u001B[0m: object of type 'NoneType' has no len()"
     ]
    }
   ],
   "source": [
    "train_losses = []\n",
    "train_counter = []\n",
    "\n",
    "# We train for some extra 2 epochs\n",
    "for epoch in range(1, 2 + 1):\n",
    "  train(epoch)\n",
    "\n",
    "# and update\n",
    "tiledb_model_1 = PyTorchTileDBModel(uri='tiledb-pytorch-mnist-1', model=network, optimizer=optimizer)\n",
    "tiledb_model_1.save(update=True, \n",
    "                    meta={'epochs': epochs,\n",
    "                          'train_loss': train_losses})\n",
    "\n",
    "# Check array directory\n",
    "print()\n",
    "pprint(glob.glob('tiledb-pytorch-mnist-1/*'))\n",
    "\n",
    "# tiledb.array_fragments() requires TileDB-Py version > 0.8.5\n",
    "fragments_info = tiledb.array_fragments('tiledb-pytorch-mnist-1')\n",
    "\n",
    "print()\n",
    "print(\"====== FRAGMENTS  INFO ======\")\n",
    "print(\"array uri: {}\".format(fragments_info.array_uri))\n",
    "print(\"number of fragments: {}\".format(len(fragments_info)))\n",
    "\n",
    "for fragment_num, fragment in enumerate(fragments_info, start=1):\n",
    "    print()\n",
    "    print(\"===== FRAGMENT NUMBER {} =====\".format(fragment.num))\n",
    "    print(\"fragment uri: {}\".format(fragment.uri))\n",
    "    print(\"timestamp range: {}\".format(fragment.timestamp_range))\n",
    "    print(\n",
    "        \"number of unconsolidated metadata: {}\".format(\n",
    "            fragment.unconsolidated_metadata_num\n",
    "        )\n",
    "    )\n",
    "    print(\"version: {}\".format(fragment.version))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, a very interesting and useful, for machine learning models, TileDB feature that is described\n",
    "[here](https://docs.tiledb.com/main/concepts/data-format#groups) and [here](https://docs.tiledb.com/main/how-to/object-management#creating-tiledb-groups)\n",
    "are groups. Assuming we want to solve the MNIST problem, and we want to try several architectures. We can save each architecture\n",
    "as a separate TileDB array with native versioning each time it is re-trained, and then organise all models that solve the same problem (MNIST)\n",
    "as a TileDB array group with any kind of hierarchy. Let's firstly define a new model architecture."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "class OtherNet(nn.Module):\n",
    "    # For the sake of simplicity we just tweak the initial architecture by replacing a relu with relu6.\n",
    "    def __init__(self):\n",
    "        super(OtherNet, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 10, kernel_size=5)\n",
    "        self.conv2 = nn.Conv2d(10, 20, kernel_size=5)\n",
    "        self.conv2_drop = nn.Dropout2d()\n",
    "        self.fc1 = nn.Linear(320, 50)\n",
    "        self.fc2 = nn.Linear(50, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(F.max_pool2d(self.conv1(x), 2))\n",
    "        x = F.relu(F.max_pool2d(self.conv2_drop(self.conv2(x)), 2))\n",
    "        x = x.view(-1, 320)\n",
    "        x = F.relu6(self.fc1(x))\n",
    "        x = F.dropout(x, training=self.training)\n",
    "        x = self.fc2(x)\n",
    "        return F.log_softmax(x, dim = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then train it and save it as a new TileDB array."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1 [0/60000 (0%)]\tLoss: 2.313390\n",
      "Train Epoch: 1 [1280/60000 (2%)]\tLoss: 1.527101\n",
      "Train Epoch: 1 [2560/60000 (4%)]\tLoss: 0.971071\n",
      "Train Epoch: 1 [3840/60000 (6%)]\tLoss: 0.857200\n",
      "Train Epoch: 1 [5120/60000 (9%)]\tLoss: 0.513735\n",
      "Train Epoch: 1 [6400/60000 (11%)]\tLoss: 0.778414\n",
      "Train Epoch: 1 [7680/60000 (13%)]\tLoss: 0.419221\n",
      "Train Epoch: 1 [8960/60000 (15%)]\tLoss: 0.507100\n",
      "Train Epoch: 1 [10240/60000 (17%)]\tLoss: 0.543142\n",
      "Train Epoch: 1 [11520/60000 (19%)]\tLoss: 0.465935\n",
      "Train Epoch: 1 [12800/60000 (21%)]\tLoss: 0.572005\n",
      "Train Epoch: 1 [14080/60000 (23%)]\tLoss: 0.583393\n",
      "Train Epoch: 1 [15360/60000 (26%)]\tLoss: 0.410894\n",
      "Train Epoch: 1 [16640/60000 (28%)]\tLoss: 0.496960\n",
      "Train Epoch: 1 [17920/60000 (30%)]\tLoss: 0.500474\n",
      "Train Epoch: 1 [19200/60000 (32%)]\tLoss: 0.343156\n",
      "Train Epoch: 1 [20480/60000 (34%)]\tLoss: 0.501805\n",
      "Train Epoch: 1 [21760/60000 (36%)]\tLoss: 0.543956\n",
      "Train Epoch: 1 [23040/60000 (38%)]\tLoss: 0.558721\n",
      "Train Epoch: 1 [24320/60000 (41%)]\tLoss: 0.365339\n",
      "Train Epoch: 1 [25600/60000 (43%)]\tLoss: 0.427882\n",
      "Train Epoch: 1 [26880/60000 (45%)]\tLoss: 0.413578\n",
      "Train Epoch: 1 [28160/60000 (47%)]\tLoss: 0.437229\n",
      "Train Epoch: 1 [29440/60000 (49%)]\tLoss: 0.424080\n",
      "Train Epoch: 1 [30720/60000 (51%)]\tLoss: 0.486696\n",
      "Train Epoch: 1 [32000/60000 (53%)]\tLoss: 0.345004\n",
      "Train Epoch: 1 [33280/60000 (55%)]\tLoss: 0.429583\n",
      "Train Epoch: 1 [34560/60000 (58%)]\tLoss: 0.329117\n",
      "Train Epoch: 1 [35840/60000 (60%)]\tLoss: 0.303430\n",
      "Train Epoch: 1 [37120/60000 (62%)]\tLoss: 0.379547\n",
      "Train Epoch: 1 [38400/60000 (64%)]\tLoss: 0.465007\n",
      "Train Epoch: 1 [39680/60000 (66%)]\tLoss: 0.301131\n",
      "Train Epoch: 1 [40960/60000 (68%)]\tLoss: 0.264844\n",
      "Train Epoch: 1 [42240/60000 (70%)]\tLoss: 0.346164\n",
      "Train Epoch: 1 [43520/60000 (72%)]\tLoss: 0.441803\n",
      "Train Epoch: 1 [44800/60000 (75%)]\tLoss: 0.318938\n",
      "Train Epoch: 1 [46080/60000 (77%)]\tLoss: 0.254888\n",
      "Train Epoch: 1 [47360/60000 (79%)]\tLoss: 0.319974\n",
      "Train Epoch: 1 [48640/60000 (81%)]\tLoss: 0.382300\n",
      "Train Epoch: 1 [49920/60000 (83%)]\tLoss: 0.404844\n",
      "Train Epoch: 1 [51200/60000 (85%)]\tLoss: 0.267870\n",
      "Train Epoch: 1 [52480/60000 (87%)]\tLoss: 0.374096\n",
      "Train Epoch: 1 [53760/60000 (90%)]\tLoss: 0.442077\n",
      "Train Epoch: 1 [55040/60000 (92%)]\tLoss: 0.533317\n",
      "Train Epoch: 1 [56320/60000 (94%)]\tLoss: 0.429747\n",
      "Train Epoch: 1 [57600/60000 (96%)]\tLoss: 0.439626\n",
      "Train Epoch: 1 [58880/60000 (98%)]\tLoss: 0.338414\n"
     ]
    }
   ],
   "source": [
    "network = OtherNet()\n",
    "optimizer = optim.Adam(network.parameters(), lr=learning_rate)\n",
    "\n",
    "train_losses = []\n",
    "train_counter = []\n",
    "\n",
    "for epoch in range(1, epochs + 1):\n",
    "    train(epoch)\n",
    "\n",
    "tiledb_model_2 = PyTorchTileDBModel(uri='tiledb-pytorch-mnist-2', model=network, optimizer=optimizer)\n",
    "\n",
    "tiledb_model_2.save(update=False, \n",
    "                    meta={'epochs': epochs,\n",
    "                          'train_loss': train_losses})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can create a TileDB group and organise (in hierarchies, e.g., sophisticated vs less sophisticated) all our\n",
    "MNIST models as follows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "0"
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tiledb.group_create('MNIST_Group')\n",
    "os.system('mv tiledb-pytorch-mnist-1 MNIST_Group/')\n",
    "os.system('mv tiledb-pytorch-mnist-2 MNIST_Group/')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Any time we can check and query all the available models, including their metadata, for a specific problem like MNIST."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "file:///Users/konstantinostsitsimpikos/tileroot/TileDB-ML/examples/models/MNIST_Group/tiledb-pytorch-mnist-1 array\n",
      "file:///Users/konstantinostsitsimpikos/tileroot/TileDB-ML/examples/models/MNIST_Group/tiledb-pytorch-mnist-2 array\n"
     ]
    }
   ],
   "source": [
    "tiledb.ls('MNIST_Group', lambda obj_path, obj_type: print(obj_path, obj_type))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}