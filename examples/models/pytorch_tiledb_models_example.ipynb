{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This example notebook shows how we can train an [image/digit classification](https://pytorch.org/tutorials/beginner/nn_tutorial.html?highlight=mnist)\n",
    "model based on MNIST dataset, and store it as TileDB array. Firstly, let's import what we need."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import glob\n",
    "import json\n",
    "import pickle\n",
    "import os\n",
    "import shutil\n",
    "from pprint import pprint\n",
    "\n",
    "import tiledb\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "\n",
    "from tiledb.ml.models.pytorch import PyTorchTileDBModel\n",
    "\n",
    "TB_KEY = '__TENSORBOARD__'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First let's define the parameters/hyperparameters we will need."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7efcd6e724f0>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "epochs = 1\n",
    "batch_size_train = 128\n",
    "learning_rate = 0.01\n",
    "momentum = 0.5\n",
    "log_interval = 10\n",
    "\n",
    "# Set random seeds for anything using random number generation\n",
    "random_seed = 1\n",
    "\n",
    "# Disable nondeterministic algorithms\n",
    "torch.backends.cudnn.enabled = False\n",
    "torch.manual_seed(random_seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We  will also need the DataLoaders API for the dataset. We will also employ TorchVision which let's as load the MNIST\n",
    "dataset in a handy way. We'll use a batch_size of 64 for training while the values 0.1307 and 0.3081 used for\n",
    "the Normalize() transformation below are the global mean and standard deviation of the MNIST dataset,\n",
    "we'll take them as a given here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "data_home = os.path.join(os.path.pardir, 'data')\n",
    "dataset = torchvision.datasets.MNIST(\n",
    "    root=data_home, \n",
    "    train=True, \n",
    "    download=True,\n",
    "    transform=torchvision.transforms.Compose([\n",
    "        torchvision.transforms.ToTensor(),\n",
    "        torchvision.transforms.Normalize((0.1307,), (0.3081,))\n",
    "    ])\n",
    ")\n",
    "train_loader = torch.utils.data.DataLoader(dataset, batch_size=batch_size_train, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Moving on, we build our network. We'll use two 2-D convolutional layers followed by two fully-connected\n",
    "layers. As activation function we'll choose ReLUs and as a means of regularization we'll use two dropout layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 10, kernel_size=5)\n",
    "        self.conv2 = nn.Conv2d(10, 20, kernel_size=5)\n",
    "        self.conv2_drop = nn.Dropout2d()\n",
    "        self.fc1 = nn.Linear(320, 50)\n",
    "        self.fc2 = nn.Linear(50, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(F.max_pool2d(self.conv1(x), 2))\n",
    "        x = F.relu(F.max_pool2d(self.conv2_drop(self.conv2(x)), 2))\n",
    "        x = x.view(-1, 320)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.dropout(x, training=self.training)\n",
    "        x = self.fc2(x)\n",
    "        return F.log_softmax(x, dim = 1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will now initialise our Neural Network and optimizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "network = Net()\n",
    "optimizer = optim.SGD(network.parameters(), lr=learning_rate, momentum=momentum)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "We continue with the training loop and we iterate over all training data once per epoch. Loading the individual batches\n",
    "is handled by the DataLoader. We need to set the gradients to zero using optimizer.zero_grad() since PyTorch by default\n",
    "accumulates gradients. We then produce the output of the network (forward pass) and compute a negative log-likelihodd\n",
    "loss between the output and the ground truth label. The backward() call we now collect a new set of gradients which we\n",
    "propagate back into each of the network's parameters using optimizer.step()."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1 [0/60000 (0%)]\tLoss: 2.358812\n",
      "Train Epoch: 1 [1280/60000 (2%)]\tLoss: 2.285138\n",
      "Train Epoch: 1 [2560/60000 (4%)]\tLoss: 2.306635\n",
      "Train Epoch: 1 [3840/60000 (6%)]\tLoss: 2.270880\n",
      "Train Epoch: 1 [5120/60000 (9%)]\tLoss: 2.236739\n",
      "Train Epoch: 1 [6400/60000 (11%)]\tLoss: 2.243352\n",
      "Train Epoch: 1 [7680/60000 (13%)]\tLoss: 2.183357\n",
      "Train Epoch: 1 [8960/60000 (15%)]\tLoss: 2.148380\n",
      "Train Epoch: 1 [10240/60000 (17%)]\tLoss: 2.104831\n",
      "Train Epoch: 1 [11520/60000 (19%)]\tLoss: 2.004480\n",
      "Train Epoch: 1 [12800/60000 (21%)]\tLoss: 1.861931\n",
      "Train Epoch: 1 [14080/60000 (23%)]\tLoss: 1.884068\n",
      "Train Epoch: 1 [15360/60000 (26%)]\tLoss: 1.797761\n",
      "Train Epoch: 1 [16640/60000 (28%)]\tLoss: 1.687489\n",
      "Train Epoch: 1 [17920/60000 (30%)]\tLoss: 1.509098\n",
      "Train Epoch: 1 [19200/60000 (32%)]\tLoss: 1.766841\n",
      "Train Epoch: 1 [20480/60000 (34%)]\tLoss: 1.469026\n",
      "Train Epoch: 1 [21760/60000 (36%)]\tLoss: 1.349417\n",
      "Train Epoch: 1 [23040/60000 (38%)]\tLoss: 1.291566\n",
      "Train Epoch: 1 [24320/60000 (41%)]\tLoss: 1.022800\n",
      "Train Epoch: 1 [25600/60000 (43%)]\tLoss: 1.045352\n",
      "Train Epoch: 1 [26880/60000 (45%)]\tLoss: 1.095343\n",
      "Train Epoch: 1 [28160/60000 (47%)]\tLoss: 1.226705\n",
      "Train Epoch: 1 [29440/60000 (49%)]\tLoss: 1.148985\n",
      "Train Epoch: 1 [30720/60000 (51%)]\tLoss: 0.845941\n",
      "Train Epoch: 1 [32000/60000 (53%)]\tLoss: 0.968638\n",
      "Train Epoch: 1 [33280/60000 (55%)]\tLoss: 0.853761\n",
      "Train Epoch: 1 [34560/60000 (58%)]\tLoss: 0.926097\n",
      "Train Epoch: 1 [35840/60000 (60%)]\tLoss: 1.018820\n",
      "Train Epoch: 1 [37120/60000 (62%)]\tLoss: 0.807615\n",
      "Train Epoch: 1 [38400/60000 (64%)]\tLoss: 0.879665\n",
      "Train Epoch: 1 [39680/60000 (66%)]\tLoss: 0.820776\n",
      "Train Epoch: 1 [40960/60000 (68%)]\tLoss: 0.801570\n",
      "Train Epoch: 1 [42240/60000 (70%)]\tLoss: 0.825205\n",
      "Train Epoch: 1 [43520/60000 (72%)]\tLoss: 0.855643\n",
      "Train Epoch: 1 [44800/60000 (75%)]\tLoss: 0.768875\n",
      "Train Epoch: 1 [46080/60000 (77%)]\tLoss: 0.722621\n",
      "Train Epoch: 1 [47360/60000 (79%)]\tLoss: 0.548631\n",
      "Train Epoch: 1 [48640/60000 (81%)]\tLoss: 0.760798\n",
      "Train Epoch: 1 [49920/60000 (83%)]\tLoss: 0.654459\n",
      "Train Epoch: 1 [51200/60000 (85%)]\tLoss: 0.697698\n",
      "Train Epoch: 1 [52480/60000 (87%)]\tLoss: 0.805110\n",
      "Train Epoch: 1 [53760/60000 (90%)]\tLoss: 0.790563\n",
      "Train Epoch: 1 [55040/60000 (92%)]\tLoss: 0.759908\n",
      "Train Epoch: 1 [56320/60000 (94%)]\tLoss: 0.535904\n",
      "Train Epoch: 1 [57600/60000 (96%)]\tLoss: 0.643054\n",
      "Train Epoch: 1 [58880/60000 (98%)]\tLoss: 0.657785\n"
     ]
    }
   ],
   "source": [
    "train_losses = []\n",
    "train_counter = []\n",
    "\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "writer = SummaryWriter()\n",
    "\n",
    "\n",
    "def train(epoch):\n",
    "  network.train()\n",
    "  for batch_idx, (data, target) in enumerate(train_loader):\n",
    "    optimizer.zero_grad()\n",
    "    output = network(data)\n",
    "    loss = F.nll_loss(output, target)\n",
    "    writer.add_scalar(\"Loss/train\", loss, epoch)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    if batch_idx % log_interval == 0:\n",
    "      print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "        epoch, batch_idx * len(data), len(train_loader.dataset),\n",
    "        100. * batch_idx / len(train_loader), loss.item()))\n",
    "      train_losses.append(loss.item())\n",
    "      train_counter.append(\n",
    "        (batch_idx*64) + ((epoch-1)*len(train_loader.dataset)))\n",
    "\n",
    "for epoch in range(1, epochs + 1):\n",
    "  train(epoch)\n",
    "writer.flush()\n",
    "writer.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now save the trained model as a TileDB array. In case we want to train  the model further in a later time, we can also save\n",
    "the optimizer in our TileDB array. In case we will use our model only for inference, we don't have to save the optimizer and we\n",
    "only keep the model. We first declare a PytTorchTileDB object and initialize it with the corresponding TileDB uri, model and optimizer,\n",
    "and then save the model as a TileDB array. Finally, we can save any kind of metadata (in any structure, i.e., list, tuple or dictionary)\n",
    "by passing a dictionary to the meta attribute."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "uri = os.path.join(data_home, 'pytorch-mnist-1')\n",
    "tiledb_model_1 = PyTorchTileDBModel(uri=uri, model=network, optimizer=optimizer)\n",
    "\n",
    "tiledb_model_1.save(update=False,\n",
    "                    meta={'epochs': epochs,\n",
    "                          'train_loss': train_losses},\n",
    "                    summary_writer=writer)\n",
    "tiledb_model_1.load_tensorboard()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above step will create a TileDB array in your working directory. For information about the structure of a dense\n",
    "TileDB array in terms of files on disk please take a look [here](https://docs.tiledb.com/main/concepts/data-format).\n",
    "Let's open our TileDB array model and check metadata. Metadata that are of type list, dict or tuple have been JSON\n",
    "serialized while saving, i.e., we need json.loads to deserialize them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['../data/pytorch-mnist-1/__fragment_meta',\n",
      " '../data/pytorch-mnist-1/__meta',\n",
      " '../data/pytorch-mnist-1/__fragments',\n",
      " '../data/pytorch-mnist-1/__commits',\n",
      " '../data/pytorch-mnist-1/__schema']\n",
      "Key: TILEDB_ML_MODEL_ML_FRAMEWORK, Value: PYTORCH\n",
      "Key: TILEDB_ML_MODEL_ML_FRAMEWORK_VERSION, Value: 1.12.0.post2\n",
      "Key: TILEDB_ML_MODEL_PREVIEW, Value: Net(\n",
      "  (conv1): Conv2d(1, 10, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2): Conv2d(10, 20, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2_drop): Dropout2d(p=0.5, inplace=False)\n",
      "  (fc1): Linear(in_features=320, out_features=50, bias=True)\n",
      "  (fc2): Linear(in_features=50, out_features=10, bias=True)\n",
      ")\n",
      "Key: TILEDB_ML_MODEL_PYTHON_VERSION, Value: 3.7.13\n",
      "Key: TILEDB_ML_MODEL_STAGE, Value: STAGING\n",
      "Key: __TENSORBOARD__, Value: <binary_data>\n",
      "Key: epochs, Value: 1\n",
      "Key: train_loss, Value: (2.358812093734741, 2.285137891769409, 2.3066349029541016, 2.2708797454833984, 2.23673939704895, 2.243352174758911, 2.1833572387695312, 2.1483800411224365, 2.1048314571380615, 2.0044803619384766, 1.8619309663772583, 1.8840677738189697, 1.7977608442306519, 1.6874892711639404, 1.5090980529785156, 1.7668406963348389, 1.46902596950531, 1.3494172096252441, 1.2915657758712769, 1.0228004455566406, 1.045351505279541, 1.0953425168991089, 1.2267048358917236, 1.1489849090576172, 0.8459413051605225, 0.968638002872467, 0.8537610769271851, 0.9260968565940857, 1.0188199281692505, 0.8076149821281433, 0.8796654939651489, 0.8207758665084839, 0.8015697598457336, 0.8252049088478088, 0.8556432127952576, 0.7688748836517334, 0.7226207256317139, 0.5486305952072144, 0.7607976794242859, 0.6544589996337891, 0.6976978778839111, 0.8051103949546814, 0.7905630469322205, 0.7599075436592102, 0.5359035134315491, 0.6430540680885315, 0.6577849984169006)\n"
     ]
    }
   ],
   "source": [
    "# Check array directory\n",
    "pprint(glob.glob(f'{uri}/*'))\n",
    "\n",
    "# Open in write mode in order to add metadata\n",
    "model_array_1 = tiledb.open(uri)\n",
    "for key, value in model_array_1.meta.items():\n",
    "    if isinstance(value, bytes)  and key != TB_KEY:\n",
    "        value = json.loads(value)\n",
    "    print(\"Key: {}, Value: {}\".format(key, value if key != TB_KEY else \"<binary_data>\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, in array's metadata we have by default information about the backend we used for training (pytorch),\n",
    "pytorch version, python version and the extra metadata about epochs and training loss that we added.\n",
    "We can load and check any of the aforementioned without having to load the entire model in memory.\n",
    "Moreover, we can add any kind of extra information in model's metadata also by opening the TileDB array and adding new keys."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Key: TILEDB_ML_MODEL_ML_FRAMEWORK, Value: PYTORCH\n",
      "Key: TILEDB_ML_MODEL_ML_FRAMEWORK_VERSION, Value: 1.12.0.post2\n",
      "Key: TILEDB_ML_MODEL_PREVIEW, Value: Net(\n",
      "  (conv1): Conv2d(1, 10, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2): Conv2d(10, 20, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2_drop): Dropout2d(p=0.5, inplace=False)\n",
      "  (fc1): Linear(in_features=320, out_features=50, bias=True)\n",
      "  (fc2): Linear(in_features=50, out_features=10, bias=True)\n",
      ")\n",
      "Key: TILEDB_ML_MODEL_PYTHON_VERSION, Value: 3.7.13\n",
      "Key: TILEDB_ML_MODEL_STAGE, Value: STAGING\n",
      "Key: __TENSORBOARD__, Value: <binary_data>\n",
      "Key: epochs, Value: 1\n",
      "Key: new_meta, Value: [\"Any kind of info\"]\n",
      "Key: train_loss, Value: (2.358812093734741, 2.285137891769409, 2.3066349029541016, 2.2708797454833984, 2.23673939704895, 2.243352174758911, 2.1833572387695312, 2.1483800411224365, 2.1048314571380615, 2.0044803619384766, 1.8619309663772583, 1.8840677738189697, 1.7977608442306519, 1.6874892711639404, 1.5090980529785156, 1.7668406963348389, 1.46902596950531, 1.3494172096252441, 1.2915657758712769, 1.0228004455566406, 1.045351505279541, 1.0953425168991089, 1.2267048358917236, 1.1489849090576172, 0.8459413051605225, 0.968638002872467, 0.8537610769271851, 0.9260968565940857, 1.0188199281692505, 0.8076149821281433, 0.8796654939651489, 0.8207758665084839, 0.8015697598457336, 0.8252049088478088, 0.8556432127952576, 0.7688748836517334, 0.7226207256317139, 0.5486305952072144, 0.7607976794242859, 0.6544589996337891, 0.6976978778839111, 0.8051103949546814, 0.7905630469322205, 0.7599075436592102, 0.5359035134315491, 0.6430540680885315, 0.6577849984169006)\n"
     ]
    }
   ],
   "source": [
    "# Open the array in write mode\n",
    "with tiledb.Array(uri, \"w\") as A:\n",
    "    # Keep all history\n",
    "    A.meta['new_meta'] = json.dumps(['Any kind of info'])\n",
    "\n",
    "# Check that everything is there\n",
    "model_array_1 = tiledb.open(uri)\n",
    "for key, value in model_array_1.meta.items():\n",
    "    if isinstance(value, bytes) and key != TB_KEY:\n",
    "        value = json.loads(value)\n",
    "    print(\"Key: {}, Value: {}\".format(key, value if key != TB_KEY else \"<binary_data>\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the case of PyTorch models, internally, we save model's state_dict and optimizer's state_dict,\n",
    "as [variable sized attributes)](https://docs.tiledb.com/main/how-to/arrays/writing-arrays/var-length-attributes)\n",
    "(pickled), i.e., we can open the TileDB and get only the state_dict of the model or optimizer,\n",
    "without bringing the whole model in memory. For example, we can load model's and optimizer's state_dict\n",
    "for model `pytorch-mnist-1` as follows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Type: <class 'collections.OrderedDict'> , Keys: odict_keys(['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias'])\n",
      "Type: <class 'dict'>, Keys: dict_keys(['state', 'param_groups'])\n"
     ]
    }
   ],
   "source": [
    "# First open arrays\n",
    "model_array_1 = tiledb.open(uri)[:]\n",
    "\n",
    "# Load model state_dict\n",
    "model_1_state_dict = pickle.loads(model_array_1['model_state_dict'].item(0))\n",
    "\n",
    "# Load optimizer state_dict\n",
    "optimizer_1_state_dict = pickle.loads(model_array_1['optimizer_state_dict'].item(0))\n",
    "\n",
    "print(f'Type: {type(model_1_state_dict)} , Keys: {model_1_state_dict.keys()}')\n",
    "print(f'Type: {type(optimizer_1_state_dict)}, Keys: {optimizer_1_state_dict.keys()}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Moving on, we can load the trained models for prediction, evaluation or retraining, as usual with\n",
    "PyTorch models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Place holder for the loaded model\n",
    "network = Net()\n",
    "optimizer = optim.SGD(network.parameters(), lr=learning_rate, momentum=momentum)\n",
    "\n",
    "# Load returns possible extra attributes, other than model's and optimizer's state dicts. In case there were\n",
    "# no extra attributes it will return an empty dict\n",
    "_ = tiledb_model_1.load(model=network, optimizer=optimizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What is really nice with saving models as TileDB array, is native versioning based on fragments as described [here](https://docs.tiledb.com/main/concepts/data-format#immutable-fragments). We can load a model, retrain it with new data and update the already existing TileDB model array with new model parameters and metadata. All information, old and new will be there and accessible. This is extremely useful when you retrain with new data or trying different architectures for the same problem, and you want to keep track of all your experiments without having to store different model instances. In our case, let's continue training model_1 with the rest of our dataset and for 2 more epochs. After training is done, you will notice the extra directories and files (fragments) added to `pytorch-mnist-1` TileDB array directory, which keep all versions of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1 [0/60000 (0%)]\tLoss: 0.573006\n",
      "Train Epoch: 1 [1280/60000 (2%)]\tLoss: 0.601681\n",
      "Train Epoch: 1 [2560/60000 (4%)]\tLoss: 0.752959\n",
      "Train Epoch: 1 [3840/60000 (6%)]\tLoss: 0.664741\n",
      "Train Epoch: 1 [5120/60000 (9%)]\tLoss: 0.644972\n",
      "Train Epoch: 1 [6400/60000 (11%)]\tLoss: 0.541002\n",
      "Train Epoch: 1 [7680/60000 (13%)]\tLoss: 0.667386\n",
      "Train Epoch: 1 [8960/60000 (15%)]\tLoss: 0.462301\n",
      "Train Epoch: 1 [10240/60000 (17%)]\tLoss: 0.631507\n",
      "Train Epoch: 1 [11520/60000 (19%)]\tLoss: 0.613044\n",
      "Train Epoch: 1 [12800/60000 (21%)]\tLoss: 0.634575\n",
      "Train Epoch: 1 [14080/60000 (23%)]\tLoss: 0.526842\n",
      "Train Epoch: 1 [15360/60000 (26%)]\tLoss: 0.575023\n",
      "Train Epoch: 1 [16640/60000 (28%)]\tLoss: 0.404579\n",
      "Train Epoch: 1 [17920/60000 (30%)]\tLoss: 0.728565\n",
      "Train Epoch: 1 [19200/60000 (32%)]\tLoss: 0.524444\n",
      "Train Epoch: 1 [20480/60000 (34%)]\tLoss: 0.614918\n",
      "Train Epoch: 1 [21760/60000 (36%)]\tLoss: 0.627587\n",
      "Train Epoch: 1 [23040/60000 (38%)]\tLoss: 0.631840\n",
      "Train Epoch: 1 [24320/60000 (41%)]\tLoss: 0.429085\n",
      "Train Epoch: 1 [25600/60000 (43%)]\tLoss: 0.579470\n",
      "Train Epoch: 1 [26880/60000 (45%)]\tLoss: 0.431121\n",
      "Train Epoch: 1 [28160/60000 (47%)]\tLoss: 0.505631\n",
      "Train Epoch: 1 [29440/60000 (49%)]\tLoss: 0.478399\n",
      "Train Epoch: 1 [30720/60000 (51%)]\tLoss: 0.554100\n",
      "Train Epoch: 1 [32000/60000 (53%)]\tLoss: 0.515864\n",
      "Train Epoch: 1 [33280/60000 (55%)]\tLoss: 0.550555\n",
      "Train Epoch: 1 [34560/60000 (58%)]\tLoss: 0.533414\n",
      "Train Epoch: 1 [35840/60000 (60%)]\tLoss: 0.409655\n",
      "Train Epoch: 1 [37120/60000 (62%)]\tLoss: 0.546622\n",
      "Train Epoch: 1 [38400/60000 (64%)]\tLoss: 0.501863\n",
      "Train Epoch: 1 [39680/60000 (66%)]\tLoss: 0.435006\n",
      "Train Epoch: 1 [40960/60000 (68%)]\tLoss: 0.422183\n",
      "Train Epoch: 1 [42240/60000 (70%)]\tLoss: 0.356396\n",
      "Train Epoch: 1 [43520/60000 (72%)]\tLoss: 0.377008\n",
      "Train Epoch: 1 [44800/60000 (75%)]\tLoss: 0.434437\n",
      "Train Epoch: 1 [46080/60000 (77%)]\tLoss: 0.650021\n",
      "Train Epoch: 1 [47360/60000 (79%)]\tLoss: 0.588891\n",
      "Train Epoch: 1 [48640/60000 (81%)]\tLoss: 0.645413\n",
      "Train Epoch: 1 [49920/60000 (83%)]\tLoss: 0.411683\n",
      "Train Epoch: 1 [51200/60000 (85%)]\tLoss: 0.644087\n",
      "Train Epoch: 1 [52480/60000 (87%)]\tLoss: 0.569591\n",
      "Train Epoch: 1 [53760/60000 (90%)]\tLoss: 0.429919\n",
      "Train Epoch: 1 [55040/60000 (92%)]\tLoss: 0.624053\n",
      "Train Epoch: 1 [56320/60000 (94%)]\tLoss: 0.454052\n",
      "Train Epoch: 1 [57600/60000 (96%)]\tLoss: 0.509696\n",
      "Train Epoch: 1 [58880/60000 (98%)]\tLoss: 0.674107\n",
      "Train Epoch: 2 [0/60000 (0%)]\tLoss: 0.425954\n",
      "Train Epoch: 2 [1280/60000 (2%)]\tLoss: 0.545427\n",
      "Train Epoch: 2 [2560/60000 (4%)]\tLoss: 0.428018\n",
      "Train Epoch: 2 [3840/60000 (6%)]\tLoss: 0.326388\n",
      "Train Epoch: 2 [5120/60000 (9%)]\tLoss: 0.380663\n",
      "Train Epoch: 2 [6400/60000 (11%)]\tLoss: 0.578326\n",
      "Train Epoch: 2 [7680/60000 (13%)]\tLoss: 0.554632\n",
      "Train Epoch: 2 [8960/60000 (15%)]\tLoss: 0.269548\n",
      "Train Epoch: 2 [10240/60000 (17%)]\tLoss: 0.377522\n",
      "Train Epoch: 2 [11520/60000 (19%)]\tLoss: 0.393217\n",
      "Train Epoch: 2 [12800/60000 (21%)]\tLoss: 0.378511\n",
      "Train Epoch: 2 [14080/60000 (23%)]\tLoss: 0.295535\n",
      "Train Epoch: 2 [15360/60000 (26%)]\tLoss: 0.477590\n",
      "Train Epoch: 2 [16640/60000 (28%)]\tLoss: 0.298139\n",
      "Train Epoch: 2 [17920/60000 (30%)]\tLoss: 0.550014\n",
      "Train Epoch: 2 [19200/60000 (32%)]\tLoss: 0.363178\n",
      "Train Epoch: 2 [20480/60000 (34%)]\tLoss: 0.431328\n",
      "Train Epoch: 2 [21760/60000 (36%)]\tLoss: 0.377542\n",
      "Train Epoch: 2 [23040/60000 (38%)]\tLoss: 0.272112\n",
      "Train Epoch: 2 [24320/60000 (41%)]\tLoss: 0.447334\n",
      "Train Epoch: 2 [25600/60000 (43%)]\tLoss: 0.356538\n",
      "Train Epoch: 2 [26880/60000 (45%)]\tLoss: 0.441073\n",
      "Train Epoch: 2 [28160/60000 (47%)]\tLoss: 0.322216\n",
      "Train Epoch: 2 [29440/60000 (49%)]\tLoss: 0.368352\n",
      "Train Epoch: 2 [30720/60000 (51%)]\tLoss: 0.434976\n",
      "Train Epoch: 2 [32000/60000 (53%)]\tLoss: 0.527472\n",
      "Train Epoch: 2 [33280/60000 (55%)]\tLoss: 0.433748\n",
      "Train Epoch: 2 [34560/60000 (58%)]\tLoss: 0.458764\n",
      "Train Epoch: 2 [35840/60000 (60%)]\tLoss: 0.518815\n",
      "Train Epoch: 2 [37120/60000 (62%)]\tLoss: 0.471178\n",
      "Train Epoch: 2 [38400/60000 (64%)]\tLoss: 0.275469\n",
      "Train Epoch: 2 [39680/60000 (66%)]\tLoss: 0.397159\n",
      "Train Epoch: 2 [40960/60000 (68%)]\tLoss: 0.392574\n",
      "Train Epoch: 2 [42240/60000 (70%)]\tLoss: 0.395142\n",
      "Train Epoch: 2 [43520/60000 (72%)]\tLoss: 0.470279\n",
      "Train Epoch: 2 [44800/60000 (75%)]\tLoss: 0.306299\n",
      "Train Epoch: 2 [46080/60000 (77%)]\tLoss: 0.302847\n",
      "Train Epoch: 2 [47360/60000 (79%)]\tLoss: 0.518914\n",
      "Train Epoch: 2 [48640/60000 (81%)]\tLoss: 0.332354\n",
      "Train Epoch: 2 [49920/60000 (83%)]\tLoss: 0.382687\n",
      "Train Epoch: 2 [51200/60000 (85%)]\tLoss: 0.308637\n",
      "Train Epoch: 2 [52480/60000 (87%)]\tLoss: 0.312392\n",
      "Train Epoch: 2 [53760/60000 (90%)]\tLoss: 0.246227\n",
      "Train Epoch: 2 [55040/60000 (92%)]\tLoss: 0.296688\n",
      "Train Epoch: 2 [56320/60000 (94%)]\tLoss: 0.209723\n",
      "Train Epoch: 2 [57600/60000 (96%)]\tLoss: 0.324043\n",
      "Train Epoch: 2 [58880/60000 (98%)]\tLoss: 0.290833\n",
      "\n",
      "['../data/pytorch-mnist-1/__fragment_meta',\n",
      " '../data/pytorch-mnist-1/__meta',\n",
      " '../data/pytorch-mnist-1/__fragments',\n",
      " '../data/pytorch-mnist-1/__commits',\n",
      " '../data/pytorch-mnist-1/__schema']\n",
      "\n",
      "====== FRAGMENTS  INFO ======\n",
      "array uri: ../data/pytorch-mnist-1\n",
      "number of fragments: 2\n",
      "\n",
      "===== FRAGMENT NUMBER 0 =====\n",
      "fragment uri: file:///home/gsk/projects/TileDB-ML/examples/data/pytorch-mnist-1/__fragments/__1660136128168_1660136128168_03679b1d42ec4110becccd5cf2d1566f_14\n",
      "timestamp range: (1660136128168, 1660136128168)\n",
      "number of unconsolidated metadata: 2\n",
      "version: 14\n",
      "\n",
      "===== FRAGMENT NUMBER 1 =====\n",
      "fragment uri: file:///home/gsk/projects/TileDB-ML/examples/data/pytorch-mnist-1/__fragments/__1660136148231_1660136148231_54917a9c1ed54687a37c2c9541788244_14\n",
      "timestamp range: (1660136148231, 1660136148231)\n",
      "number of unconsolidated metadata: 2\n",
      "version: 14\n"
     ]
    }
   ],
   "source": [
    "train_losses = []\n",
    "train_counter = []\n",
    "\n",
    "# We train for some extra 2 epochs\n",
    "for epoch in range(1, 2 + 1):\n",
    "  train(epoch)\n",
    "\n",
    "# and update\n",
    "tiledb_model_1 = PyTorchTileDBModel(uri=uri, model=network, optimizer=optimizer)\n",
    "tiledb_model_1.save(update=True, \n",
    "                    meta={'epochs': epochs,\n",
    "                          'train_loss': train_losses})\n",
    "\n",
    "# Check array directory\n",
    "print()\n",
    "pprint(glob.glob(f'{uri}/*'))\n",
    "\n",
    "# tiledb.array_fragments() requires TileDB-Py version > 0.8.5\n",
    "fragments_info = tiledb.array_fragments(uri)\n",
    "\n",
    "print()\n",
    "print(\"====== FRAGMENTS  INFO ======\")\n",
    "print(\"array uri: {}\".format(fragments_info.array_uri))\n",
    "print(\"number of fragments: {}\".format(len(fragments_info)))\n",
    "\n",
    "for fragment_num, fragment in enumerate(fragments_info, start=1):\n",
    "    print()\n",
    "    print(\"===== FRAGMENT NUMBER {} =====\".format(fragment.num))\n",
    "    print(\"fragment uri: {}\".format(fragment.uri))\n",
    "    print(\"timestamp range: {}\".format(fragment.timestamp_range))\n",
    "    print(\n",
    "        \"number of unconsolidated metadata: {}\".format(\n",
    "            fragment.unconsolidated_metadata_num\n",
    "        )\n",
    "    )\n",
    "    print(\"version: {}\".format(fragment.version))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, a very interesting and useful, for machine learning models, TileDB feature that is described\n",
    "[here](https://docs.tiledb.com/main/concepts/data-format#groups) and [here](https://docs.tiledb.com/main/how-to/object-management#creating-tiledb-groups)\n",
    "are groups. Assuming we want to solve the MNIST problem, and we want to try several architectures. We can save each architecture\n",
    "as a separate TileDB array with native versioning each time it is re-trained, and then organise all models that solve the same problem (MNIST)\n",
    "as a TileDB array group with any kind of hierarchy. Let's firstly define a new model architecture."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "class OtherNet(nn.Module):\n",
    "    # For the sake of simplicity we just tweak the initial architecture by replacing a relu with relu6.\n",
    "    def __init__(self):\n",
    "        super(OtherNet, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 10, kernel_size=5)\n",
    "        self.conv2 = nn.Conv2d(10, 20, kernel_size=5)\n",
    "        self.conv2_drop = nn.Dropout2d()\n",
    "        self.fc1 = nn.Linear(320, 50)\n",
    "        self.fc2 = nn.Linear(50, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(F.max_pool2d(self.conv1(x), 2))\n",
    "        x = F.relu(F.max_pool2d(self.conv2_drop(self.conv2(x)), 2))\n",
    "        x = x.view(-1, 320)\n",
    "        x = F.relu6(self.fc1(x))\n",
    "        x = F.dropout(x, training=self.training)\n",
    "        x = self.fc2(x)\n",
    "        return F.log_softmax(x, dim = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then train it and save it as a new TileDB array."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1 [0/60000 (0%)]\tLoss: 2.313390\n",
      "Train Epoch: 1 [1280/60000 (2%)]\tLoss: 1.527101\n",
      "Train Epoch: 1 [2560/60000 (4%)]\tLoss: 0.971071\n",
      "Train Epoch: 1 [3840/60000 (6%)]\tLoss: 0.857200\n",
      "Train Epoch: 1 [5120/60000 (9%)]\tLoss: 0.513735\n",
      "Train Epoch: 1 [6400/60000 (11%)]\tLoss: 0.778414\n",
      "Train Epoch: 1 [7680/60000 (13%)]\tLoss: 0.419274\n",
      "Train Epoch: 1 [8960/60000 (15%)]\tLoss: 0.507868\n",
      "Train Epoch: 1 [10240/60000 (17%)]\tLoss: 0.543029\n",
      "Train Epoch: 1 [11520/60000 (19%)]\tLoss: 0.414866\n",
      "Train Epoch: 1 [12800/60000 (21%)]\tLoss: 0.563519\n",
      "Train Epoch: 1 [14080/60000 (23%)]\tLoss: 0.561157\n",
      "Train Epoch: 1 [15360/60000 (26%)]\tLoss: 0.394708\n",
      "Train Epoch: 1 [16640/60000 (28%)]\tLoss: 0.480180\n",
      "Train Epoch: 1 [17920/60000 (30%)]\tLoss: 0.512548\n",
      "Train Epoch: 1 [19200/60000 (32%)]\tLoss: 0.423522\n",
      "Train Epoch: 1 [20480/60000 (34%)]\tLoss: 0.407304\n",
      "Train Epoch: 1 [21760/60000 (36%)]\tLoss: 0.511762\n",
      "Train Epoch: 1 [23040/60000 (38%)]\tLoss: 0.476604\n",
      "Train Epoch: 1 [24320/60000 (41%)]\tLoss: 0.387796\n",
      "Train Epoch: 1 [25600/60000 (43%)]\tLoss: 0.367321\n",
      "Train Epoch: 1 [26880/60000 (45%)]\tLoss: 0.329760\n",
      "Train Epoch: 1 [28160/60000 (47%)]\tLoss: 0.467560\n",
      "Train Epoch: 1 [29440/60000 (49%)]\tLoss: 0.325833\n",
      "Train Epoch: 1 [30720/60000 (51%)]\tLoss: 0.489378\n",
      "Train Epoch: 1 [32000/60000 (53%)]\tLoss: 0.351727\n",
      "Train Epoch: 1 [33280/60000 (55%)]\tLoss: 0.491555\n",
      "Train Epoch: 1 [34560/60000 (58%)]\tLoss: 0.377189\n",
      "Train Epoch: 1 [35840/60000 (60%)]\tLoss: 0.389670\n",
      "Train Epoch: 1 [37120/60000 (62%)]\tLoss: 0.329024\n",
      "Train Epoch: 1 [38400/60000 (64%)]\tLoss: 0.387832\n",
      "Train Epoch: 1 [39680/60000 (66%)]\tLoss: 0.249163\n",
      "Train Epoch: 1 [40960/60000 (68%)]\tLoss: 0.277498\n",
      "Train Epoch: 1 [42240/60000 (70%)]\tLoss: 0.335240\n",
      "Train Epoch: 1 [43520/60000 (72%)]\tLoss: 0.339373\n",
      "Train Epoch: 1 [44800/60000 (75%)]\tLoss: 0.336837\n",
      "Train Epoch: 1 [46080/60000 (77%)]\tLoss: 0.337218\n",
      "Train Epoch: 1 [47360/60000 (79%)]\tLoss: 0.313214\n",
      "Train Epoch: 1 [48640/60000 (81%)]\tLoss: 0.483387\n",
      "Train Epoch: 1 [49920/60000 (83%)]\tLoss: 0.406790\n",
      "Train Epoch: 1 [51200/60000 (85%)]\tLoss: 0.373719\n",
      "Train Epoch: 1 [52480/60000 (87%)]\tLoss: 0.334809\n",
      "Train Epoch: 1 [53760/60000 (90%)]\tLoss: 0.551137\n",
      "Train Epoch: 1 [55040/60000 (92%)]\tLoss: 0.381777\n",
      "Train Epoch: 1 [56320/60000 (94%)]\tLoss: 0.528861\n",
      "Train Epoch: 1 [57600/60000 (96%)]\tLoss: 0.294103\n",
      "Train Epoch: 1 [58880/60000 (98%)]\tLoss: 0.288652\n"
     ]
    }
   ],
   "source": [
    "network = OtherNet()\n",
    "optimizer = optim.Adam(network.parameters(), lr=learning_rate)\n",
    "\n",
    "train_losses = []\n",
    "train_counter = []\n",
    "\n",
    "for epoch in range(1, epochs + 1):\n",
    "    train(epoch)\n",
    "\n",
    "uri2 = os.path.join(data_home, 'pytorch-mnist-2')\n",
    "tiledb_model_2 = PyTorchTileDBModel(uri=uri2, model=network, optimizer=optimizer)\n",
    "\n",
    "tiledb_model_2.save(update=False, \n",
    "                    meta={'epochs': epochs,\n",
    "                          'train_loss': train_losses})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can create a TileDB group and organise (in hierarchies, e.g., sophisticated vs less sophisticated) all our\n",
    "MNIST models as follows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'../data/tiledb-pytorch-mnist/pytorch-mnist-2'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "group = os.path.join(data_home, 'tiledb-pytorch-mnist')\n",
    "tiledb.group_create(group)\n",
    "shutil.move(uri, group)\n",
    "shutil.move(uri2, group)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Any time we can check and query all the available models, including their metadata, for a specific problem like MNIST."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "file:///home/gsk/projects/TileDB-ML/examples/data/tiledb-pytorch-mnist/pytorch-mnist-1 array\n",
      "file:///home/gsk/projects/TileDB-ML/examples/data/tiledb-pytorch-mnist/pytorch-mnist-2 array\n"
     ]
    }
   ],
   "source": [
    "tiledb.ls(group, lambda obj_path, obj_type: print(obj_path, obj_type))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
