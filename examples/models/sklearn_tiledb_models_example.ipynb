{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "This example notebook shows how we can train an [image/digit classification](https://scikit-learn.org/stable/auto_examples/linear_model/plot_sparse_logistic_regression_mnist.html)\n",
    "model based on MNIST dataset, and store it as TileDB array. Firstly, let's import what we need."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import glob\n",
    "import json\n",
    "import os\n",
    "import shutil\n",
    "from pprint import pprint\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import tiledb\n",
    "from sklearn.datasets import fetch_openml\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.utils import check_random_state\n",
    "from sklearn import tree\n",
    "\n",
    "from tiledb.ml.models.sklearn import SklearnTileDBModel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Then load our data, split in train and test and perform basic scaling by employing a standard scaler."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data fetching...\n"
     ]
    }
   ],
   "source": [
    "data_home = os.path.join(os.path.pardir, \"data\")\n",
    "train_samples = 5000\n",
    "\n",
    "# Load data from https://www.openml.org/d/554\n",
    "print('Data fetching...')\n",
    "X, y = fetch_openml('mnist_784', version=1, return_X_y=True, as_frame=False, data_home=data_home)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data scaling...\n"
     ]
    }
   ],
   "source": [
    "random_state = check_random_state(0)\n",
    "permutation = random_state.permutation(X.shape[0])\n",
    "X = X[permutation]\n",
    "y = y[permutation]\n",
    "X = X.reshape((X.shape[0], -1))\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, train_size=train_samples, test_size=10000)\n",
    "\n",
    "print('Data scaling...')\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "We move on by declaring a simple Logistic Regression classifier, train it and print the accuracy score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model fit...\n",
      "Model score...\n",
      "Sparsity with L1 penalty: 81.12%\n",
      "Test score with L1 penalty: 0.8317\n"
     ]
    }
   ],
   "source": [
    "clf = LogisticRegression(\n",
    "    C=50. / train_samples, penalty='l1', solver='saga', tol=0.1\n",
    ")\n",
    "\n",
    "print('Model fit...')\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "print('Model score...')\n",
    "sparsity = np.mean(clf.coef_ == 0) * 100\n",
    "score = clf.score(X_test, y_test)\n",
    "\n",
    "print(\"Sparsity with L1 penalty: %.2f%%\" % sparsity)\n",
    "print(\"Test score with L1 penalty: %.4f\" % score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "We can now save the trained model as a TileDB array. For information about the structure of a dense\n",
    "TileDB array in terms of files on disk please take a look [here](https://docs.tiledb.com/main/concepts/data-format).\n",
    "At the moment (will change in the future) we use pickle, which is one of the [most common scenarios for sklearn models](https://scikit-learn.org/stable/modules/model_persistence.html),\n",
    "to serialize the whole model and save it as a [variable sized attribute](https://docs.tiledb.com/main/how-to/arrays/writing-arrays/var-length-attributes)\n",
    "in a TileDB array.  We first declare a SklearnTileDBModel object (with the corresponding uri and model attributes) and then save the model as a TileDB array.\n",
    "Finally, we can save any kind of metadata (in any structure, i.e., list, tuple or dictionary) by passing a dictionary to the meta attribute."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "uri = os.path.join(data_home, 'sklearn-mnist-1')\n",
    "tiledb_model_1 = SklearnTileDBModel(uri=uri, model=clf)\n",
    "\n",
    "tiledb_model_1.save(meta={'Sparsity_with_L1_penalty': sparsity,\n",
    "                          'score': score})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above step will create a TileDB array in your working directory. Let's open our TileDB array model and check metadata.\n",
    "Metadata that are of type list, dict or tuple have been JSON\n",
    "serialized while saving, i.e., we need json.loads to deserialize them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['../data/sklearn-mnist-1/__meta',\n",
      " '../data/sklearn-mnist-1/__fragment_meta',\n",
      " '../data/sklearn-mnist-1/__commits',\n",
      " '../data/sklearn-mnist-1/__schema',\n",
      " '../data/sklearn-mnist-1/__fragments']\n",
      "\n",
      "Key: Sparsity_with_L1_penalty, Value: 81.12244897959184\n",
      "Key: TILEDB_ML_MODEL_ML_FRAMEWORK, Value: SKLEARN\n",
      "Key: TILEDB_ML_MODEL_ML_FRAMEWORK_VERSION, Value: 1.1.3\n",
      "Key: TILEDB_ML_MODEL_PREVIEW, Value: LogisticRegression(C=0.01, penalty='l1', solver='saga', tol=0.1)\n",
      "Key: TILEDB_ML_MODEL_PYTHON_VERSION, Value: 3.9.9\n",
      "Key: TILEDB_ML_MODEL_STAGE, Value: STAGING\n",
      "Key: model_size, Value: 63531\n",
      "Key: score, Value: 0.8317\n"
     ]
    }
   ],
   "source": [
    "# Check array directory\n",
    "pprint(glob.glob(f'{uri}/*'))\n",
    "\n",
    "# Open in write mode in order to add metadata\n",
    "print()\n",
    "model_array_1 = tiledb.open(uri)\n",
    "for key, value in model_array_1.meta.items():\n",
    "    if isinstance(value, bytes):\n",
    "        value = json.loads(value)\n",
    "    print(\"Key: {}, Value: {}\".format(key, value))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, in array's metadata we have by default information about the backend we used for training (sklearn),\n",
    "sklearn version, python version and the extra metadata about epochs and training loss that we added.\n",
    "We can load and check any of the aforementioned without having to load the entire model in memory.\n",
    "Moreover, we can add any kind of extra information in model's metadata also by opening the TileDB array and adding new keys."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Key: Sparsity_with_L1_penalty, Value: 81.12244897959184\n",
      "Key: TILEDB_ML_MODEL_ML_FRAMEWORK, Value: SKLEARN\n",
      "Key: TILEDB_ML_MODEL_ML_FRAMEWORK_VERSION, Value: 1.1.3\n",
      "Key: TILEDB_ML_MODEL_PREVIEW, Value: LogisticRegression(C=0.01, penalty='l1', solver='saga', tol=0.1)\n",
      "Key: TILEDB_ML_MODEL_PYTHON_VERSION, Value: 3.9.9\n",
      "Key: TILEDB_ML_MODEL_STAGE, Value: STAGING\n",
      "Key: model_size, Value: 63531\n",
      "Key: new_meta, Value: [\"Any kind of info\"]\n",
      "Key: score, Value: 0.8317\n"
     ]
    }
   ],
   "source": [
    "# Open the array in write mode\n",
    "with tiledb.Array(uri, \"w\") as A:\n",
    "    # Keep all history\n",
    "    A.meta['new_meta'] = json.dumps(['Any kind of info'])\n",
    "\n",
    "# Check that everything is there\n",
    "model_array_1 = tiledb.open(uri)\n",
    "for key, value in model_array_1.meta.items():\n",
    "    if isinstance(value, bytes):\n",
    "        value = json.loads(value)\n",
    "    print(\"Key: {}, Value: {}\".format(key, value))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Moving on, we can load the trained models for evaluation or retraining, as usual with Sklearn models. What is really nice with saving models as TileDB array, is native versioning based on fragments as described [here](https://docs.tiledb.com/main/concepts/data-format#immutable-fragments). We can load a model, retrain it with new data and update the already existing TileDB model array with new model parameters and metadata. All information, old and new will be there and accessible. This is extremely useful when you retrain with new data or trying different architectures for the same problem, and you want to keep track of all your experiments without having to store different model instances. In our case, let's continue training `sklearn-mnist-1` with test data (just for simplicity). After training is done, we can save the model again with `update=True`. You will notice the extra directories and files (fragments) added to `sklearn-mnist-1` TileDB array directory, which keep all versions of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model score...\n",
      "Sparsity with L1 penalty: 81.12%\n",
      "Test score with L1 penalty: 0.8317\n",
      "Model fit...\n",
      "Model score...\n",
      "Sparsity with L1 penalty: 45.46%\n",
      "Test score with L1 penalty: 0.7301\n",
      "\n",
      "['../data/sklearn-mnist-1/__meta',\n",
      " '../data/sklearn-mnist-1/__fragment_meta',\n",
      " '../data/sklearn-mnist-1/__commits',\n",
      " '../data/sklearn-mnist-1/__schema',\n",
      " '../data/sklearn-mnist-1/__fragments']\n",
      "\n",
      "====== FRAGMENTS  INFO ======\n",
      "array uri: ../data/sklearn-mnist-1\n",
      "number of fragments: 2\n",
      "\n",
      "===== FRAGMENT NUMBER 0 =====\n",
      "timestamp range: (1670425614775, 1670425614775)\n",
      "number of unconsolidated metadata: 2\n",
      "version: 16\n",
      "\n",
      "===== FRAGMENT NUMBER 1 =====\n",
      "timestamp range: (1670425618384, 1670425618384)\n",
      "number of unconsolidated metadata: 2\n",
      "version: 16\n"
     ]
    }
   ],
   "source": [
    "loaded_clf = tiledb_model_1.load()\n",
    "\n",
    "# Sparsity and score should be the same as in the previous step.\n",
    "print('Model score...')\n",
    "sparsity = np.mean(loaded_clf.coef_ == 0) * 100\n",
    "score = loaded_clf.score(X_test, y_test)\n",
    "\n",
    "print(\"Sparsity with L1 penalty: %.2f%%\" % sparsity)\n",
    "print(\"Test score with L1 penalty: %.4f\" % score)\n",
    "\n",
    "\n",
    "# We retrain with test data just for the sake of simplicity.\n",
    "print('Model fit...')\n",
    "loaded_clf.fit(X_test, y_test)\n",
    "\n",
    "print('Model score...')\n",
    "sparsity = np.mean(loaded_clf.coef_ == 0) * 100\n",
    "score = loaded_clf.score(X_test, y_test)\n",
    "\n",
    "print(\"Sparsity with L1 penalty: %.2f%%\" % sparsity)\n",
    "print(\"Test score with L1 penalty: %.4f\" % score)\n",
    "\n",
    "\n",
    "tiledb_model_1 = SklearnTileDBModel(uri=uri, model=loaded_clf)\n",
    "tiledb_model_1.save(update=True,\n",
    "                    meta={'Sparsity_with_L1_penalty': sparsity,\n",
    "                          'score': score})\n",
    "\n",
    "# Check array directory\n",
    "print()\n",
    "pprint(glob.glob(f'{uri}/*'))\n",
    "\n",
    "\n",
    "# tiledb.array_fragments() requires TileDB-Py version > 0.8.5\n",
    "fragments_info = tiledb.array_fragments(uri)\n",
    "\n",
    "print()\n",
    "print(\"====== FRAGMENTS  INFO ======\")\n",
    "print(\"array uri: {}\".format(fragments_info.array_uri))\n",
    "print(\"number of fragments: {}\".format(len(fragments_info)))\n",
    "\n",
    "for fragment_num, fragment in enumerate(fragments_info, start=1):\n",
    "    print()\n",
    "    print(\"===== FRAGMENT NUMBER {} =====\".format(fragment.num))\n",
    "    print(\"timestamp range: {}\".format(fragment.timestamp_range))\n",
    "    print(\n",
    "        \"number of unconsolidated metadata: {}\".format(\n",
    "            fragment.unconsolidated_metadata_num\n",
    "        )\n",
    "    )\n",
    "    print(\"version: {}\".format(fragment.version))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, a very interesting and useful, for machine learning models, TileDB feature that is described\n",
    "[here](https://docs.tiledb.com/main/concepts/data-format#groups) and [here](https://docs.tiledb.com/main/how-to/object-management#creating-tiledb-groups)\n",
    "are groups. Assuming we want to solve the MNIST problem, and we want to try several architectures. We can save each architecture\n",
    "as a separate TileDB array with native versioning each time it is re-trained, and then organise all models that solve the same problem (MNIST)\n",
    "as a TileDB array group with any kind of hierarchy. Let's firstly define a new model architecture, then train a model and save\n",
    "it as a new TileDB array."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fit...\n",
      "Test score: 0.7654\n"
     ]
    }
   ],
   "source": [
    "# We declare a Decision Tree classifier\n",
    "clf = tree.DecisionTreeClassifier()\n",
    "\n",
    "# Fit the model\n",
    "print('Fit...')\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate\n",
    "score = clf.score(X_test, y_test)\n",
    "print(\"Test score: %.4f\" % score)\n",
    "\n",
    "# Declare a SklearnTileDBModel object\n",
    "uri2 = os.path.join(data_home, 'sklearn-mnist-2')\n",
    "tiledb_model_2 = SklearnTileDBModel(uri=uri2, model=clf)\n",
    "\n",
    "# Save model as a TileDB array\n",
    "tiledb_model_2.save(meta={'score': score})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can create a TileDB group and organise (in hierarchies, e.g., sophisticated vs less sophisticated) all our\n",
    "MNIST models as follows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "'../data/tiledb-sklearn-mnist/sklearn-mnist-2'"
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "group = os.path.join(data_home, 'tiledb-sklearn-mnist')\n",
    "tiledb.group_create(group)\n",
    "shutil.move(uri, group)\n",
    "shutil.move(uri2, group)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Any time we can check and query all the available models, including their metadata, for a specific problem like MNIST."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "file:///Users/george/PycharmProjects/TileDB-ML/examples/data/tiledb-sklearn-mnist/sklearn-mnist-1 array\n",
      "file:///Users/george/PycharmProjects/TileDB-ML/examples/data/tiledb-sklearn-mnist/sklearn-mnist-2 array\n"
     ]
    }
   ],
   "source": [
    "tiledb.ls(group, lambda obj_path, obj_type: print(obj_path, obj_type))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
