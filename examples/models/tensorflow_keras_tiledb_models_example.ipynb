{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This example notebook shows how we can train an image classification model, as described [here](https://github.com/tensorflow/docs/blob/master/site/en/tutorials/quickstart/beginner.ipynb),\n",
    "and store it as TileDB array. Firstly, let's import what we need."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import glob\n",
    "import json\n",
    "import os\n",
    "import pickle\n",
    "import shutil\n",
    "from pprint import pprint\n",
    "\n",
    "import tensorflow as tf\n",
    "import tiledb\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from tiledb.ml.models.tensorflow_keras import TensorflowKerasTileDBModel\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load MNIST dataset for Keras datasets and scale."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "(x_train, y_train), (x_test, y_test) = tf.keras.datasets.mnist.load_data()\n",
    "x_train, x_test = x_train / 255.0, x_test / 255.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can then define a function that creates a basic digit classifier for the MNIST dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def create_model():\n",
    "    model = tf.keras.models.Sequential([\n",
    "        tf.keras.layers.Flatten(input_shape=(28, 28)),\n",
    "        tf.keras.layers.Dense(128, activation='relu'),\n",
    "        tf.keras.layers.Dropout(0.2),\n",
    "        tf.keras.layers.Dense(10)\n",
    "    ])\n",
    "\n",
    "    loss_fn = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    "\n",
    "    model.compile(optimizer='adam',\n",
    "                  loss=loss_fn,\n",
    "                  metrics=['accuracy'])\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can then train a model using some of our data. Let's assume that we initially train with the first 30000\n",
    "observations from our dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-08-22 12:59:10.788587: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-08-22 12:59:10.833063: I tensorflow/core/profiler/lib/profiler_session.cc:131] Profiler session initializing.\n",
      "2024-08-22 12:59:10.833094: I tensorflow/core/profiler/lib/profiler_session.cc:146] Profiler session started.\n",
      "2024-08-22 12:59:10.833256: I tensorflow/core/profiler/lib/profiler_session.cc:164] Profiler session tear down.\n",
      "2024-08-22 12:59:11.107514: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:185] None of the MLIR Optimization Passes are enabled (registered 2)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      " 77/938 [=>............................] - ETA: 2s - loss: 1.0641 - accuracy: 0.6981"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-08-22 12:59:11.703057: I tensorflow/core/profiler/lib/profiler_session.cc:131] Profiler session initializing.\n",
      "2024-08-22 12:59:11.703101: I tensorflow/core/profiler/lib/profiler_session.cc:146] Profiler session started.\n",
      "2024-08-22 12:59:11.710090: I tensorflow/core/profiler/lib/profiler_session.cc:66] Profiler session collecting data.\n",
      "2024-08-22 12:59:11.712768: I tensorflow/core/profiler/lib/profiler_session.cc:164] Profiler session tear down.\n",
      "2024-08-22 12:59:11.729544: I tensorflow/core/profiler/rpc/client/save_profile.cc:136] Creating directory: ../data/logs/train/plugins/profile/2024_08_22_12_59_11\n",
      "\n",
      "2024-08-22 12:59:11.731462: I tensorflow/core/profiler/rpc/client/save_profile.cc:142] Dumped gzipped tool data for trace.json.gz to ../data/logs/train/plugins/profile/2024_08_22_12_59_11/ktsitsi.trace.json.gz\n",
      "2024-08-22 12:59:11.742256: I tensorflow/core/profiler/rpc/client/save_profile.cc:136] Creating directory: ../data/logs/train/plugins/profile/2024_08_22_12_59_11\n",
      "\n",
      "2024-08-22 12:59:11.742923: I tensorflow/core/profiler/rpc/client/save_profile.cc:142] Dumped gzipped tool data for memory_profile.json.gz to ../data/logs/train/plugins/profile/2024_08_22_12_59_11/ktsitsi.memory_profile.json.gz\n",
      "2024-08-22 12:59:11.744557: I tensorflow/core/profiler/rpc/client/capture_profile.cc:251] Creating directory: ../data/logs/train/plugins/profile/2024_08_22_12_59_11\n",
      "Dumped tool data for xplane.pb to ../data/logs/train/plugins/profile/2024_08_22_12_59_11/ktsitsi.xplane.pb\n",
      "Dumped tool data for overview_page.pb to ../data/logs/train/plugins/profile/2024_08_22_12_59_11/ktsitsi.overview_page.pb\n",
      "Dumped tool data for input_pipeline.pb to ../data/logs/train/plugins/profile/2024_08_22_12_59_11/ktsitsi.input_pipeline.pb\n",
      "Dumped tool data for tensorflow_stats.pb to ../data/logs/train/plugins/profile/2024_08_22_12_59_11/ktsitsi.tensorflow_stats.pb\n",
      "Dumped tool data for kernel_stats.pb to ../data/logs/train/plugins/profile/2024_08_22_12_59_11/ktsitsi.kernel_stats.pb\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "938/938 [==============================] - 3s 2ms/step - loss: 0.3811 - accuracy: 0.8888\n"
     ]
    },
    {
     "data": {
      "text/plain": "<keras.callbacks.History at 0x165542460>"
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_home = os.path.join(os.path.pardir, \"data\")\n",
    "model = create_model()\n",
    "cb_tb = tf.keras.callbacks.TensorBoard(log_dir=os.path.join(data_home, 'logs'))\n",
    "cb_early = callback = tf.keras.callbacks.EarlyStopping(monitor='loss', min_delta=0.01)\n",
    "\n",
    "model.fit(x_train[:30000], y_train[:30000], epochs=5, callbacks=[cb_tb,cb_early])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now save the trained model as a TileDB array. In case we want to train  the model further in a later time, we can save\n",
    "optimizer's information in our TileDB array. In case we will use our model only for inference, we don't have to save optimizer's\n",
    "information and we only keep model's weights. We first declare a TileDB-Keras model object (with the corresponding uri and model attributes)\n",
    "and then save the model as a TileDB array."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "save() got an unexpected keyword argument 'callbacks'",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mTypeError\u001B[0m                                 Traceback (most recent call last)",
      "\u001B[0;32m/var/folders/m9/58b7bc3128xfv4mmg8k45xgw0000gn/T/ipykernel_56886/772079740.py\u001B[0m in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[1;32m      2\u001B[0m \u001B[0mtiledb_model_1\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mTensorflowKerasTileDBModel\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0muri\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0muri\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mmodel\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mmodel\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m      3\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m----> 4\u001B[0;31m \u001B[0mtiledb_model_1\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0msave\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0minclude_optimizer\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0;32mTrue\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mcallbacks\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0mcb_tb\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m      5\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m      6\u001B[0m \u001B[0mfragments_info\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mtiledb\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0marray_fragments\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0muri\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;31mTypeError\u001B[0m: save() got an unexpected keyword argument 'callbacks'"
     ]
    }
   ],
   "source": [
    "uri = os.path.join(data_home, 'keras-mnist-sequential-1')\n",
    "tiledb_model_1 = TensorflowKerasTileDBModel(uri=uri, model=model)\n",
    "\n",
    "tiledb_model_1.save(include_optimizer=True, callbacks=[cb_tb])\n",
    "\n",
    "fragments_info = tiledb.array_fragments(uri)\n",
    "print(\"number of fragments: {}\".format(len(fragments_info)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above step will create a TileDB array in your working directory. For information about the structure of a dense\n",
    "TileDB array in terms of files on disk please take a look [here](https://docs.tiledb.com/main/concepts/data-format).\n",
    "Let's open our TileDB array model and check metadata. Metadata that are of type list, dict or tuple have been JSON\n",
    "serialized while saving, i.e., we need json.loads to deserialize them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Check array directory\n",
    "pprint(glob.glob(f'{uri}/*'))\n",
    "\n",
    "# Open in write mode in order to add metadata\n",
    "model_array_1 = tiledb.open(uri)\n",
    "for key, value in model_array_1.meta.items():\n",
    "    print(\"Key: {}, Value: {}\".format(key, value))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, in array's metadata we have by default information about the backend we used for training, keras version,\n",
    "python version, model configuration and training configuration. We can load and check any of the aforementioned without\n",
    "having to load the entire model in memory. Moreover, we can add any kind of extra information about model accuracy, model\n",
    "version, deployment status etc, in the model's metadata either while saving the model, by passing a dictionary with any\n",
    "kind of information, or by opening the TileDB array and adding new keys. Both cases are shown below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Open the array in write mode\n",
    "with tiledb.Array(uri, \"w\") as A:\n",
    "    # Keep all history\n",
    "    A.meta['loss'] = json.dumps(model.history.history['loss'])\n",
    "    A.meta['accuracy'] = json.dumps(model.history.history['accuracy'])\n",
    "\n",
    "    # Or keep last epoch's loss and accuracy\n",
    "    A.meta['last_epoch_loss'] = json.dumps(model.history.history['loss'][-1])\n",
    "    A.meta['last_epoch_accuracy'] = json.dumps(model.history.history['accuracy'][-1])\n",
    "\n",
    "# Check that everything is there\n",
    "model_array_1 = tiledb.open(uri)\n",
    "for key, value in model_array_1.meta.items():\n",
    "    if isinstance(value, bytes):\n",
    "        value = json.loads(value)\n",
    "    print(\"Key: {}, Value: {}\".format(key, value))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also save any kind of metadata while saving the model as a TileDB array, and avoid opening it multiple times."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "model = create_model()\n",
    "model.fit(x_train[:30000], y_train[:30000], epochs=5)\n",
    "\n",
    "uri2 = os.path.join(data_home, 'keras-mnist-sequential-2')\n",
    "tiledb_model_2 = TensorflowKerasTileDBModel(uri=uri2, model=model)\n",
    "\n",
    "tiledb_model_2.save(include_optimizer=True,\n",
    "                    meta={\"accuracy\": model.history.history['accuracy'],\n",
    "                          \"loss\": model.history.history['loss'],\n",
    "                          \"status\": 'experimental'})\n",
    "\n",
    "# Check array directory\n",
    "print()\n",
    "pprint(glob.glob(f'{uri2}/*'))\n",
    "\n",
    "# Check that everything is there\n",
    "print()\n",
    "model_array_2 = tiledb.open(uri2)\n",
    "for key, value in model_array_2.meta.items():\n",
    "    if isinstance(value, bytes):\n",
    "        value = json.loads(value)\n",
    "    print(\"Key: {}, Value: {}\".format(key, value))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "For the case of Tensorflow Keras models, apart from model configuration (architecture) which is saved in the metadata\n",
    "section of the TileDB array, we save model's weights and optimizer's weights, as variable sized attributes (pickled),\n",
    "i.e., we can open the TileDB and get only the weights of a model or model's optimizer without bringing the whole model in\n",
    "memory. For example, we can load model's and optimizer's weights for models `keras-mnist-sequential-1` and\n",
    "`keras-mnist-sequential-2` as follows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Load model weights\n",
    "model_1_weights = tiledb_model_1.get_weights()\n",
    "model_2_weights = tiledb_model_2.get_weights()\n",
    "\n",
    "# Load optimizer weights\n",
    "optimizer_1_weights = tiledb_model_1.get_optimizer_weights()\n",
    "optimizer_2_weights = tiledb_model_2.get_optimizer_weights()\n",
    "\n",
    "plt.rcParams[\"figure.figsize\"] = (8,10)\n",
    "\n",
    "# Maybe plot a part of layer 1 weights for both NNs\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2)\n",
    "fig.suptitle('1st 50x50 weights of layer 1 for NN1 and NN2', size=16, y=0.72)\n",
    "ax1.matshow(model_1_weights[0][:50, :50])\n",
    "ax2.matshow(model_2_weights[0][:50, :50])\n",
    "fig.tight_layout()\n",
    "fig.subplots_adjust(top=0.95)\n",
    "plt.show()\n",
    "\n",
    "# Maybe plot a part of optimizer weights for both NNs\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2)\n",
    "fig.suptitle('1st 50x50 optimizer weights for NN1 and NN2', size=16, y=0.72)\n",
    "ax1.matshow(optimizer_1_weights[2][:50, :50])\n",
    "ax2.matshow(optimizer_2_weights[2][:50, :50])\n",
    "fig.tight_layout()\n",
    "fig.subplots_adjust(top=0.95)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Moving on, we can load the trained models for prediction or evaluation (we have to compile the model), as usual with\n",
    "Tensorflow Keras models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "loaded_model_1 = tiledb_model_1.load()\n",
    "loaded_model_2 = tiledb_model_2.load()\n",
    "\n",
    "# Make some predictions\n",
    "pred_1 = loaded_model_1.predict(x_test)\n",
    "pred_2 = loaded_model_2.predict(x_test)\n",
    "print(type(pred_1), pred_1.shape)\n",
    "print(type(pred_2), pred_2.shape)\n",
    "\n",
    "# Evaluate models\n",
    "loaded_model_1 = tiledb_model_1.load(compile_model=True)\n",
    "loaded_model_2 = tiledb_model_2.load(compile_model=True)\n",
    "loaded_model_1.evaluate(x_test, y_test)\n",
    "loaded_model_2.evaluate(x_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What is really nice with saving models as TileDB array, is native versioning based on fragments as described\n",
    "[here](https://docs.tiledb.com/main/concepts/data-format#immutable-fragments). We can load a model, retrain it\n",
    "with new data and update the already existing TileDB model array with new model parameters and metadata. All information, old\n",
    "and new will be there and accessible. This is extremely useful when you retrain with new data or trying different architectures\n",
    "for the same problem, and you want to keep track of all your experiments without having to store different model instances. In our case,\n",
    "let's continue training model_1 with the rest of our dataset and for 5 more epochs. After training is done, you will\n",
    "notice the extra directories and files (fragments) added to `keras-mnist-sequential-1` TileDB array directory,\n",
    "which keep all versions of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "loaded_model_1 = tiledb_model_1.load(compile_model=True)\n",
    "loaded_model_1.fit(x_train[30000:], y_train[30000:], epochs=5)\n",
    "\n",
    "# and update\n",
    "tiledb_model_1 = TensorflowKerasTileDBModel(uri=uri, model=loaded_model_1)\n",
    "tiledb_model_1.save(include_optimizer=True,\n",
    "                    meta={\"accuracy\": model.history.history['accuracy'],\n",
    "                          \"loss\": model.history.history['loss'],\n",
    "                          \"version\": '0.0.1',\n",
    "                          \"status\": 'experimental'})\n",
    "\n",
    "# Check array directory\n",
    "print()\n",
    "pprint(glob.glob(f'{uri}/*'))\n",
    "\n",
    "# tiledb.array_fragments() requires TileDB-Py version > 0.8.5\n",
    "fragments_info = tiledb.array_fragments(uri)\n",
    "\n",
    "print()\n",
    "print(\"====== FRAGMENTS  INFO ======\")\n",
    "print(\"array uri: {}\".format(fragments_info.array_uri))\n",
    "print(\"number of fragments: {}\".format(len(fragments_info)))\n",
    "\n",
    "for fragment_num, fragment in enumerate(fragments_info, start=1):\n",
    "    print()\n",
    "    print(\"===== FRAGMENT NUMBER {} =====\".format(fragment.num))\n",
    "    print(\"fragment uri: {}\".format(fragment.uri))\n",
    "    print(\"timestamp range: {}\".format(fragment.timestamp_range))\n",
    "    print(\n",
    "        \"number of unconsolidated metadata: {}\".format(\n",
    "            fragment.unconsolidated_metadata_num\n",
    "        )\n",
    "    )\n",
    "    print(\"version: {}\".format(fragment.version))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, a very interesting and useful, for machine learning models, TileDB feature that is described\n",
    "[here](https://docs.tiledb.com/main/concepts/data-format#groups) and [here](https://docs.tiledb.com/main/how-to/object-management#creating-tiledb-groups)\n",
    "are groups. Assuming we want to solve the MNIST problem, and we want to try several architectures. We can save each architecture\n",
    "as a separate TileDB array with native versioning each time it is re-trained, and then organise all models that solve the same problem (MNIST)\n",
    "as a TileDB array group with any kind of hierarchy. Let's firstly define a new model architecture."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def create_deeper_model():\n",
    "    # For the sake of simplicity we just add an extra dense layer to the previous architecture.\n",
    "    model = tf.keras.models.Sequential([\n",
    "        tf.keras.layers.Flatten(input_shape=(28, 28)),\n",
    "        tf.keras.layers.Dense(256, activation='relu'),\n",
    "        tf.keras.layers.Dense(128, activation='relu'),\n",
    "        tf.keras.layers.Dropout(0.2),\n",
    "        tf.keras.layers.Dense(10)\n",
    "    ])\n",
    "\n",
    "    loss_fn = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    "\n",
    "    model.compile(optimizer='adam',\n",
    "                  loss=loss_fn,\n",
    "                  metrics=['accuracy'])\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Then train it and save it as a new TileDB array."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "model = create_deeper_model()\n",
    "model.fit(x_train, y_train, epochs=5)\n",
    "\n",
    "uri3 = os.path.join(data_home, 'keras-mnist-sequential-deeper')\n",
    "tiledb_deeper_model = TensorflowKerasTileDBModel(uri=uri3, model=model)\n",
    "\n",
    "tiledb_deeper_model.save(include_optimizer=True,\n",
    "                         meta={\"accuracy\": model.history.history['accuracy'],\n",
    "                               \"loss\": model.history.history['loss'],\n",
    "                               \"status\": 'experimental'})\n",
    "\n",
    "# Check array directory\n",
    "print()\n",
    "pprint(glob.glob(f'{uri3}/*'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Now we can create a TileDB group and organise (in hierarchies, e.g., sophisticated vs less sophisticated) all our\n",
    "MNIST models as follows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "group = os.path.join(data_home, 'tiledb-keras-mnist')\n",
    "tiledb.group_create(group)\n",
    "shutil.move(uri, group)\n",
    "shutil.move(uri2, group)\n",
    "shutil.move(uri3, group)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Any time we can check and query all the available models, including their metadata, for a specific problem like MNIST."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "tiledb.ls(group, lambda obj_path, obj_type: print(obj_path, obj_type))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}