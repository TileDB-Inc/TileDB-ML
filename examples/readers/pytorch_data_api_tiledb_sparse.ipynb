{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This example notebook shows how we can train a simple Regression classifier.\n",
    "We employ TileDB as a storage engine for our training data and labels.\n",
    "We will use the MovieLens 100K public data set, available [here](https://grouplens.org/datasets/movielens/100k/). We will first download the\n",
    "MovieLens, which contains 100.000 ratings, by 943 users on 1682 items.\n",
    "Continuing, we will use our TileDB support for PyTorch Sparse Dataloader API in order to train the classifier.\n",
    "First, let's import what we need and download our data. We will transform our data to a sparse format\n",
    "in order to show the support of TileDB in ingesting and providing to the Pytorch framework sparse datasets. Sparse\n",
    "datasets are important and frequently found and used in applications like recommender systems et.al. For example by designing\n",
    "a Factorisation Machine [FM model](https://www.csie.ntu.edu.tw/~b97053/paper/Rendle2010FM.pdf) someone could take advantage of data sparsity and build a refined recommendation system."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import urllib.request\n",
    "import numpy as np\n",
    "import tiledb\n",
    "import torch\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset\n",
    "Download MovieLens dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "('u.data', <http.client.HTTPMessage at 0x1440f2eb0>)"
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "url = 'https://files.grouplens.org/datasets/movielens/ml-100k/u.data'\n",
    "filename = 'u.data'\n",
    "urllib.request.urlretrieve(url, filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "Use pandas to display dataset in readable form"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "   user_id  item_id  rating\n0      196      242       3\n1      186      302       3\n2       22      377       1\n3      244       51       2\n4      166      346       1",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>user_id</th>\n      <th>item_id</th>\n      <th>rating</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>196</td>\n      <td>242</td>\n      <td>3</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>186</td>\n      <td>302</td>\n      <td>3</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>22</td>\n      <td>377</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>244</td>\n      <td>51</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>166</td>\n      <td>346</td>\n      <td>1</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "(100000, 3)"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "data = pd.read_csv(filename, sep=\"\\t\", header=None, engine='python')\n",
    "data.columns = [\"user_id\", \"item_id\", \"rating\", \"timestamp\"]\n",
    "data.drop([\"timestamp\"], axis=1, inplace=True)\n",
    "display(data.head())\n",
    "display(data.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Data Analysis / Sparsity\n",
    "Before we apply the one-hot transformation let’s check the memory usage of our original data frame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memory usage is 2.4 MB\n"
     ]
    }
   ],
   "source": [
    "BYTES_TO_MB_DIV = 0.000001\n",
    "def print_memory_usage_of_data_frame(df):\n",
    "    mem = round(df.memory_usage().sum() * BYTES_TO_MB_DIV, 3)\n",
    "    print(\"Memory usage is \" + str(mem) + \" MB\")\n",
    "\n",
    "print_memory_usage_of_data_frame(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Data transformation\n",
    "\n",
    "Now, let’s apply the transformation and check the memory usage of the transformed data frame."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "data": {
      "text/plain": "   rating  user_id_1  user_id_2  user_id_3  user_id_4  user_id_5  user_id_6  \\\n0       3          0          0          0          0          0          0   \n1       3          0          0          0          0          0          0   \n2       1          0          0          0          0          0          0   \n3       2          0          0          0          0          0          0   \n4       1          0          0          0          0          0          0   \n\n   user_id_7  user_id_8  user_id_9  ...  item_id_1673  item_id_1674  \\\n0          0          0          0  ...             0             0   \n1          0          0          0  ...             0             0   \n2          0          0          0  ...             0             0   \n3          0          0          0  ...             0             0   \n4          0          0          0  ...             0             0   \n\n   item_id_1675  item_id_1676  item_id_1677  item_id_1678  item_id_1679  \\\n0             0             0             0             0             0   \n1             0             0             0             0             0   \n2             0             0             0             0             0   \n3             0             0             0             0             0   \n4             0             0             0             0             0   \n\n   item_id_1680  item_id_1681  item_id_1682  \n0             0             0             0  \n1             0             0             0  \n2             0             0             0  \n3             0             0             0  \n4             0             0             0  \n\n[5 rows x 2626 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>rating</th>\n      <th>user_id_1</th>\n      <th>user_id_2</th>\n      <th>user_id_3</th>\n      <th>user_id_4</th>\n      <th>user_id_5</th>\n      <th>user_id_6</th>\n      <th>user_id_7</th>\n      <th>user_id_8</th>\n      <th>user_id_9</th>\n      <th>...</th>\n      <th>item_id_1673</th>\n      <th>item_id_1674</th>\n      <th>item_id_1675</th>\n      <th>item_id_1676</th>\n      <th>item_id_1677</th>\n      <th>item_id_1678</th>\n      <th>item_id_1679</th>\n      <th>item_id_1680</th>\n      <th>item_id_1681</th>\n      <th>item_id_1682</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>3</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>3</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>2</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n  </tbody>\n</table>\n<p>5 rows × 2626 columns</p>\n</div>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "(100000, 2626)"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memory usage is 263.3 MB\n"
     ]
    }
   ],
   "source": [
    "data_one_hot = pd.get_dummies(data, columns=['user_id', 'item_id'])\n",
    "display(data_one_hot.head())\n",
    "display(data_one_hot.shape)\n",
    "print_memory_usage_of_data_frame(data_one_hot)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "We will slice the dataset `user_movie` will be our `x_train` data transformed with one-hot encoding.\n",
    "So we expect its schema to be the number of ratings as rows and binary columns for users + binary columns\n",
    "for each item (in our case movies). This will lead to (100000, 2625)\n",
    "\n",
    "The target data will be the `ratings`, which will include the ratings and thus will have a shape of (100000,1)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "user_movie = data_one_hot[data_one_hot.columns.difference(['rating'])]\n",
    "ratings = data['rating']"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Data Ingestion\n",
    "Then we proceed with ingesting `user_movies` into sparse TileDB arrays as our training data and `rating` into dense TileDB\n",
    "array as our target data. Here, we should point out that besides the\n",
    "flexibility of TileDB in defining a schema, i.e., multiple dimensions, multiple attributes, compression etc,\n",
    "we choose to define a simple schema. So, for a numpy array of D number of dimensions we create a dense TileDB array,\n",
    "with the same number of dimensions, and a single attribute of data type numpy float32."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def get_schema(data: np.array, batch_size: int, sparse: bool) -> tiledb.ArraySchema:\n",
    "    dims = [\n",
    "        tiledb.Dim(\n",
    "            name=\"dim_\" + str(dim),\n",
    "            domain=(0, data.shape[dim] - 1),\n",
    "            tile=data.shape[dim] if dim > 0 else batch_size,\n",
    "            dtype=np.int32,\n",
    "        )\n",
    "        for dim in range(data.ndim)\n",
    "    ]\n",
    "\n",
    "    # TileDB schema\n",
    "    schema = tiledb.ArraySchema(\n",
    "        domain=tiledb.Domain(*dims),\n",
    "        sparse=sparse,\n",
    "        attrs=[tiledb.Attr(name=\"features\", dtype=np.float32)],\n",
    "    )\n",
    "\n",
    "    return schema\n",
    "\n",
    "# Let's define an ingestion function\n",
    "def ingest_in_tiledb(data: np.array, batch_size: int, uri: str, sparse: bool):\n",
    "    schema = get_schema(data, batch_size, sparse)\n",
    "\n",
    "    # Create the (empty) array on disk.\n",
    "    tiledb.Array.create(uri, schema)\n",
    "\n",
    "    # Ingest\n",
    "    with tiledb.open(uri, \"w\") as tiledb_array:\n",
    "        idx = np.nonzero(data) if sparse else slice(None)\n",
    "        tiledb_array[idx] = {\"features\": data[idx]}"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "We transform our sliced/split data to numpy arrays, so we can ingest them in TileDB arrays"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [],
   "source": [
    "user_movie = user_movie.to_numpy()\n",
    "ratings = ratings.to_numpy()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "We ingest `user_movie` as sparse TileDB array and `ratings` as dense TileDB array"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [],
   "source": [
    "#Ingest images\n",
    "ingest_in_tiledb(data=user_movie, batch_size=64, uri='training_images', sparse=True)\n",
    "\n",
    "# Ingest labels\n",
    "ingest_in_tiledb(data=ratings, batch_size=64, uri='training_labels', sparse=False)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now explore our TileDB arrays and check their structure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ArraySchema(\n",
      "  domain=Domain(*[\n",
      "    Dim(name='dim_0', domain=(0, 99999), tile=64, dtype='int32'),\n",
      "    Dim(name='dim_1', domain=(0, 2624), tile=2625, dtype='int32'),\n",
      "  ]),\n",
      "  attrs=[\n",
      "    Attr(name='features', dtype='float32', var=False, nullable=False),\n",
      "  ],\n",
      "  cell_order='row-major',\n",
      "  tile_order='row-major',\n",
      "  capacity=10000,\n",
      "  sparse=True,\n",
      "  allows_duplicates=False,\n",
      "  coords_filters=FilterList([ZstdFilter(level=-1)]),\n",
      ")\n",
      "\n",
      "ArraySchema(\n",
      "  domain=Domain(*[\n",
      "    Dim(name='dim_0', domain=(0, 99999), tile=64, dtype='int32'),\n",
      "  ]),\n",
      "  attrs=[\n",
      "    Attr(name='features', dtype='float32', var=False, nullable=False),\n",
      "  ],\n",
      "  cell_order='row-major',\n",
      "  tile_order='row-major',\n",
      "  capacity=10000,\n",
      "  sparse=False,\n",
      "  coords_filters=FilterList([ZstdFilter(level=-1)]),\n",
      ")\n",
      "\n"
     ]
    }
   ],
   "source": [
    "user_movie_array = tiledb.open('training_images')\n",
    "ratings_array = tiledb.open('training_labels')\n",
    "\n",
    "print(user_movie_array.schema)\n",
    "print(ratings_array.schema)"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Model training\n",
    "Although we used Factorization Machines as a reference model to create\n",
    "our training set, here we will train a simple Logistic Regression model in Pytorch only\n",
    "for demonstration purposes. Anyone can easily build any Model to train on the data."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Declare Model Class"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "class LogisticRegression(nn.Module):\n",
    "    def __init__(self, shape):\n",
    "        super(LogisticRegression, self).__init__()\n",
    "        self.linear = torch.nn.Linear(shape[0], shape[1])\n",
    "\n",
    "    def forward(self, x):\n",
    "        outputs = self.linear(x)\n",
    "        return outputs"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Train the Model\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1 Batch: 0 Loss: 12.104012\n",
      "Train Epoch: 1 Batch: 500 Loss: 1.472791\n",
      "Train Epoch: 1 Batch: 1000 Loss: 0.881996\n",
      "Train Epoch: 1 Batch: 1500 Loss: 1.014111\n",
      "Train Epoch: 1 Batch: 2000 Loss: 1.543179\n",
      "Train Epoch: 1 Batch: 2500 Loss: 1.280166\n",
      "Train Epoch: 1 Batch: 3000 Loss: 1.497870\n",
      "Train Epoch: 2 Batch: 0 Loss: 1.403207\n",
      "Train Epoch: 2 Batch: 500 Loss: 1.085330\n",
      "Train Epoch: 2 Batch: 1000 Loss: 0.796749\n",
      "Train Epoch: 2 Batch: 1500 Loss: 0.949489\n",
      "Train Epoch: 2 Batch: 2000 Loss: 1.351271\n",
      "Train Epoch: 2 Batch: 2500 Loss: 1.202208\n",
      "Train Epoch: 2 Batch: 3000 Loss: 1.448072\n",
      "Train Epoch: 3 Batch: 0 Loss: 1.323732\n",
      "Train Epoch: 3 Batch: 500 Loss: 0.966254\n",
      "Train Epoch: 3 Batch: 1000 Loss: 0.751343\n",
      "Train Epoch: 3 Batch: 1500 Loss: 0.913042\n",
      "Train Epoch: 3 Batch: 2000 Loss: 1.259153\n",
      "Train Epoch: 3 Batch: 2500 Loss: 1.148458\n",
      "Train Epoch: 3 Batch: 3000 Loss: 1.414244\n",
      "Train Epoch: 4 Batch: 0 Loss: 1.277605\n",
      "Train Epoch: 4 Batch: 500 Loss: 0.917042\n",
      "Train Epoch: 4 Batch: 1000 Loss: 0.723554\n",
      "Train Epoch: 4 Batch: 1500 Loss: 0.892419\n",
      "Train Epoch: 4 Batch: 2000 Loss: 1.208037\n",
      "Train Epoch: 4 Batch: 2500 Loss: 1.107316\n",
      "Train Epoch: 4 Batch: 3000 Loss: 1.389068\n"
     ]
    }
   ],
   "source": [
    "from tiledb.ml.readers.pytorch_sparse import PyTorchTileDBSparseDataset\n",
    "\n",
    "with tiledb.open('training_images') as x, tiledb.open('training_labels') as y:\n",
    "    tiledb_dataset = PyTorchTileDBSparseDataset(x_array=x, y_array=y, batch_size=32)\n",
    "    train_loader = torch.utils.data.DataLoader(tiledb_dataset, batch_size=None)\n",
    "\n",
    "    #Number of ratings x (user + movies)\n",
    "    datashape_x = (100000, 2625)\n",
    "\n",
    "    logre = LogisticRegression(shape=(2625, 1))\n",
    "    criterion = nn.MSELoss()\n",
    "    optimizer = optim.SGD(logre.parameters(), lr=0.01, momentum=0.5)\n",
    "\n",
    "    for epoch in range(1, 5):\n",
    "        logre.train()\n",
    "        for batch_idx, (inputs, labels) in enumerate(train_loader):\n",
    "            # zero the parameter gradients\n",
    "            optimizer.zero_grad()\n",
    "            # forward + backward + optimize\n",
    "            outputs = logre(inputs.to(torch.float))\n",
    "            loss = criterion(outputs, labels.type(torch.FloatTensor).view(-1,1))\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            if batch_idx % 500 == 0:\n",
    "                print('Train Epoch: {} Batch: {} Loss: {:.6f}'.format(\n",
    "                epoch, batch_idx, loss.item()))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Garbage Collection\n",
    "We delete the RAW dataset `u.data`"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [],
   "source": [
    "os.remove(filename)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}