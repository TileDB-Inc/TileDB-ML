{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This example notebook shows how we can train an image classification model, as described [here](https://github.com/tensorflow/docs/blob/master/site/en/tutorials/quickstart/beginner.ipynb),\n",
    "with the use of TileDB support for Tensorflow Data API for dense TileDB arrays. We will firstly ingest our MNIST dataset in two dense TileDB arrays, i.e, x and y,\n",
    "and then move on with training of a classification model with Keras. Firstly, let's import what we need."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import tensorflow as tf\n",
    "import tiledb\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load MNIST dataset for Keras datasets and scale."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "mnist = tf.keras.datasets.mnist\n",
    "(images, labels), _ = mnist.load_data()\n",
    "images = images / 255.0\n",
    "\n",
    "print(\"Images:\", images.shape, images.dtype)\n",
    "print(\"Labels:\", labels.shape, labels.dtype)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we proceed with ingesting images and labels into dense TileDB arrays. Here, we should point out that besides the\n",
    "flexibility of TileDB in defining a schema, i.e., multiple dimensions, multiple attributes, compression etc,\n",
    "we choose to define a simple schema. So, for a numpy array of D number of dimensions we create a dense TileDB array,\n",
    "with the same number of dimensions, and a single attribute of data type numpy float32. Moreover, the\n",
    "tile extend of the 1st dimension should always be equal with the batch size, in order to achieve optimal reads while\n",
    "training. Let's define an ingestion function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def ingest_in_tiledb(data: np.array, batch_size: int, uri: str):\n",
    "    # Equal number of dimensions with the numpy array.\n",
    "    dims = [\n",
    "        tiledb.Dim(\n",
    "            name=\"dim_\" + str(dim),\n",
    "            domain=(0, data.shape[dim] - 1),\n",
    "            tile=data.shape[dim] if dim > 0 else batch_size,\n",
    "            dtype=np.int32,\n",
    "        )\n",
    "        for dim in range(data.ndim)\n",
    "    ]\n",
    "\n",
    "    # TileDB schema\n",
    "    schema = tiledb.ArraySchema(\n",
    "        domain=tiledb.Domain(*dims),\n",
    "        sparse=False,\n",
    "        attrs=[tiledb.Attr(name=\"features\", dtype=data.dtype)],\n",
    "    )\n",
    "    # Create array\n",
    "    tiledb.Array.create(uri, schema)\n",
    "\n",
    "    # Ingest\n",
    "    with tiledb.open(uri, \"w\") as tiledb_array:\n",
    "        tiledb_array[:] = {\"features\": data}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Now we proceed with ingestion."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "data_dir = os.path.join(\"data/tensorflow_data_api_tiledb_dense\")\n",
    "os.makedirs(data_dir, exist_ok=True)\n",
    "\n",
    "# Ingest images\n",
    "training_images = os.path.join(data_dir, 'training_images')\n",
    "if not os.path.exists(training_images):\n",
    "    ingest_in_tiledb(data=images, batch_size=64, uri=training_images)\n",
    "\n",
    "# Ingest labels\n",
    "training_labels = os.path.join(data_dir, 'training_labels')\n",
    "if not os.path.exists(training_labels):\n",
    "    ingest_in_tiledb(data=labels, batch_size=64, uri=training_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now explore our TileDB arrays and check their structure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "images_array = tiledb.open(training_images)\n",
    "labels_array = tiledb.open(training_labels)\n",
    "\n",
    "print(images_array.schema)\n",
    "print(labels_array.schema)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can easily now slice our data and create some plots. We can either slice an image or a part of\n",
    "an image. Because we use only one attribute, we always slice with attribute with index equal to 0.\n",
    "Some examples below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Plot an image\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.imshow(images_array[0][images_array.schema.attr(0).name], cmap=\"gray\")\n",
    "\n",
    "# Plot part of the same image\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.imshow(images_array[0, 5:20, 5:20][images_array.schema.attr(0).name], cmap=\"gray\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can then define a function that creates a basic digit classifier for the MNIST dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def create_model():\n",
    "    model = tf.keras.models.Sequential([\n",
    "        tf.keras.layers.Flatten(input_shape=(28, 28)),\n",
    "        tf.keras.layers.Dense(128, activation='relu'),\n",
    "        tf.keras.layers.Dropout(0.2),\n",
    "        tf.keras.layers.Dense(10)\n",
    "    ])\n",
    "\n",
    "    loss_fn = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    "\n",
    "    model.compile(optimizer='adam',\n",
    "                  loss=loss_fn,\n",
    "                  metrics=['accuracy'])\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Now we move on with creating a model, opening the arrays, define a Tensorflow TileDB dataset and train the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from tiledb.ml.readers.tensorflow import TensorflowTileDBDataset\n",
    "from tiledb.ml.readers.types import ArrayParams\n",
    "\n",
    "model = create_model()\n",
    "\n",
    "ctx = tiledb.Ctx({\"sm.memory_budget\": 1024**2})\n",
    "with tiledb.open(training_images, ctx=ctx) as x, tiledb.open(training_labels, ctx=ctx) as y:\n",
    "    tiledb_dataset = TensorflowTileDBDataset(\n",
    "        ArrayParams(array=x, fields=['features']),\n",
    "        ArrayParams(array=y, fields=['features']),\n",
    "        batch_size=64, shuffle_buffer_size=128\n",
    "    )\n",
    "    model.fit(tiledb_dataset, epochs=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}